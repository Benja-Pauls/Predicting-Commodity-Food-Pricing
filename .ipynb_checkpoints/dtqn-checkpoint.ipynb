{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb39b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union, Tuple, Optional, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "from torchrl.data import BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec\n",
    "from torchrl.envs import EnvBase\n",
    "from tensordict import TensorDict\n",
    "from enum import Enum\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "import wandb\n",
    "import csv\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from __future__ import annotations\n",
    "import math\n",
    "from abc import ABC, abstractmethod\n",
    "from time import time, sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d41283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a long enough positional encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # Register as a buffer that is not a model parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is [batch_size, seq_len, feature_size]\n",
    "        # Adjust positional encoding to have the same size as the input\n",
    "        pe = self.pe[:x.size(1), :]  # Shape: [seq_len, d_model]\n",
    "        pe = pe.squeeze(1)  # Remove the singleton dimension\n",
    "        # Ensure pe is expanded to match the batch size of x\n",
    "        pe = pe.unsqueeze(0).repeat(x.size(0), 1, 1)  # Shape: [batch_size, seq_len, d_model]\n",
    "        # The add operation below should now be valid\n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28c07e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, head_size, num_heads, ff_dim, dropout=0):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout)\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=ff_dim, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=ff_dim, out_channels=d_model, kernel_size=1)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Note: MultiheadAttention expects input of shape (L, N, E) where L is the sequence length, N is the batch size, and E is the embedding dimension.\n",
    "        attn_output, _ = self.attention(query, key, value)\n",
    "        out1 = self.layernorm1(query + attn_output)\n",
    "        \n",
    "        # Conv1D layers expect input of shape (N, C, L), hence we permute\n",
    "        out1_permuted = out1.permute(1, 2, 0)\n",
    "        ff_output = F.relu(self.conv1(out1_permuted))\n",
    "        ff_output = self.conv2(ff_output)\n",
    "        \n",
    "        # Permute back to match the MultiheadAttention output shape\n",
    "        ff_output = ff_output.permute(2, 0, 1)\n",
    "        out2 = self.layernorm2(out1 + self.dropout(ff_output))\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccecf2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_input_samples, d_model, head_size, num_heads, ff_dim, dropout=0, num_transformers=10):\n",
    "        super(TransformerModel, self).__init__()  # Corrected super() call\n",
    "        self.d_model = d_model\n",
    "        self.input_projection = nn.Linear(1, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, num_input_samples)\n",
    "        self.transformers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, head_size, num_heads, ff_dim, dropout) \n",
    "            for _ in range(num_transformers)\n",
    "        ])\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)  # Projects input to d_model dimensions\n",
    "        x = x + self.pos_encoding(x)  # Apply positional encoding\n",
    "\n",
    "        for transformer in self.transformers:\n",
    "            x = transformer(x, x, x)  # Process through transformer blocks\n",
    "\n",
    "        x = x.mean(dim=1, keepdim=True)  # Aggregate features\n",
    "        x = self.output_layer(x)  # Apply the output layer to get the final prediction\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7425a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationEmbeddingRepresentation(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_embedding: nn.Module,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.observation_embedding = observation_embedding\n",
    "\n",
    "    def forward(self, obs: torch.Tensor):\n",
    "        batch, seq = obs.size(0), obs.size(1)\n",
    "        # Flatten batch and seq dims\n",
    "        obs = torch.flatten(obs, start_dim=0, end_dim=1)\n",
    "        obs_embed = self.observation_embedding(obs)\n",
    "        obs_embed = obs_embed.reshape(batch, seq, obs_embed.size(-1))\n",
    "        return obs_embed\n",
    "    @staticmethod\n",
    "    def make_action_representation(\n",
    "        num_actions: int,\n",
    "        action_dim: int,\n",
    "    ) -> ObservationEmbeddingRepresentation:\n",
    "        embed = nn.Sequential(\n",
    "            nn.Embedding(num_actions, action_dim), nn.Flatten(start_dim=-2)\n",
    "        )\n",
    "        return ObservationEmbeddingRepresentation(observation_embedding=embed)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_continuous_representation(obs_dim: int, outer_embed_size: int):\n",
    "        \"\"\"\n",
    "        For use in continuous observation environments. Projects the observation to the\n",
    "            specified dimensionality for use in the network.\n",
    "\n",
    "        Args:\n",
    "            obs_dim:    The length of the observation vector (assuming 1d)\n",
    "            embed_size: The length of the resulting embedding vector\n",
    "        \"\"\"\n",
    "        embedding = nn.Linear(obs_dim, outer_embed_size)\n",
    "        return ObservationEmbeddingRepresentation(observation_embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29e2f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flattened_size(\n",
    "    height: int, width: int, kernels: list, paddings: list, strides: list\n",
    ") -> int:\n",
    "    for i in range(len(kernels)):\n",
    "        height = update_size(height, kernels[i], paddings[i], strides[i])\n",
    "        width = update_size(width, kernels[i], paddings[i], strides[i])\n",
    "    return int(height * width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fd1f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_size(component: int, kernel: int, padding: int, stride: int) -> int:\n",
    "    return math.floor((component - kernel + 2 * padding) / stride) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "581148ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionEmbeddingRepresentation(nn.Module):\n",
    "    def __init__(self, num_actions: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(num_actions, action_dim),\n",
    "            nn.Flatten(start_dim=-2),\n",
    "        )\n",
    "\n",
    "    def forward(self, action: torch.Tensor):\n",
    "        return self.embedding(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9aaf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        network_factory: Callable[[], Module],\n",
    "        buffer_size: int,\n",
    "        device: torch.device,\n",
    "        env_obs_length: int,\n",
    "        max_env_steps: int,\n",
    "        obs_mask: Union[int, float],\n",
    "        num_actions: int,\n",
    "        is_discrete_env: bool,\n",
    "        learning_rate: float = 0.0003,\n",
    "        batch_size: int = 32,\n",
    "        context_len: int = 1,\n",
    "        gamma: float = 0.99,\n",
    "        grad_norm_clip: float = 1.0,\n",
    "        target_update_frequency: int = 10_000,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.context_len = context_len\n",
    "        self.env_obs_length = env_obs_length\n",
    "        # Initialize environment & networks\n",
    "        self.policy_network = network_factory()\n",
    "        self.target_network = network_factory()\n",
    "        # Ensure network's parameters are the same\n",
    "        self.target_update()\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # We can be more efficient with space if we are using discrete environments\n",
    "        # and don't need to use floats\n",
    "        if is_discrete_env:\n",
    "            self.obs_context_type = np.int_\n",
    "            self.obs_tensor_type = torch.long\n",
    "        else:\n",
    "            self.obs_context_type = np.float32\n",
    "            self.obs_tensor_type = torch.float32\n",
    "\n",
    "        # PyTorch config\n",
    "        self.device = device\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(\n",
    "            buffer_size,\n",
    "            env_obs_length=env_obs_length,\n",
    "            obs_mask=obs_mask,\n",
    "            max_episode_steps=max_env_steps,\n",
    "            context_len=context_len,\n",
    "        )\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.grad_norm_clip = grad_norm_clip\n",
    "        self.target_update_frequency = target_update_frequency\n",
    "\n",
    "        # Logging\n",
    "        self.num_train_steps = 0\n",
    "        self.td_errors = RunningAverage(100)\n",
    "        self.grad_norms = RunningAverage(100)\n",
    "        self.qvalue_max = RunningAverage(100)\n",
    "        self.target_max = RunningAverage(100)\n",
    "        self.qvalue_mean = RunningAverage(100)\n",
    "        self.target_mean = RunningAverage(100)\n",
    "        self.qvalue_min = RunningAverage(100)\n",
    "        self.target_min = RunningAverage(100)\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.train_mode = TrainMode.TRAIN\n",
    "        self.obs_mask = obs_mask\n",
    "\n",
    "        self.train_context = Context(\n",
    "            context_len, obs_mask, self.num_actions, env_obs_length\n",
    "        )\n",
    "        self.eval_context = Context(\n",
    "            context_len, obs_mask, self.num_actions, env_obs_length\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def context(self) -> Context:\n",
    "        if self.train_mode == TrainMode.TRAIN:\n",
    "            return self.train_context\n",
    "        elif self.train_mode == TrainMode.EVAL:\n",
    "            return self.eval_context\n",
    "\n",
    "    def eval_on(self) -> None:\n",
    "        self.train_mode = TrainMode.EVAL\n",
    "        self.policy_network.eval()\n",
    "\n",
    "    def eval_off(self) -> None:\n",
    "        self.train_mode = TrainMode.TRAIN\n",
    "        self.policy_network.train()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, epsilon=0.0) -> int:\n",
    "        \"\"\"Use policy_network to get an e-greedy action given the current obs.\"\"\"\n",
    "        if RNG.rng.random() < epsilon:\n",
    "            return RNG.rng.integers(self.num_actions)\n",
    "        q_values = self.policy_network(\n",
    "            torch.as_tensor(\n",
    "                self.context.obs[min(self.context.timestep, self.context_len - 1)],\n",
    "                dtype=self.obs_tensor_type,\n",
    "                device=self.device,\n",
    "            )\n",
    "            .unsqueeze(0)\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def observe(self, obs, action, reward, done) -> None:\n",
    "        if self.train_mode == TrainMode.TRAIN:\n",
    "            self.replay_buffer.store(obs, action, reward, done)\n",
    "\n",
    "    def context_reset(self, obs: np.ndarray) -> None:\n",
    "        self.context.reset(obs)\n",
    "        if self.train_mode == TrainMode.TRAIN:\n",
    "            self.replay_buffer.store_obs(obs)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"Perform one gradient step of the network\"\"\"\n",
    "        if not self.replay_buffer.can_sample(self.batch_size):\n",
    "            return\n",
    "\n",
    "        self.eval_off()\n",
    "        obss, actions, rewards, next_obss, _, dones, _ = self.replay_buffer.sample(\n",
    "            self.batch_size\n",
    "        )\n",
    "\n",
    "        # We pull obss/next_obss as [batch-size x 1 x obs-dim]\n",
    "        obss = torch.as_tensor(obss, dtype=self.obs_tensor_type, device=self.device)\n",
    "        next_obss = torch.as_tensor(\n",
    "            next_obss, dtype=self.obs_tensor_type, device=self.device\n",
    "        )\n",
    "        # Actions is [batch-size x 1 x 1] which we want to be [batch-size x 1]\n",
    "        actions = torch.as_tensor(actions, dtype=torch.long, device=self.device)\n",
    "        # Rewards/Dones are [batch-size x 1 x 1] which we want to be [batch-size]\n",
    "        rewards = torch.as_tensor(\n",
    "            rewards, dtype=torch.float32, device=self.device\n",
    "        ).squeeze()\n",
    "        dones = torch.as_tensor(dones, dtype=torch.long, device=self.device).squeeze()\n",
    "\n",
    "        # obss is [batch-size x obs-dim] and after network is [batch-size x action-dim]\n",
    "        # Then we gather it and squeeze to [batch-size]\n",
    "        q_values = self.policy_network(obss)\n",
    "        # [batch-seq-actions]\n",
    "        q_values = q_values.gather(2, actions).squeeze()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # We use DDQN, so the policy network determines which future actions we'd\n",
    "            # take, but the target network determines the value of those\n",
    "            next_obs_qs = self.policy_network(next_obss)\n",
    "            argmax = torch.argmax(next_obs_qs, axis=-1).unsqueeze(-1)\n",
    "            next_obs_q_values = (\n",
    "                self.target_network(next_obss).gather(2, argmax).squeeze()\n",
    "            )\n",
    "\n",
    "            # here goes BELLMAN\n",
    "            targets = rewards + (1 - dones) * (next_obs_q_values * self.gamma)\n",
    "\n",
    "        self.qvalue_max.add(q_values.max().item())\n",
    "        self.qvalue_mean.add(q_values.mean().item())\n",
    "        self.qvalue_min.add(q_values.min().item())\n",
    "\n",
    "        self.target_max.add(targets.max().item())\n",
    "        self.target_mean.add(targets.mean().item())\n",
    "        self.target_min.add(targets.min().item())\n",
    "\n",
    "        # Optimization step\n",
    "        loss = F.mse_loss(q_values, targets)\n",
    "        self.td_errors.add(loss.item())\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        norm = torch.nn.utils.clip_grad_norm_(\n",
    "            self.policy_network.parameters(),\n",
    "            self.grad_norm_clip,\n",
    "            error_if_nonfinite=True,\n",
    "        )\n",
    "        self.grad_norms.add(norm.item())\n",
    "        self.optimizer.step()\n",
    "        self.num_train_steps += 1\n",
    "\n",
    "        if self.num_train_steps % self.target_update_frequency == 0:\n",
    "            self.target_update()\n",
    "\n",
    "    def target_update(self) -> None:\n",
    "        \"\"\"Hard update where we copy the network parameters from the policy network to the target network\"\"\"\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    def save_mini_checkpoint(self, checkpoint_dir: str, wandb_id: str) -> None:\n",
    "        torch.save(\n",
    "            {\"step\": self.num_train_steps, \"wandb_id\": wandb_id},\n",
    "            checkpoint_dir + \"_mini_checkpoint.pt\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def load_mini_checkpoint(checkpoint_dir: str) -> dict:\n",
    "        return torch.load(checkpoint_dir + \"_mini_checkpoint.pt\")\n",
    "\n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        checkpoint_dir: str,\n",
    "        wandb_id: str,\n",
    "        episode_successes: RunningAverage,\n",
    "        episode_rewards: RunningAverage,\n",
    "        episode_lengths: RunningAverage,\n",
    "        eps: LinearAnneal,\n",
    "    ) -> None:\n",
    "        self.save_mini_checkpoint(checkpoint_dir=checkpoint_dir, wandb_id=wandb_id)\n",
    "        torch.save(\n",
    "            # np.savez_compressed(\n",
    "            # checkpoint_dir + \"_checkpoint\",\n",
    "            {\n",
    "                \"step\": self.num_train_steps,\n",
    "                \"wandb_id\": wandb_id,\n",
    "                # Replay Buffer: Don't keep the observation index saved\n",
    "                \"replay_buffer_pos\": [self.replay_buffer.pos[0], 0],\n",
    "                # Neural Net\n",
    "                \"policy_net_state_dict\": self.policy_network.state_dict(),\n",
    "                \"target_net_state_dict\": self.target_network.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"epsilon\": eps.val,\n",
    "                # Results\n",
    "                \"episode_successes\": episode_successes,\n",
    "                \"episode_rewards\": episode_rewards,\n",
    "                \"episode_lengths\": episode_lengths,\n",
    "                # Losses\n",
    "                \"td_errors\": self.td_errors,\n",
    "                \"grad_norms\": self.grad_norms,\n",
    "                \"qvalue_max\": self.qvalue_max,\n",
    "                \"qvalue_mean\": self.qvalue_mean,\n",
    "                \"qvalue_min\": self.qvalue_min,\n",
    "                \"target_max\": self.target_max,\n",
    "                \"target_mean\": self.target_mean,\n",
    "                \"target_min\": self.target_min,\n",
    "                # RNG states\n",
    "                \"random_rng_state\": random.getstate(),\n",
    "                \"rng_bit_generator_state\": RNG.rng.bit_generator.state,\n",
    "                \"numpy_rng_state\": np.random.get_state(),\n",
    "                \"torch_rng_state\": torch.get_rng_state(),\n",
    "                \"torch_cuda_rng_state\": torch.cuda.get_rng_state()\n",
    "                if torch.cuda.is_available()\n",
    "                else torch.get_rng_state(),\n",
    "            },\n",
    "            checkpoint_dir + \"_checkpoint.pt\",\n",
    "        )\n",
    "        joblib.dump(self.replay_buffer.obss, checkpoint_dir + \"buffer_obss.sav\")\n",
    "        joblib.dump(self.replay_buffer.actions, checkpoint_dir + \"buffer_actions.sav\")\n",
    "        joblib.dump(self.replay_buffer.rewards, checkpoint_dir + \"buffer_rewards.sav\")\n",
    "        joblib.dump(self.replay_buffer.dones, checkpoint_dir + \"buffer_dones.sav\")\n",
    "        joblib.dump(\n",
    "            self.replay_buffer.episode_lengths, checkpoint_dir + \"buffer_eplens.sav\"\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(\n",
    "        self, checkpoint_dir: str\n",
    "    ) -> Tuple[str, RunningAverage, RunningAverage, RunningAverage, float]:\n",
    "        checkpoint = torch.load(checkpoint_dir + \"_checkpoint.pt\")\n",
    "        # checkpoint = np.load(checkpoint_dir + \"_checkpoint.npz\", allow_pickle=True)\n",
    "\n",
    "        self.num_train_steps = checkpoint[\"step\"]\n",
    "        # Replay Buffer\n",
    "        self.replay_buffer.pos = checkpoint[\"replay_buffer_pos\"]\n",
    "        self.replay_buffer.obss = joblib.load(checkpoint_dir + \"buffer_obss.sav\")\n",
    "        self.replay_buffer.actions = joblib.load(checkpoint_dir + \"buffer_actions.sav\")\n",
    "        self.replay_buffer.rewards = joblib.load(checkpoint_dir + \"buffer_rewards.sav\")\n",
    "        self.replay_buffer.dones = joblib.load(checkpoint_dir + \"buffer_dones.sav\")\n",
    "        self.replay_buffer.episode_lengths = joblib.load(\n",
    "            checkpoint_dir + \"buffer_eplens.sav\"\n",
    "        )\n",
    "        # Neural Net\n",
    "        self.policy_network.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
    "        self.target_network.load_state_dict(checkpoint[\"target_net_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        # Losses\n",
    "        self.td_errors = checkpoint[\"td_errors\"]\n",
    "        self.grad_norms = checkpoint[\"grad_norms\"]\n",
    "        self.qvalue_max = checkpoint[\"qvalue_max\"]\n",
    "        self.qvalue_mean = checkpoint[\"qvalue_mean\"]\n",
    "        self.qvalue_min = checkpoint[\"qvalue_min\"]\n",
    "        self.target_max = checkpoint[\"target_max\"]\n",
    "        self.target_mean = checkpoint[\"target_mean\"]\n",
    "        self.target_min = checkpoint[\"target_min\"]\n",
    "        # RNG states\n",
    "        random.setstate(checkpoint[\"random_rng_state\"])\n",
    "        RNG.rng.bit_generator.state = checkpoint[\"rng_bit_generator_state\"]\n",
    "        np.random.set_state(checkpoint[\"numpy_rng_state\"])\n",
    "        torch.set_rng_state(checkpoint[\"torch_rng_state\"])\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_rng_state(checkpoint[\"torch_cuda_rng_state\"])\n",
    "\n",
    "        # Results\n",
    "        episode_successes = checkpoint[\"episode_successes\"]\n",
    "        episode_rewards = checkpoint[\"episode_rewards\"]\n",
    "        episode_lengths = checkpoint[\"episode_lengths\"]\n",
    "        # Exploration value\n",
    "        epsilon = checkpoint[\"epsilon\"]\n",
    "\n",
    "        return (\n",
    "            checkpoint[\"wandb_id\"],\n",
    "            episode_successes,\n",
    "            episode_rewards,\n",
    "            episode_lengths,\n",
    "            epsilon,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac8876c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTQN(nn.Module):\n",
    "    \"\"\"Deep Transformer Q-Network for partially observable reinforcement learning.\n",
    "\n",
    "    Args:\n",
    "        obs_dim:            The length of the observation vector.\n",
    "        num_actions:        The number of possible environments actions.\n",
    "        embed_per_obs_dim:  Used for discrete observation space. Length of the embed for each\n",
    "            element in the observation dimension.\n",
    "        action_dim:         The number of features to give the action.\n",
    "        inner_embed_size:   The dimensionality of the network. Referred to as d_k by the\n",
    "            original transformer.\n",
    "        num_heads:          The number of heads to use in the MultiHeadAttention.\n",
    "        num_layers:         The number of transformer blocks to use.\n",
    "        history_len:        The maximum number of observations to take in.\n",
    "        dropout:            Dropout percentage. Default: `0.0`\n",
    "        gate:               Which layer to use after the attention and feedforward submodules (choices: `res`\n",
    "            or `gru`). Default: `res`\n",
    "        identity:           Whether or not to use identity map reordering. Default: `False`\n",
    "        pos:                The kind of position encodings to use. `0` uses no position encodings, `1` uses\n",
    "            learned position encodings, and `sin` uses sinusoidal encodings. Default: `1`\n",
    "        discrete:           Whether or not the environment has discrete observations. Default: `False`\n",
    "        vocab_sizes:        If discrete env only. Represents the number of observations in the\n",
    "            environment. If the environment has multiple obs dims with different number\n",
    "            of observations in each dim, this can be supplied as a vector. Default: `None`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        num_actions: int,\n",
    "        embed_per_obs_dim: int,\n",
    "        action_dim: int,\n",
    "        inner_embed_size: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        history_len: int,\n",
    "        dropout: float = 0.0,\n",
    "        identity: bool = False,\n",
    "        pos: Union[str, int] = 1,\n",
    "        discrete: bool = False,\n",
    "        vocab_sizes: Optional[Union[np.ndarray, int]] = None,\n",
    "        bag_size: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.discrete = discrete\n",
    "        # Input Embedding: Allocate space for the action embedding\n",
    "        obs_output_dim = inner_embed_size - action_dim\n",
    "        if action_dim > 0:\n",
    "            self.action_embedding = ActionEmbeddingRepresentation(\n",
    "                num_actions=num_actions, action_dim=action_dim\n",
    "            )\n",
    "        else:\n",
    "            self.action_embedding = None\n",
    "        self.obs_embedding = (\n",
    "            ObservationEmbeddingRepresentation.make_continuous_representation(\n",
    "                obs_dim=obs_dim, outer_embed_size=obs_output_dim\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # PosEnum.LEARNED/SIN/NONE\n",
    "        pos_function_map = PositionEncoding\n",
    "        self.position_embedding = pos_function_map\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        if identity:\n",
    "            transformer_block = TransformerIdentityLayer\n",
    "        else:\n",
    "            transformer_block = TransformerLayer\n",
    "        self.transformer_layers = nn.Sequential(\n",
    "            *[\n",
    "                transformer_block(\n",
    "                    num_heads,\n",
    "                    inner_embed_size,\n",
    "                    history_len,\n",
    "                    dropout,\n",
    "                    attn_gate,\n",
    "                    mlp_gate,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.bag_size = bag_size\n",
    "        self.bag_attn_weights = None\n",
    "        if bag_size > 0:\n",
    "            self.bag_attention = nn.MultiheadAttention(\n",
    "                inner_embed_size,\n",
    "                num_heads,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            self.ffn = nn.Sequential(\n",
    "                nn.Linear(inner_embed_size * 2, inner_embed_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(inner_embed_size, num_actions),\n",
    "            )\n",
    "        else:\n",
    "            self.ffn = nn.Sequential(\n",
    "                nn.Linear(inner_embed_size, inner_embed_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(inner_embed_size, num_actions),\n",
    "            )\n",
    "\n",
    "        self.history_len = history_len\n",
    "        self.apply(torch_utils.init_weights)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        obss: torch.Tensor,\n",
    "        actions: Optional[torch.Tensor] = None,\n",
    "        bag_obss: Optional[torch.Tensor] = None,\n",
    "        bag_actions: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        obss    is  batch x seq_len  x obs_dim\n",
    "        actions is  batch x seq_len  x       1\n",
    "        bag     is  batch x bag_size x obs_dim\n",
    "        \"\"\"\n",
    "        history_len = obss.size(1)\n",
    "        assert (\n",
    "            history_len <= self.history_len\n",
    "        ), \"Cannot forward, history is longer than expected.\"\n",
    "\n",
    "        # If the observations are images, obs_dim is the dimensions of the image\n",
    "        obs_dim = obss.size()[2:] if len(obss.size()) > 3 else obss.size(2)\n",
    "        assert (\n",
    "            obs_dim == self.obs_dim\n",
    "        ), f\"Obs dim is incorrect. Expected {self.obs_dim} got {obs_dim}\"\n",
    "\n",
    "        token_embeddings = self.obs_embedding(obss)\n",
    "\n",
    "        # Just to keep shapes correct if we choose to disble including actions\n",
    "        if self.action_embedding is not None:\n",
    "            # [batch x seq_len x 1] -> [batch x seq_len x action_embed]\n",
    "            action_embed = self.action_embedding(actions)\n",
    "\n",
    "            if history_len > 1:\n",
    "                action_embed = torch.roll(action_embed, 1, 1)\n",
    "                # First observation in the sequence doesn't have a previous action, so zero the features\n",
    "                action_embed[:, 0, :] = 0.0\n",
    "            token_embeddings = torch.concat([action_embed, token_embeddings], dim=-1)\n",
    "\n",
    "        # [batch x seq_len x model_embed] -> [batch x seq_len x model_embed]\n",
    "        working_memory = self.transformer_layers(\n",
    "            self.dropout(\n",
    "                token_embeddings + self.position_embedding()[:, :history_len, :]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if self.bag_size > 0:\n",
    "            # [batch x bag_size x action_embed] + [batch x bag_size x obs_embed] -> [batch x bag_size x model_embed]\n",
    "            if self.action_embedding is not None:\n",
    "                bag_embeddings = torch.concat(\n",
    "                    [self.action_embedding(bag_actions), self.obs_embedding(bag_obss)],\n",
    "                    dim=-1,\n",
    "                )\n",
    "            else:\n",
    "                bag_embeddings = self.obs_embedding(bag_obss)\n",
    "            # [batch x seq_len x model_embed] x [batch x bag_size x model_embed] -> [batch x seq_len x model_embed]\n",
    "            persistent_memory, self.attn_weights = self.bag_attention(\n",
    "                working_memory, bag_embeddings, bag_embeddings\n",
    "            )\n",
    "            output = self.ffn(torch.concat([working_memory, persistent_memory], dim=-1))\n",
    "        else:\n",
    "            output = self.ffn(working_memory)\n",
    "\n",
    "        return output[:, -history_len:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e0d079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainMode(Enum):\n",
    "    TRAIN = 1\n",
    "    EVAL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b5dbe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DtqnAgent(DqnAgent):\n",
    "    # noinspection PyTypeChecker\n",
    "    def __init__(\n",
    "        self,\n",
    "        network_factory: Callable[[], Module],\n",
    "        buffer_size: int,\n",
    "        device: torch.device,\n",
    "        env_obs_length: int,\n",
    "        max_env_steps: int,\n",
    "        obs_mask: Union[int, float],\n",
    "        num_actions: int,\n",
    "        is_discrete_env: bool,\n",
    "        learning_rate: float = 0.0003,\n",
    "        batch_size: int = 32,\n",
    "        context_len: int = 50,\n",
    "        gamma: float = 0.99,\n",
    "        grad_norm_clip: float = 1.0,\n",
    "        target_update_frequency: int = 10_000,\n",
    "        history: int = 50,\n",
    "        bag_size: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            network_factory,\n",
    "            buffer_size,\n",
    "            device,\n",
    "            env_obs_length,\n",
    "            max_env_steps,\n",
    "            obs_mask,\n",
    "            num_actions,\n",
    "            is_discrete_env,\n",
    "            learning_rate,\n",
    "            batch_size,\n",
    "            context_len,\n",
    "            gamma,\n",
    "            grad_norm_clip,\n",
    "            target_update_frequency,\n",
    "        )\n",
    "        self.history = history\n",
    "        self.train_context = Context(\n",
    "            context_len,\n",
    "            obs_mask,\n",
    "            num_actions,\n",
    "            env_obs_length,\n",
    "        )\n",
    "        self.eval_context = Context(\n",
    "            context_len,\n",
    "            obs_mask,\n",
    "            num_actions,\n",
    "            env_obs_length,\n",
    "        )\n",
    "        self.train_bag = Bag(bag_size, obs_mask, env_obs_length)\n",
    "        self.eval_bag = Bag(bag_size, obs_mask, env_obs_length)\n",
    "\n",
    "    @property\n",
    "    def bag(self) -> Bag:\n",
    "        if self.train_mode == TrainMode.TRAIN:\n",
    "            return self.train_bag\n",
    "        elif self.train_mode == TrainMode.EVAL:\n",
    "            return self.eval_bag\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, epsilon: float = 0.0) -> int:\n",
    "        if RNG.rng.random() < epsilon:\n",
    "            return RNG.rng.integers(self.num_actions)\n",
    "        # Truncate the context of observations and actions to remove padding if it exists\n",
    "        context_obs_tensor = torch.as_tensor(\n",
    "            self.context.obs[: min(self.context.max_length, self.context.timestep + 1)],\n",
    "            dtype=self.obs_tensor_type,\n",
    "            device=self.device,\n",
    "        ).unsqueeze(0)\n",
    "        context_action_tensor = torch.as_tensor(\n",
    "            self.context.action[\n",
    "                : min(self.context.max_length, self.context.timestep + 1)\n",
    "            ],\n",
    "            dtype=torch.long,\n",
    "            device=self.device,\n",
    "        ).unsqueeze(0)\n",
    "        # Always include the full bag, even if it has padding TODO:\n",
    "        bag_obs_tensor = torch.as_tensor(\n",
    "            self.bag.obss, dtype=self.obs_tensor_type, device=self.device\n",
    "        ).unsqueeze(0)\n",
    "        bag_action_tensor = torch.as_tensor(\n",
    "            self.bag.actions, dtype=torch.long, device=self.device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        q_values = self.policy_network(\n",
    "            context_obs_tensor, context_action_tensor, bag_obs_tensor, bag_action_tensor\n",
    "        )\n",
    "\n",
    "        # We take the argmax of the last timestep's Q values\n",
    "        # In other words, select the highest q value action\n",
    "        return torch.argmax(q_values[:, -1, :]).item()\n",
    "\n",
    "    def context_reset(self, obs: np.ndarray) -> None:\n",
    "        self.context.reset(obs)\n",
    "        if self.train_mode == TrainMode.TRAIN:\n",
    "            self.replay_buffer.store_obs(obs)\n",
    "        if self.bag.size > 0:\n",
    "            self.bag.reset()\n",
    "\n",
    "    def observe(self, obs: np.ndarray, action: int, reward: float, done: bool) -> None:\n",
    "        \"\"\"Add an observation to the context. If the context would evict an observation to make room,\n",
    "        attempt to put the observation in the bag, which may require evicting something else from the bag.\n",
    "\n",
    "        If we're in train mode, then we also add the transition to our replay buffer.\"\"\"\n",
    "        evicted_obs, evicted_action = self.context.add_transition(\n",
    "            obs, action, reward, done\n",
    "        )\n",
    "        # If there is an evicted obs, we need to decide if it should go in the bag or not\n",
    "        if self.bag.size > 0 and evicted_obs is not None:\n",
    "            # Bag is already full\n",
    "            if not self.bag.add(evicted_obs, evicted_action):\n",
    "                # For each possible bag, get the Q-values\n",
    "                possible_bag_obss = np.tile(self.bag.obss, (self.bag.size + 1, 1, 1))\n",
    "                possible_bag_actions = np.tile(\n",
    "                    self.bag.actions, (self.bag.size + 1, 1, 1)\n",
    "                )\n",
    "                for i in range(self.bag.size):\n",
    "                    possible_bag_obss[i, i] = evicted_obs\n",
    "                    possible_bag_actions[i, i] = evicted_action\n",
    "                tiled_context = np.tile(self.context.obs, (self.bag.size + 1, 1, 1))\n",
    "                tiled_actions = np.tile(self.context.action, (self.bag.size + 1, 1, 1))\n",
    "                q_values = self.policy_network(\n",
    "                    torch.as_tensor(\n",
    "                        tiled_context, dtype=self.obs_tensor_type, device=self.device\n",
    "                    ),\n",
    "                    torch.as_tensor(\n",
    "                        tiled_actions, dtype=torch.long, device=self.device\n",
    "                    ),\n",
    "                    torch.as_tensor(\n",
    "                        possible_bag_obss,\n",
    "                        dtype=self.obs_tensor_type,\n",
    "                        device=self.device,\n",
    "                    ),\n",
    "                    torch.as_tensor(\n",
    "                        possible_bag_actions, dtype=torch.long, device=self.device\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                bag_idx = torch.argmax(torch.mean(torch.max(q_values, 2)[0], 1))\n",
    "                self.bag.obss = possible_bag_obss[bag_idx]\n",
    "                self.bag.actions = possible_bag_actions[bag_idx]\n",
    "\n",
    "        if self.train_mode == TrainMode.TRAIN:\n",
    "            self.replay_buffer.store(obs, action, reward, done, self.context.timestep)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        if not self.replay_buffer.can_sample(self.batch_size):\n",
    "            return\n",
    "        self.eval_off()\n",
    "        if self.bag.size > 0:\n",
    "            (\n",
    "                obss,\n",
    "                actions,\n",
    "                rewards,\n",
    "                next_obss,\n",
    "                next_actions,\n",
    "                dones,\n",
    "                episode_lengths,\n",
    "                bag_obss,\n",
    "                bag_actions,\n",
    "            ) = self.replay_buffer.sample_with_bag(self.batch_size, self.bag)\n",
    "            # Bags: [batch-size x bag-size x obs-dim]\n",
    "            bag_obss = torch.as_tensor(\n",
    "                bag_obss, dtype=self.obs_tensor_type, device=self.device\n",
    "            )\n",
    "            bag_actions = torch.as_tensor(\n",
    "                bag_actions, dtype=torch.long, device=self.device\n",
    "            )\n",
    "        else:\n",
    "            (\n",
    "                obss,\n",
    "                actions,\n",
    "                rewards,\n",
    "                next_obss,\n",
    "                next_actions,\n",
    "                dones,\n",
    "                episode_lengths,\n",
    "            ) = self.replay_buffer.sample(self.batch_size)\n",
    "            bag_obss = None\n",
    "            bag_actions = None\n",
    "\n",
    "        # Obss and Next obss: [batch-size x hist-len x obs-dim]\n",
    "        obss = torch.as_tensor(obss, dtype=self.obs_tensor_type, device=self.device)\n",
    "        next_obss = torch.as_tensor(\n",
    "            next_obss, dtype=self.obs_tensor_type, device=self.device\n",
    "        )\n",
    "        # Actions: [batch-size x hist-len x 1]\n",
    "        actions = torch.as_tensor(actions, dtype=torch.long, device=self.device)\n",
    "        next_actions = torch.as_tensor(\n",
    "            next_actions, dtype=torch.long, device=self.device\n",
    "        )\n",
    "        # Rewards: [batch-size x hist-len x 1]\n",
    "        rewards = torch.as_tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        # Dones: [batch-size x hist-len x 1]\n",
    "        dones = torch.as_tensor(dones, dtype=torch.long, device=self.device)\n",
    "\n",
    "        # obss is [batch-size x hist-len x obs-len]\n",
    "        # then q_values is [batch-size x hist-len x n-actions]\n",
    "        q_values = self.policy_network(obss, actions, bag_obss, bag_actions)\n",
    "\n",
    "        # After gathering, Q values becomes [batch-size x hist-len x 1] then\n",
    "        # after squeeze becomes [batch-size x hist-len]\n",
    "        q_values = q_values.gather(2, actions).squeeze()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Next obss goes from [batch-size x hist-len x obs-dim] to\n",
    "            # [batch-size x hist-len x n-actions] and then goes through gather and squeeze\n",
    "            # to become [batch-size x hist-len]\n",
    "            if self.history:\n",
    "                argmax = torch.argmax(\n",
    "                    self.policy_network(next_obss, next_actions, bag_obss, bag_actions),\n",
    "                    dim=2,\n",
    "                ).unsqueeze(-1)\n",
    "                next_obs_q_values = self.target_network(\n",
    "                    next_obss, next_actions, bag_obss, bag_actions\n",
    "                )\n",
    "                next_obs_q_values = next_obs_q_values.gather(2, argmax).squeeze()\n",
    "\n",
    "            # here goes BELLMAN\n",
    "            targets = rewards.squeeze() + (1 - dones.squeeze()) * (\n",
    "                next_obs_q_values * self.gamma\n",
    "            )\n",
    "\n",
    "        q_values = q_values[:, -self.history :]\n",
    "        targets = targets[:, -self.history :]\n",
    "        # Calculate loss\n",
    "        loss = F.mse_loss(q_values, targets)\n",
    "        # Log Losses\n",
    "        self.qvalue_max.add(q_values.max().item())\n",
    "        self.qvalue_mean.add(q_values.mean().item())\n",
    "        self.qvalue_min.add(q_values.min().item())\n",
    "\n",
    "        self.target_max.add(targets.max().item())\n",
    "        self.target_mean.add(targets.mean().item())\n",
    "        self.target_min.add(targets.min().item())\n",
    "\n",
    "        self.td_errors.add(loss.item())\n",
    "        # Optimization step\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        norm = torch.nn.utils.clip_grad_norm_(\n",
    "            self.policy_network.parameters(),\n",
    "            self.grad_norm_clip,\n",
    "            error_if_nonfinite=True,\n",
    "        )\n",
    "        # Logging\n",
    "        self.grad_norms.add(norm.item())\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.num_train_steps += 1\n",
    "\n",
    "        if self.num_train_steps % self.target_update_frequency == 0:\n",
    "            self.target_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c16a1",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02d614b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MAP = {\n",
    "    \"DTQN\": DTQN,\n",
    "    \"DTQN-bag\": DTQN,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48475d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_MAP = {\n",
    "    \"DTQN\": DtqnAgent,\n",
    "    \"DTQN-bag\": DtqnAgent,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b32e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent(\n",
    "    model_str: str,\n",
    "    envs: Tuple[gym.Env],\n",
    "    embed_per_obs_dim: int,\n",
    "    action_dim: int,\n",
    "    inner_embed: int,\n",
    "    buffer_size: int,\n",
    "    device: torch.device,\n",
    "    learning_rate: float,\n",
    "    batch_size: int,\n",
    "    context_len: int,\n",
    "    max_env_steps: int,\n",
    "    history: int,\n",
    "    target_update_frequency: int,\n",
    "    gamma: float,\n",
    "    num_heads: int = 1,\n",
    "    num_layers: int = 1,\n",
    "    dropout: float = 0.0,\n",
    "    identity: bool = False,\n",
    "    gate: str = \"res\",\n",
    "    pos: str = \"learned\",\n",
    "    bag_size: int = 0,\n",
    "):\n",
    "    \"\"\"Function to create the agent. This will also set up the policy and target networks that the agent needs.\n",
    "    Arguments:\n",
    "        model_str: str, the name of the Q-function model we are going to use.\n",
    "        envs: Tuple[gym.Env], a list of gym environments the agent will train and evaluate on. They must all have the same observation and action space.\n",
    "        ember_per_obs_dim: int, the number of features to give each dimension of the observation. This is only used for discrete domains.\n",
    "        action_dim: int, the number of features to give each action.\n",
    "        inner_embed: int, the size of the main transformer model.\n",
    "        buffer_size: int, the number of transitions to store in the replay buffer.\n",
    "        device: torch.device, the device to use for training.\n",
    "        learning_rate: float, the learning rate for the ADAM optimiser.\n",
    "        batch_size: int, the batch size to use for training.\n",
    "        context_len: int, the maximum sequence length to use as input to the network.\n",
    "        max_env_steps: int, the maximum number of steps allowed in the environment before timeout. This will be inferred if not explicitly supplied.\n",
    "        history: int, the number of Q-values to use during training for each sample.\n",
    "        target_update_frequency: int, the number of training steps between (hard) target network update.\n",
    "        gamma: float, the discount factor.\n",
    "        -DTQN-Specific-\n",
    "        num_heads: int, the number of heads to use in the MultiHeadAttention.\n",
    "        num_layers: int, the number of transformer blocks to use.\n",
    "        dropout: float, the dropout percentage to use.\n",
    "        identity: bool, whether or not to use identity map reordering.\n",
    "        gate: str, which combine step to use (residual skip connection or GRU)\n",
    "        pos: str, which type of position encoding to use (\"learned\", \"sin\", or \"none\")\n",
    "        bag_size: int, the size of the persistent memory bag\n",
    "\n",
    "    Returns:\n",
    "        the agent we created with all those arguments, complete with replay buffer, context, policy and target network.\n",
    "    \"\"\"\n",
    "    # All envs must have the same observation shape\n",
    "    env_obs_length = env_processing.get_env_obs_length(envs[0])\n",
    "    env_obs_mask = env_processing.get_env_obs_mask(envs[0])\n",
    "    if max_env_steps <= 0:\n",
    "        max_env_steps = max([env_processing.get_env_max_steps(env) for env in envs])\n",
    "    if isinstance(env_obs_mask, np.ndarray):\n",
    "        obs_vocab_size = env_obs_mask.max() + 1\n",
    "    else:\n",
    "        obs_vocab_size = env_obs_mask + 1\n",
    "    is_discrete_env = isinstance(\n",
    "        envs[0].observation_space,\n",
    "        (gym.spaces.Discrete, gym.spaces.MultiDiscrete, gym.spaces.MultiBinary),\n",
    "    )\n",
    "    # Keep the history between 1 and context length\n",
    "    if history < 1 or history > context_len:\n",
    "        print(\n",
    "            f\"History must be 1 < history <= context_len, but history is {history} and context len is {context_len}. Clipping history to {np.clip(history, 1, context_len)}...\"\n",
    "        )\n",
    "        history = np.clip(history, 1, context_len)\n",
    "    # All envs must share same action space\n",
    "    num_actions = envs[0].action_space.n\n",
    "\n",
    "    def make_dtqn(network_cls):\n",
    "        \"\"\"Creates DTQN\"\"\"\n",
    "        return lambda: network_cls(\n",
    "            env_obs_length,\n",
    "            num_actions,\n",
    "            embed_per_obs_dim,\n",
    "            action_dim,\n",
    "            inner_embed,\n",
    "            num_heads,\n",
    "            num_layers,\n",
    "            context_len,\n",
    "            dropout=dropout,\n",
    "            gate=gate,\n",
    "            identity=identity,\n",
    "            pos=pos,\n",
    "            discrete=is_discrete_env,\n",
    "            vocab_sizes=obs_vocab_size,\n",
    "            target_update_frequency=target_update_frequency,\n",
    "            bag_size=bag_size,\n",
    "        ).to(device)\n",
    "    network_factory = make_dtqn(MODEL_MAP[model_str])\n",
    "    return AGENT_MAP[model_str](\n",
    "        network_factory,\n",
    "        buffer_size,\n",
    "        device,\n",
    "        env_obs_length,\n",
    "        max_env_steps,\n",
    "        env_obs_mask,\n",
    "        num_actions,\n",
    "        is_discrete_env,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        gamma=gamma,\n",
    "        context_len=context_len,\n",
    "        embed_size=inner_embed,\n",
    "        history=history,\n",
    "        target_update_frequency=target_update_frequency,\n",
    "        bag_size=bag_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d31a2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bag:\n",
    "    \"\"\"A Dataclass dedicated to storing important observations that would have fallen out of the agent's context\n",
    "\n",
    "    Args:\n",
    "        bag_size: Size of bag\n",
    "        obs_mask: The mask to use to indicate the observation is padding\n",
    "        obs_length: shape of an observation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bag_size: int, obs_mask: Union[int, float], obs_length: int):\n",
    "        self.size = bag_size\n",
    "        self.obs_mask = obs_mask\n",
    "        self.obs_length = obs_length\n",
    "        # Current position in bag\n",
    "        self.pos = 0\n",
    "\n",
    "        self.obss, self.actions = self.make_empty_bag()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.pos = 0\n",
    "        self.obss, self.actions = self.make_empty_bag()\n",
    "\n",
    "    def add(self, obs: np.ndarray, action: int) -> bool:\n",
    "        if not self.is_full:\n",
    "            self.obss[self.pos] = obs\n",
    "            self.actions[self.pos] = action\n",
    "            self.pos += 1\n",
    "            return True\n",
    "        else:\n",
    "            # Reject adding the observation-action\n",
    "            return False\n",
    "\n",
    "    def export(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return self.obss[: self.pos], self.actions[: self.pos]\n",
    "\n",
    "    def make_empty_bag(self) -> np.ndarray:\n",
    "        # Image\n",
    "        if isinstance(self.obs_length, tuple):\n",
    "            return np.full((self.size, *self.obs_length), self.obs_mask), np.full(\n",
    "                (self.size, 1), 0\n",
    "            )\n",
    "        # Non-Image\n",
    "        else:\n",
    "            return np.full((self.size, self.obs_length), self.obs_mask), np.full(\n",
    "                (self.size, 1), 0\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def is_full(self) -> bool:\n",
    "        return self.pos >= self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "646c2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noinspection PyAttributeOutsideInit\n",
    "class Context:\n",
    "    \"\"\"A Dataclass dedicated to storing the agent's history (up to the previous `max_length` transitions)\n",
    "\n",
    "    Args:\n",
    "        context_length: The maximum number of transitions to store\n",
    "        obs_mask: The mask to use for observations not yet seen\n",
    "        num_actions: The number of possible actions we can take in the environment\n",
    "        env_obs_length: The dimension of the observations (assume 1d arrays)\n",
    "        init_hidden: The initial value of the hidden states (used for RNNs)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_length: int,\n",
    "        obs_mask: int,\n",
    "        num_actions: int,\n",
    "        env_obs_length: int,\n",
    "        init_hidden: Tuple[torch.Tensor] = None,\n",
    "    ):\n",
    "        self.max_length = context_length\n",
    "        self.env_obs_length = env_obs_length\n",
    "        self.num_actions = num_actions\n",
    "        self.obs_mask = obs_mask\n",
    "        self.reward_mask = 0.0\n",
    "        self.done_mask = True\n",
    "        self.timestep = 0\n",
    "        self.init_hidden = init_hidden\n",
    "\n",
    "    def reset(self, obs: np.ndarray):\n",
    "        \"\"\"Resets to a fresh context\"\"\"\n",
    "        # Account for images\n",
    "        if isinstance(self.env_obs_length, tuple):\n",
    "            self.obs = np.full(\n",
    "                [self.max_length, *self.env_obs_length],\n",
    "                self.obs_mask,\n",
    "                dtype=np.uint8,\n",
    "            )\n",
    "        else:\n",
    "            self.obs = np.full([self.max_length, self.env_obs_length], self.obs_mask)\n",
    "        # Initial observation\n",
    "        self.obs[0] = obs\n",
    "\n",
    "        self.action = RNG.rng.integers(self.num_actions, size=(self.max_length, 1))\n",
    "        self.reward = np.full_like(self.action, self.reward_mask)\n",
    "        self.done = np.full_like(self.reward, self.done_mask, dtype=np.int32)\n",
    "        self.hidden = self.init_hidden\n",
    "        self.timestep = 0\n",
    "\n",
    "    def add_transition(\n",
    "        self, o: np.ndarray, a: int, r: float, done: bool\n",
    "    ) -> Tuple[Union[np.ndarray, None], Union[int, None]]:\n",
    "        \"\"\"Add an entire transition. If the context is full, evict the oldest transition\"\"\"\n",
    "        self.timestep += 1\n",
    "        self.obs = self.roll(self.obs)\n",
    "        self.action = self.roll(self.action)\n",
    "        self.reward = self.roll(self.reward)\n",
    "        self.done = self.roll(self.done)\n",
    "\n",
    "        t = min(self.timestep, self.max_length - 1)\n",
    "\n",
    "        # If we are going to evict an observation, we need to return it for possibly adding to the bag\n",
    "        evicted_obs = None\n",
    "        evicted_action = None\n",
    "        if self.is_full:\n",
    "            evicted_obs = self.obs[t].copy()\n",
    "            evicted_action = self.action[t]\n",
    "\n",
    "        self.obs[t] = o\n",
    "        self.action[t] = np.array([a])\n",
    "        self.reward[t] = np.array([r])\n",
    "        self.done[t] = np.array([done])\n",
    "\n",
    "        return evicted_obs, evicted_action\n",
    "\n",
    "    def export(\n",
    "        self,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Export the context\"\"\"\n",
    "        current_timestep = min(self.timestep, self.max_length) - 1\n",
    "        return (\n",
    "            self.obs[current_timestep + 1],\n",
    "            self.action[current_timestep],\n",
    "            self.reward[current_timestep],\n",
    "            self.done[current_timestep],\n",
    "        )\n",
    "\n",
    "    def roll(self, arr: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Utility function to help with insertions at the end of the array. If the context is full, we replace the first element with the new element, then 'roll' the new element to the end of the array\"\"\"\n",
    "        return np.roll(arr, -1, axis=0) if self.timestep >= self.max_length else arr\n",
    "\n",
    "    @property\n",
    "    def is_full(self) -> bool:\n",
    "        return self.timestep >= self.max_length\n",
    "\n",
    "    @staticmethod\n",
    "    def context_like(context):\n",
    "        \"\"\"Creates a new context to mimic the supplied context\"\"\"\n",
    "        return Context(\n",
    "            context.max_length,\n",
    "            context.obs_mask,\n",
    "            context.num_actions,\n",
    "            context.env_obs_length,\n",
    "            init_hidden=context.init_hidden,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34b3f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTQNEnvironment(EnvBase):\n",
    "    def __init__(self, A, B, C, D, dt, ref=1, device=\"cpu\"):\n",
    "        super(DTQNEnvironment, self).__init__() # call the constructor of the base class\n",
    "        \n",
    "        # custom property intialization - unique to this environment\n",
    "        self.dtype = np.float32\n",
    "\n",
    "        self.A, self.B, self.C, self.D, self.dt, self.ref = A, B, C, D, dt, ref\n",
    "        self.device = device # where does the outgoing data go?\n",
    "\n",
    "        self.state_size = self.A.shape[0] # how many in the first dimension\n",
    "        self.action_size = self.B.shape[1] # how many in the second dimension\n",
    "\n",
    "        self.state = np.zeros((self.state_size, 1), dtype=self.dtype)\n",
    "        \n",
    "        # specs - needs to be initialized\n",
    "        self.action_spec = BoundedTensorSpec(minimum=-1, maximum=1, shape=torch.Size([self.action_size])) # limit the action values\n",
    "\n",
    "        #this is the requirement of the abstract class but state = observation so it is never        \n",
    "        observation_spec = UnboundedContinuousTensorSpec(shape=torch.Size([self.state_size])) # unlimited observation space\n",
    "        self.observation_spec = CompositeSpec(observation=observation_spec) # has to be CompositeSpec(not sure why)\n",
    "\n",
    "        self.reward_spec = UnboundedContinuousTensorSpec(shape=torch.Size([1])) # unlimited reward space(even though we could limit it to (-inf, 0] in this particular example)\n",
    "\n",
    "    def _reset(self, tensordict, **kwargs):\n",
    "        \n",
    "        # init new state and pack it up in a tensordict\n",
    "        \n",
    "        out_tensordict = TensorDict({}, batch_size=torch.Size())\n",
    "\n",
    "        self.state = np.zeros((self.state_size, 1), dtype=self.dtype)\n",
    "        out_tensordict.set(\"observation\", torch.tensor(self.state.flatten(), device=self.device))\n",
    "\n",
    "        return out_tensordict\n",
    "\n",
    "    def _step(self, tensordict):\n",
    "        #needs to be changed\n",
    "        action = tensordict[\"action\"]\n",
    "        action = action.cpu().numpy().reshape((self.action_size, 1))\n",
    "\n",
    "        self.state += self.dt * (self.A @ self.state + self.B @ action)\n",
    "\n",
    "        y = self.C @ self.state + self.D @ action\n",
    "\n",
    "        error = (self.ref - y) ** 2\n",
    "\n",
    "        reward = -error\n",
    "\n",
    "        out_tensordict = TensorDict({\"observation\": torch.tensor(self.state.astype(self.dtype).flatten(), device=self.device),\n",
    "                                     \"reward\": torch.tensor(reward.astype(np.float32), device=self.device),\n",
    "                                     \"done\": False}, batch_size=torch.Size())\n",
    "\n",
    "        return out_tensordict\n",
    "\n",
    "    def _set_seed(self, seed):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d744bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_obs_length(env: DTQNEnvironment) -> int:\n",
    "    \"\"\"Gets the length of the observations in an environment\"\"\"\n",
    "    if isinstance(env.state_spec): \n",
    "        return env.state_size\n",
    "    else:\n",
    "        raise NotImplementedError(f\"We do not yet support {env.observation_space}\") #if nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a33c8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_obs_mask(env: DTQNEnvironment) -> Union[int, np.ndarray]:\n",
    "    \"\"\"Gets the number of observations possible (for discrete case).\n",
    "    For continuous case, please edit the -5 to something lower than\n",
    "    lowest possible observation (while still being finite) so the\n",
    "    network knows it is padding.\n",
    "    \"\"\"\n",
    "    # changed to a variable (in agent utils) when creating the agent, passed in as a obs_vocab_size\n",
    "    if isinstance(env.observation_space): # find the lowest possible indice that is realistic\n",
    "        # If you would like to use DTQN with a continuous action space, make sure this value is\n",
    "        #       below the minimum possible observation. Otherwise it will appear as a real observation\n",
    "        #       to the network which may cause issues. In our case, Car Flag has min of -1 so this is\n",
    "        #       fine.\n",
    "        # find the lowest indice\n",
    "        return -5\n",
    "    else:\n",
    "        raise NotImplementedError(f\"We do not yet support {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e05f1bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_max_steps(env: DTQNEnvironment) -> Union[int, None]:\n",
    "    \"\"\"Gets the maximum steps allowed in an episode before auto-terminating\"\"\"\n",
    "    try:\n",
    "        return env._max_episode_steps\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            return env.max_episode_steps\n",
    "        except AttributeError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d8085af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonAnneal(ABC):\n",
    "    @abstractmethod\n",
    "    def anneal(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36bc828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constant(EpsilonAnneal):\n",
    "    def __init__(self, start):\n",
    "        self.val = start\n",
    "\n",
    "    def anneal(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a391704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAnneal(EpsilonAnneal):\n",
    "    \"\"\"Linear Annealing Schedule.\n",
    "\n",
    "    Args:\n",
    "        start:      The initial value of epsilon.\n",
    "        end:        The final value of epsilon.\n",
    "        duration:   The number of anneals from start value to end value.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start: float, end: float, duration: int):\n",
    "        self.val = start\n",
    "        self.min = end\n",
    "        self.duration = duration\n",
    "\n",
    "    def anneal(self):\n",
    "        self.val = max(self.min, self.val - (self.val - self.min) / self.duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "237964b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.q = deque()\n",
    "        self.sum = 0\n",
    "\n",
    "    def add(self, val):\n",
    "        self.q.append(val)\n",
    "        self.sum += val\n",
    "        if len(self.q) > self.size:\n",
    "            self.sum -= self.q.popleft()\n",
    "\n",
    "    def mean(self):\n",
    "        # Avoid divide by 0\n",
    "        return self.sum / max(len(self.q), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b64507d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp():\n",
    "    return datetime.now().strftime(\"%B %d, %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f5c9e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_init(config, group_keys, **kwargs) -> None:\n",
    "    wandb.init(\n",
    "        project=config[\"project_name\"],\n",
    "        group=\"_\".join(\n",
    "            [f\"{key}={val}\" for key, val in config.items() if key in group_keys]\n",
    "        ),\n",
    "        config=config,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "203b5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVLogger:\n",
    "    \"\"\"Logger to write results to a CSV. The log function matches that of Weights and Biases.\n",
    "\n",
    "    Args:\n",
    "        path: path for the csv results file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str, args: argparse.Namespace):\n",
    "        self.results_path = path + \"_results.csv\"\n",
    "        self.losses_path = path + \"_losses.csv\"\n",
    "        self.envs = args.envs\n",
    "        # If we have a checkpoint, we don't want to overwrite\n",
    "        if not os.path.exists(self.results_path):\n",
    "            head_row = [\"Hours\", \"Step\"]\n",
    "            for env in self.envs:\n",
    "                head_row += [\n",
    "                    f\"{env}/SuccessRate\",\n",
    "                    f\"{env}/EpisodeLength\",\n",
    "                    f\"{env}/Return\",\n",
    "                ]\n",
    "            with open(self.results_path, \"w\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(head_row)\n",
    "        if not os.path.exists(self.losses_path):\n",
    "            with open(self.losses_path, \"w\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(\n",
    "                    [\n",
    "                        \"Hours\",\n",
    "                        \"Step\",\n",
    "                        \"TD Error\",\n",
    "                        \"Grad Norm\",\n",
    "                        \"Max Q Value\",\n",
    "                        \"Mean Q Value\",\n",
    "                        \"Min Q Value\",\n",
    "                        \"Max Target Value\",\n",
    "                        \"Mean Target Value\",\n",
    "                        \"Min Target Value\",\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "    def log(self, results: Dict[str, str], step: int):\n",
    "        results_row = [results[\"losses/hours\"], step]\n",
    "        for env in self.envs:\n",
    "            results_row += [\n",
    "                results[f\"{env}/SuccessRate\"],\n",
    "                results[f\"{env}/EpisodeLength\"],\n",
    "                results[f\"{env}/Return\"],\n",
    "            ]\n",
    "        with open(self.results_path, \"a\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(results_row)\n",
    "        with open(self.losses_path, \"a\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    results[\"losses/hours\"],\n",
    "                    step,\n",
    "                    results[\"losses/TD_Error\"],\n",
    "                    results[\"losses/Grad_Norm\"],\n",
    "                    results[\"losses/Max_Q_Value\"],\n",
    "                    results[\"losses/Mean_Q_Value\"],\n",
    "                    results[\"losses/Min_Q_Value\"],\n",
    "                    results[\"losses/Max_Target_Value\"],\n",
    "                    results[\"losses/Mean_Target_Value\"],\n",
    "                    results[\"losses/Min_Target_Value\"],\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edc1e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(\n",
    "    policy_path: str, args: argparse.Namespace, wandb_kwargs: Dict[str, str]\n",
    "):\n",
    "    if args.disable_wandb:\n",
    "        logger = CSVLogger(policy_path, args)\n",
    "    else:\n",
    "        wandb_init(\n",
    "            vars(args),\n",
    "            [\n",
    "                \"model\",\n",
    "                \"obs_embed\",\n",
    "                \"a_embed\",\n",
    "                \"in_embed\",\n",
    "                \"context\",\n",
    "                \"layers\",\n",
    "                \"bag_size\",\n",
    "                \"gate\",\n",
    "                \"identity\",\n",
    "                \"history\",\n",
    "                \"pos\",\n",
    "            ],\n",
    "            **wandb_kwargs,\n",
    "        )\n",
    "        logger = wandb\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21ee289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNG:\n",
    "    rng: np.random.Generator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61ffef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int, *args: Tuple[gym.Env]) -> None:\n",
    "    \"\"\"Sets seed for PyTorch, NumPy, and random.\n",
    "\n",
    "    Args:\n",
    "        seed: The random seed to use.\n",
    "        args: The gym environment(s) to seed.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    tseed = random.randint(1, 1e6)\n",
    "    npseed = random.randint(1, 1e6)\n",
    "    ospyseed = random.randint(1, 1e6)\n",
    "    torch.manual_seed(tseed)\n",
    "    np.random.seed(npseed)\n",
    "    for env in args:\n",
    "        env.seed(seed=seed)\n",
    "        env.observation_space.seed(seed=seed)\n",
    "        env.action_space.seed(seed=seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(ospyseed)\n",
    "    RNG.rng = np.random.Generator(np.random.PCG64(seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aea43158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_utils\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77363237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "    elif isinstance(module, nn.MultiheadAttention):\n",
    "        module.in_proj_weight.data.normal_(mean=0.0, std=0.02)\n",
    "        module.out_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        module.in_proj_bias.data.zero_()\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5122fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    FIFO Replay Buffer which stores contexts of length ``context_len`` rather than single\n",
    "        transitions\n",
    "\n",
    "    Args:\n",
    "        buffer_size: The number of transitions to store in the replay buffer\n",
    "        env_obs_length: The size (length) of the environment's observation\n",
    "        context_len: The number of transitions that will be stored as an agent's context. Default: 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int,\n",
    "        env_obs_length: Union[int, Tuple],\n",
    "        obs_mask: int,\n",
    "        max_episode_steps: int,\n",
    "        context_len: Optional[int] = 1,\n",
    "    ):\n",
    "        self.max_size = buffer_size // max_episode_steps\n",
    "        self.context_len = context_len\n",
    "        self.env_obs_length = env_obs_length\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        self.obs_mask = obs_mask\n",
    "        self.pos = [0, 0]\n",
    "\n",
    "        # Image domains\n",
    "        if isinstance(env_obs_length, tuple):\n",
    "            self.obss = np.full(\n",
    "                [\n",
    "                    self.max_size,\n",
    "                    max_episode_steps + 1,  # Keeps first and last obs together for +1\n",
    "                    *env_obs_length,\n",
    "                ],\n",
    "                obs_mask,\n",
    "                dtype=np.uint8,\n",
    "            )\n",
    "        else:\n",
    "            self.obss = np.full(\n",
    "                [\n",
    "                    self.max_size,\n",
    "                    max_episode_steps + 1,  # Keeps first and last obs together for +1\n",
    "                    env_obs_length,\n",
    "                ],\n",
    "                obs_mask,\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "\n",
    "        # Need the +1 so we have space to roll for the first observation\n",
    "        self.actions = np.zeros(\n",
    "            [self.max_size, max_episode_steps + 1, 1],\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "        self.rewards = np.zeros(\n",
    "            [self.max_size, max_episode_steps, 1],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.dones = np.ones(\n",
    "            [self.max_size, max_episode_steps, 1],\n",
    "            dtype=np.bool_,\n",
    "        )\n",
    "        self.episode_lengths = np.zeros([self.max_size], dtype=np.uint8)\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        reward: np.ndarray,\n",
    "        done: np.ndarray,\n",
    "        episode_length: Optional[int] = 0,\n",
    "    ) -> None:\n",
    "        episode_idx = self.pos[0] % self.max_size\n",
    "        obs_idx = self.pos[1]\n",
    "        self.obss[episode_idx, obs_idx + 1] = obs\n",
    "        self.actions[episode_idx, obs_idx] = action\n",
    "        self.rewards[episode_idx, obs_idx] = reward\n",
    "        self.dones[episode_idx, obs_idx] = done\n",
    "        self.episode_lengths[episode_idx] = episode_length\n",
    "        self.pos = [self.pos[0], self.pos[1] + 1]\n",
    "\n",
    "    def store_obs(self, obs: np.ndarray) -> None:\n",
    "        \"\"\"Use this at the beginning of the episode to store the first obs\"\"\"\n",
    "        episode_idx = self.pos[0] % self.max_size\n",
    "        self.cleanse_episode(episode_idx)\n",
    "        self.obss[episode_idx, 0] = obs\n",
    "\n",
    "    def can_sample(self, batch_size: int) -> bool:\n",
    "        return batch_size < self.pos[0]\n",
    "\n",
    "    def flush(self):\n",
    "        self.pos = [self.pos[0] + 1, 0]\n",
    "\n",
    "    def cleanse_episode(self, episode_idx: int) -> None:\n",
    "        # Cleanse the episode of any previous data\n",
    "        # Image domains\n",
    "        if isinstance(self.env_obs_length, tuple):\n",
    "            self.obss[episode_idx] = np.full(\n",
    "                [\n",
    "                    self.max_episode_steps\n",
    "                    + 1,  # Keeps first and last obs together for +1\n",
    "                    *self.env_obs_length,\n",
    "                ],\n",
    "                self.obs_mask,\n",
    "                dtype=np.uint8,\n",
    "            )\n",
    "        else:\n",
    "            self.obss[episode_idx] = np.full(\n",
    "                [\n",
    "                    self.max_episode_steps\n",
    "                    + 1,  # Keeps first and last obs together for +1\n",
    "                    self.env_obs_length,\n",
    "                ],\n",
    "                self.obs_mask,\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "        self.actions[episode_idx] = np.zeros(\n",
    "            [self.max_episode_steps + 1, 1],\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "        self.rewards[episode_idx] = np.zeros(\n",
    "            [self.max_episode_steps, 1],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.dones[episode_idx] = np.ones(\n",
    "            [self.max_episode_steps, 1],\n",
    "            dtype=np.bool_,\n",
    "        )\n",
    "        self.episode_lengths[episode_idx] = 0\n",
    "\n",
    "    def sample(\n",
    "        self, batch_size: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        # Exclude the current episode we're in\n",
    "        valid_episodes = [\n",
    "            i\n",
    "            for i in range(min(self.pos[0], self.max_size))\n",
    "            if i != self.pos[0] % self.max_size\n",
    "        ]\n",
    "        episode_idxes = np.array(\n",
    "            [[random.choice(valid_episodes)] for _ in range(batch_size)]\n",
    "        )\n",
    "        transition_starts = np.array(\n",
    "            [\n",
    "                random.randint(\n",
    "                    0, max(0, self.episode_lengths[idx[0]] - self.context_len)\n",
    "                )\n",
    "                for idx in episode_idxes\n",
    "            ]\n",
    "        )\n",
    "        transitions = np.array(\n",
    "            [range(start, start + self.context_len) for start in transition_starts]\n",
    "        )\n",
    "        return (\n",
    "            self.obss[episode_idxes, transitions],\n",
    "            self.actions[episode_idxes, transitions],\n",
    "            self.rewards[episode_idxes, transitions],\n",
    "            self.obss[episode_idxes, 1 + transitions],\n",
    "            self.actions[episode_idxes, 1 + transitions],\n",
    "            self.dones[episode_idxes, transitions],\n",
    "            np.clip(self.episode_lengths[episode_idxes], 0, self.context_len),\n",
    "        )\n",
    "\n",
    "    # TODO:\n",
    "    def sample_with_bag(\n",
    "        self, batch_size: int, sample_bag: Bag\n",
    "    ) -> Tuple[\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "    ]:\n",
    "        episode_idxes = np.array(\n",
    "            [\n",
    "                # Exclude the current episode we're in\n",
    "                [\n",
    "                    random.choice(\n",
    "                        [\n",
    "                            i\n",
    "                            for i in range(min(self.pos[0], self.max_size))\n",
    "                            if i != self.pos[0]\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "                for _ in range(batch_size)\n",
    "            ]\n",
    "        )\n",
    "        transition_starts = np.array(\n",
    "            [\n",
    "                random.randint(\n",
    "                    0, max(0, self.episode_lengths[idx[0]] - self.context_len)\n",
    "                )\n",
    "                for idx in episode_idxes\n",
    "            ]\n",
    "        )\n",
    "        transitions = np.array(\n",
    "            [range(start, start + self.context_len) for start in transition_starts]\n",
    "        )\n",
    "\n",
    "        # Create `batch_size` replica bags\n",
    "        bag_obss = np.full(\n",
    "            [batch_size, sample_bag.size, sample_bag.obs_length],\n",
    "            sample_bag.obs_mask,\n",
    "        )\n",
    "        bag_actions = np.full(\n",
    "            [batch_size, sample_bag.size, 1],\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        # Sample from the bag with observations that won't be in the main context\n",
    "        for bag_idx in range(batch_size):\n",
    "            # Possible bag is smaller than max bag size, so take all of it\n",
    "            if transition_starts[bag_idx] < sample_bag.size:\n",
    "                bag_obss[bag_idx, : transition_starts[bag_idx]] = self.obss[\n",
    "                    episode_idxes[bag_idx], : transition_starts[bag_idx]\n",
    "                ]\n",
    "                bag_actions[bag_idx, : transition_starts[bag_idx]] = self.actions[\n",
    "                    episode_idxes[bag_idx], : transition_starts[bag_idx]\n",
    "                ]\n",
    "            # Otherwise, randomly sample\n",
    "            else:\n",
    "                bag_obss[bag_idx] = np.array(\n",
    "                    random.sample(\n",
    "                        self.obss[episode_idxes[bag_idx], : transition_starts[bag_idx]]\n",
    "                        .squeeze()\n",
    "                        .tolist(),\n",
    "                        k=sample_bag.size,\n",
    "                    )\n",
    "                )\n",
    "                bag_actions[bag_idx] = np.expand_dims(\n",
    "                    np.array(\n",
    "                        random.sample(\n",
    "                            self.actions[\n",
    "                                episode_idxes[bag_idx], : transition_starts[bag_idx]\n",
    "                            ]\n",
    "                            .squeeze()\n",
    "                            .tolist(),\n",
    "                            k=sample_bag.size,\n",
    "                        )\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "        return (\n",
    "            self.obss[episode_idxes, transitions],\n",
    "            self.actions[episode_idxes, transitions],\n",
    "            self.rewards[episode_idxes, transitions],\n",
    "            self.obss[episode_idxes, 1 + transitions],\n",
    "            self.actions[episode_idxes, 1 + transitions],\n",
    "            self.dones[episode_idxes, transitions],\n",
    "            self.episode_lengths[episode_idxes],\n",
    "            bag_obss,\n",
    "            bag_actions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f4f7b9",
   "metadata": {},
   "source": [
    "# Run Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe40a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    agent,\n",
    "    eval_env: Env,\n",
    "    eval_episodes: int,\n",
    "    render: Optional[bool] = None,\n",
    "):\n",
    "    \"\"\"Evaluate the network for n_episodes using a greedy policy.\n",
    "\n",
    "    Arguments:\n",
    "        agent:          the agent to evaluate.\n",
    "        eval_env:       gym.Env, the environment to use for the evaluation.\n",
    "        eval_episodes:  int, the number of episodes to run.\n",
    "        render:         bool, whether or not to render the timesteps for enjoy mode.\n",
    "\n",
    "    Returns:\n",
    "        mean_success:           float, number of successes divided by number of episodes.\n",
    "        mean_return:            float, the average return per episode.\n",
    "        mean_episode_length:    float, the average episode length.\n",
    "    \"\"\"\n",
    "    # Set networks to eval mode (turns off dropout, etc.)\n",
    "    agent.eval_on()\n",
    "\n",
    "    total_reward = 0\n",
    "    num_successes = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for _ in range(eval_episodes):\n",
    "        agent.context_reset(eval_env.reset())\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        if render:\n",
    "            eval_env.render()\n",
    "            sleep(0.5)\n",
    "        while not done:\n",
    "            action = agent.get_action(epsilon=0.0)\n",
    "            obs_next, reward, done, info = eval_env.step(action)\n",
    "            agent.observe(obs_next, action, reward, done)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                eval_env.render()\n",
    "                if done:\n",
    "                    print(f\"Episode terminated. Episode reward: {ep_reward}\")\n",
    "                sleep(0.5)\n",
    "        total_reward += ep_reward\n",
    "        total_steps += agent.context.timestep\n",
    "        if info.get(\"is_success\", False) or ep_reward > 0:\n",
    "            num_successes += 1\n",
    "\n",
    "    # Set networks back to train mode\n",
    "    agent.eval_off()\n",
    "    # Prevent divide by 0\n",
    "    episodes = max(eval_episodes, 1)\n",
    "    return (\n",
    "        num_successes / episodes,\n",
    "        total_reward / episodes,\n",
    "        total_steps / episodes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    agent,\n",
    "    envs: Tuple[Env],\n",
    "    eval_envs: Tuple[Env],\n",
    "    env_strs: Tuple[str],\n",
    "    total_steps: int,\n",
    "    eps: epsilon_anneal.EpsilonAnneal,\n",
    "    eval_frequency: int,\n",
    "    eval_episodes: int,\n",
    "    policy_path: str,\n",
    "    save_policy: bool,\n",
    "    logger,\n",
    "    mean_success_rate: RunningAverage,\n",
    "    mean_episode_length: RunningAverage,\n",
    "    mean_reward: RunningAverage,\n",
    "    time_remaining: Optional[int],\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Train the agent.\n",
    "\n",
    "    Arguments:\n",
    "        agent:              the agent to train.\n",
    "        envs:               Tuple[gym.Env], the list of envs to train on.\n",
    "        eval_envs:          Tuple[gym.Env], the list of envs to evaluate with.\n",
    "        env_strs:           Tuple[str], the list of environment names.\n",
    "        total_steps:        int, the total number of timesteps to train.\n",
    "        eps:                EpsilonAnneal, the schedule to set for epsilon throughout training.\n",
    "        eval_frequency:     int, the number of training steps between evaluation periods.\n",
    "        eval_episodes:      int, the number of episodes to evaluate on for each eval period.\n",
    "        policy_path:        str, the path to store the policy and checkpoints at.\n",
    "        logger:             the logger to use (either wandb or csv).\n",
    "        mean_success_rate:  RunningAverage, the success rate over several evaluation periods.\n",
    "        mean_episode_length:RunningAverage, the episode length over several evaluation periods.\n",
    "        mean_reward:        RunningAverage, the episodic return over several evaluation periods.\n",
    "        time_remaining:     int, if using time limits, the amount of time left since starting the job.\n",
    "        verbose:            bool, whether or not to print updates to standard out.\n",
    "    \"\"\"\n",
    "    start_time = time()\n",
    "    # Turn on train mode\n",
    "    agent.eval_off()\n",
    "    # Choose an environment at the start and on every episode reset.\n",
    "    env = RNG.rng.choice(envs)\n",
    "    agent.context_reset(env.reset())\n",
    "\n",
    "    for timestep in range(agent.num_train_steps, total_steps):\n",
    "        done = step(agent, env, eps)\n",
    "\n",
    "        if done:\n",
    "            agent.replay_buffer.flush()\n",
    "            env = RNG.rng.choice(envs)\n",
    "            agent.context_reset(env.reset())\n",
    "        agent.train()\n",
    "        eps.anneal()\n",
    "\n",
    "        if timestep % eval_frequency == 0:\n",
    "            hours = (time() - start_time) / 3600\n",
    "            # Log training values\n",
    "            log_vals = {\n",
    "                \"losses/TD_Error\": agent.td_errors.mean(),\n",
    "                \"losses/Grad_Norm\": agent.grad_norms.mean(),\n",
    "                \"losses/Max_Q_Value\": agent.qvalue_max.mean(),\n",
    "                \"losses/Mean_Q_Value\": agent.qvalue_mean.mean(),\n",
    "                \"losses/Min_Q_Value\": agent.qvalue_min.mean(),\n",
    "                \"losses/Max_Target_Value\": agent.target_max.mean(),\n",
    "                \"losses/Mean_Target_Value\": agent.target_mean.mean(),\n",
    "                \"losses/Min_Target_Value\": agent.target_min.mean(),\n",
    "                \"losses/hours\": hours,\n",
    "            }\n",
    "            # Perform an evaluation for each of the eval environments and add to our log\n",
    "            for env_str, eval_env in zip(env_strs, eval_envs):\n",
    "                sr, ret, length = evaluate(agent, eval_env, eval_episodes)\n",
    "\n",
    "                log_vals.update(\n",
    "                    {\n",
    "                        f\"{env_str}/SuccessRate\": sr,\n",
    "                        f\"{env_str}/Return\": ret,\n",
    "                        f\"{env_str}/EpisodeLength\": length,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Commit the log values.\n",
    "            logger.log(\n",
    "                log_vals,\n",
    "                step=timestep,\n",
    "            )\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"[ {timestamp()} ] Training Steps: {timestep}, Env: {env_str}, Success Rate: {sr:.2f}, Return: {ret:.2f}, Episode Length: {length:.2f}, Hours: {hours:.2f}\"\n",
    "                )\n",
    "\n",
    "        if save_policy and timestep % 50_000 == 0:\n",
    "            torch.save(agent.policy_network.state_dict(), policy_path)\n",
    "\n",
    "        if time_remaining and time() - start_time >= time_remaining:\n",
    "            print(\n",
    "                f\"Reached time limit. Saving checkpoint with {agent.num_train_steps} steps completed.\"\n",
    "            )\n",
    "\n",
    "            agent.save_checkpoint(\n",
    "                policy_path,\n",
    "                wandb.run.id if logger == wandb else None,\n",
    "                mean_success_rate,\n",
    "                mean_reward,\n",
    "                mean_episode_length,\n",
    "                eps,\n",
    "            )\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c8b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(agent, env: Env, eps: float) -> bool:\n",
    "    \"\"\"Use the agent's policy to get the next action, take it, and then record the result.\n",
    "\n",
    "    Arguments:\n",
    "        agent:  the agent to use.\n",
    "        env:    gym.Env\n",
    "        eps:    the epsilon value (for epsilon-greedy policy)\n",
    "\n",
    "    Returns:\n",
    "        done: bool, whether or not the episode has finished.\n",
    "    \"\"\"\n",
    "    action = agent.get_action(epsilon=eps.val)\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    # OpenAI Gym TimeLimit truncation: don't store it in the buffer as done\n",
    "    if info.get(\"TimeLimit.truncated\", False):\n",
    "        buffer_done = False\n",
    "    else:\n",
    "        buffer_done = done\n",
    "\n",
    "    agent.observe(next_obs, action, reward, buffer_done)\n",
    "    return done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e80ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepopulate(agent, prepop_steps: int, envs: Tuple[Env]) -> None:\n",
    "    \"\"\"Prepopulate the replay buffer. Sample an enviroment on each episode.\n",
    "\n",
    "    Arguments:\n",
    "        agent:          the agent whose buffer needs to be stored.\n",
    "        prepop_steps:   int, the number of timesteps to populate.\n",
    "        envs:           Tuple[gym.Env], the list of environments to use for sampling.\n",
    "    \"\"\"\n",
    "    timestep = 0\n",
    "    while timestep < prepop_steps:\n",
    "        env = RNG.rng.choice(envs)\n",
    "        agent.context_reset(env.reset())\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = RNG.rng.integers(env.action_space.n)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            # OpenAI Gym TimeLimit truncation: don't store it in the buffer as done\n",
    "            if info.get(\"TimeLimit.truncated\", False):\n",
    "                buffer_done = False\n",
    "            else:\n",
    "                buffer_done = done\n",
    "\n",
    "            agent.observe(next_obs, action, reward, buffer_done)\n",
    "            timestep += 1\n",
    "        agent.replay_buffer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cd608",
   "metadata": {},
   "source": [
    "Args\n",
    "- obs_embed\n",
    "- envs\n",
    "- num_steps\n",
    "- model\n",
    "- a_embed\n",
    "- in_embed\n",
    "- buf_size\n",
    "- lr\n",
    "- batch\n",
    "- context\n",
    "- max_episode_steps\n",
    "- history\n",
    "- tuf\n",
    "- discount\n",
    "- heads\n",
    "- layers\n",
    "- dropout\n",
    "- identity\n",
    "- pos\n",
    "- bad_size\n",
    "- eval_frequency\n",
    "- eval_episodes\n",
    "- save_policy\n",
    "- verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--project-name\",\n",
    "        type=str,\n",
    "        default=\"DTQN-test\",\n",
    "        help=\"The project name (for wandb) or directory name (for local logging) to store the results.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--disable-wandb\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Use `--disable-wandb` to log locally.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--time-limit\",\n",
    "        type=float,\n",
    "        default=None,\n",
    "        help=\"Time limit allowed for job. Useful for some cluster jobs such as slurm.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        default=\"DTQN\",\n",
    "        choices=list(MODEL_MAP.keys()),\n",
    "        help=\"Network model to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--envs\",\n",
    "        type=str,\n",
    "        nargs=\"+\",\n",
    "        default=\"DiscreteCarFlag-v0\",\n",
    "        help=\"Domain to use. You can supply multiple domains, but they must have the same observation and action space. With multiple environments, the agent will sample a new one on each episode reset for conducting policy rollouts and collection experience. During evaluation, it will perform the same evaluation for each domain (Note: this may significantly slow down your run! Consider increasing the eval-frequency or reducing the eval-episodes).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-steps\",\n",
    "        type=int,\n",
    "        default=2_000_000,\n",
    "        help=\"Number of steps to train the agent.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tuf\",\n",
    "        type=int,\n",
    "        default=10_000,\n",
    "        help=\"How many steps between each (hard) target network update.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", type=float, default=3e-4, help=\"Learning rate for the optimizer.\"\n",
    "    )\n",
    "    parser.add_argument(\"--batch\", type=int, default=32, help=\"Batch size.\")\n",
    "    parser.add_argument(\n",
    "        \"--buf-size\",\n",
    "        type=int,\n",
    "        default=500_000,\n",
    "        help=\"Number of timesteps to store in replay buffer. Note that we store the max length episodes given by the environment, so episodes that take longer will be padded at the end. This does not affect training but may affect the number of real observations in the buffer.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval-frequency\",\n",
    "        type=int,\n",
    "        default=5_000,\n",
    "        help=\"How many training timesteps between agent evaluations.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval-episodes\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Number of episodes for each evaluation period.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", type=str, default=\"cuda\", help=\"Pytorch device to use.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--context\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"For DRQN and DTQN, the context length to use to train the network.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--obs-embed\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"For discrete observation domains only. The number of features to give each observation.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--a-embed\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"The number of features to give each action. A value of 0 will prevent the policy from using the previous action.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--in-embed\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"The dimensionality of the network. In the transformer, this is referred to as `d_model`.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-episode-steps\",\n",
    "        type=int,\n",
    "        default=-1,\n",
    "        help=\"The maximum number of steps allowed in the environment. If `env` has a `max_episode_steps`, this will be inferred. Otherwise, this argument must be supplied.\",\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=1, help=\"The random seed to use.\")\n",
    "    parser.add_argument(\n",
    "        \"--save-policy\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Use this to save the policy so you can load it later for rendering.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--verbose\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Print out evaluation results as they come in to the console.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--render\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Enjoy mode (NOTE: must have a trained policy saved).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--history\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"This is how many (intermediate) Q-values we use to train for each context. To turn off intermediate Q-value prediction, set `--history 1`. To use the entire context, set history equal to the context length.\",\n",
    "    )\n",
    "    # DTQN-Specific\n",
    "    parser.add_argument(\n",
    "        \"--heads\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Number of heads to use for the transformer.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--layers\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"Number of transformer blocks to use for the transformer.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\", type=float, default=0.0, help=\"Dropout probability.\"\n",
    "    )\n",
    "    parser.add_argument(\"--discount\", type=float, default=0.99, help=\"Discount factor.\")\n",
    "    parser.add_argument(\n",
    "        \"--identity\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to use identity map reordering.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pos\",\n",
    "        default=\"learned\",\n",
    "        choices=[\"learned\", \"sin\", \"none\"],\n",
    "        help=\"The type of positional encodings to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--bag-size\", type=int, default=0, help=\"The size of the persistent memory bag.\"\n",
    "    )\n",
    "    # For slurm\n",
    "    parser.add_argument(\n",
    "        \"--slurm-job-id\",\n",
    "        default=0,\n",
    "        type=str,\n",
    "        help=\"The `$SLURM_JOB_ID` assigned to this job.\",\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095dc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_embed = 8 # For discrete observation domains only. The number of features to give each observation\n",
    "envs = \"string for domain - refer back to custom env\" \n",
    "# Domain to use. You can supply multiple domains, but they must have the same observation and action space. \n",
    "# With multiple environments, the agent will sample a new one on each episode reset for conducting policy \n",
    "# rollouts and collection experience. During evaluation, it will perform the same evaluation for each domain\n",
    "# (Note: this may significantly slow down your run! Consider increasing the eval-frequency or reducing the eval-episodes)\n",
    "num_steps = 2_000_000 # Number of steps to train the agent\n",
    "model = \"DTQN\" # Network model to use\n",
    "a_embed = 0 # The number of features to give each action. A value of 0 will prevent the policy from using the previous action\n",
    "in_embed = 1298 # The dimensionality of the network. In the transformer, this is referred to as `d_model`\n",
    "buf_size = 500_000 #Number of timesteps to store in replay buffer.\n",
    "# Note that we store the max length episodes given by the environment, so episodes that take longer will be padded at the end. \n",
    "# This does not affect training but may affect the number of real observations in the buffer\n",
    "lr = 3e-4 # Learning rate for the optimizer\n",
    "batch = 32 # Batch size\n",
    "context = 50 # For DRQN and DTQN, the context length to use to train the network\n",
    "max_episode_steps = -1 # The maximum number of steps allowed in the environment. \n",
    "#If `env` has a `max_episode_steps`, this will be inferred. Otherwise, this argument must be supplied.\n",
    "history = 50 #This is how many (intermediate) Q-values we use to train for each context. \n",
    "# To turn off intermediate Q-value prediction, set `--history 1`. To use the entire context, \n",
    "# set history equal to the context length.\n",
    "tuf = 10_000 # How many steps between each (hard) target network update\n",
    "discount = 0.99 # Discount factor\n",
    "heads = 8 # Number of heads to use for the transformer\n",
    "layers = 2 # Number of transformer blocks to use for the transformer\n",
    "dropout = 0.0 # Dropout probability\n",
    "identity = True # Whether or not to use identity map reordering\n",
    "pos = # type f positional encoder to use (look back at one they had to see if there is something to switch otherwise set value)\n",
    "bag_size = 0 # The size of the persistent memory bag\n",
    "eval_frequency = 5_000 #How many training timesteps between agent evaluations\n",
    "eval_episodes = 10 #Number of episodes for each evaluation period.\n",
    "save_policy = True # Use this to save the policy so you can load it later for rendering.\n",
    "verbose = True # (boolean - Print out evaluation results as they come in to the console): default = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(args):\n",
    "    \"\"\"Uses the command-line arguments to create the agent and associated tools, then begin training.\"\"\"\n",
    "    start_time = time()\n",
    "    # Create envs, set seed, create RL agent\n",
    "    envs = []\n",
    "    eval_envs = []\n",
    "    for env_str in args.envs:\n",
    "        envs.append(env_processing.make_env(env_str))\n",
    "        eval_envs.append(env_processing.make_env(env_str))\n",
    "    device = torch.device(args.device)\n",
    "    set_global_seed(args.seed, *(envs + eval_envs))\n",
    "\n",
    "    eps = epsilon_anneal.LinearAnneal(1.0, 0.1, args.num_steps // 10)\n",
    "\n",
    "    agent = get_agent(\n",
    "        args.model,\n",
    "        envs,\n",
    "        args.obs_embed,\n",
    "        args.a_embed,\n",
    "        args.in_embed,\n",
    "        args.buf_size,\n",
    "        device,\n",
    "        args.lr,\n",
    "        args.batch,\n",
    "        args.context,\n",
    "        args.max_episode_steps,\n",
    "        args.history,\n",
    "        args.tuf,\n",
    "        args.discount,\n",
    "        # DTQN specific\n",
    "        args.heads,\n",
    "        args.layers,\n",
    "        args.dropout,\n",
    "        args.identity,\n",
    "        args.pos,\n",
    "        args.bag_size,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[ {timestamp()} ] Creating {args.model} with {sum(p.numel() for p in agent.policy_network.parameters())} parameters\"\n",
    "    )\n",
    "\n",
    "    # Create logging dir\n",
    "    policy_save_dir = os.path.join(\n",
    "        os.getcwd(), \"policies\", args.project_name, *args.envs\n",
    "    )\n",
    "    os.makedirs(policy_save_dir, exist_ok=True)\n",
    "    policy_path = os.path.join(\n",
    "        policy_save_dir,\n",
    "        f\"model={args.model}_envs={','.join(args.envs)}_obs_embed={args.obs_embed}_a_embed={args.a_embed}_in_embed={args.in_embed}_context={args.context}_heads={args.heads}_layers={args.layers}_\"\n",
    "        f\"batch={args.batch}_identity={args.identity}_history={args.history}_pos={args.pos}_bag={args.bag_size}_seed={args.seed}\",\n",
    "    )\n",
    "\n",
    "    # Enjoy mode\n",
    "    if args.render:\n",
    "        agent.policy_network.load_state_dict(\n",
    "            torch.load(policy_path, map_location=\"cpu\")\n",
    "        )\n",
    "        evaluate(agent, eval_envs[0], 1_000_000, render=True)\n",
    "\n",
    "    # If there is already a saved checkpoint, load it and resume training if more steps are needed\n",
    "    # Or exit early if we have already finished training.\n",
    "    if os.path.exists(policy_path + \"_mini_checkpoint.pt\"):\n",
    "        steps_completed = agent.load_mini_checkpoint(policy_path)[\"step\"]\n",
    "        print(\n",
    "            f\"Found a mini checkpoint that completed {steps_completed} training steps.\"\n",
    "        )\n",
    "        if steps_completed >= args.num_steps:\n",
    "            print(f\"Removing checkpoint and exiting...\")\n",
    "            if os.path.exists(policy_path + \"_checkpoint.pt\"):\n",
    "                os.remove(policy_path + \"_checkpoint.pt\")\n",
    "            exit(0)\n",
    "        else:\n",
    "            (\n",
    "                wandb_id,\n",
    "                mean_success_rate,\n",
    "                mean_reward,\n",
    "                mean_episode_length,\n",
    "                eps_val,\n",
    "            ) = agent.load_checkpoint(policy_path)\n",
    "            eps.val = eps_val\n",
    "            wandb_kwargs = {\"resume\": \"must\", \"id\": wandb_id}\n",
    "    # Begin training from scratch\n",
    "    else:\n",
    "        wandb_kwargs = {\"resume\": None}\n",
    "        # Prepopulate the replay buffer\n",
    "        prepopulate(agent, 50_000, envs)\n",
    "        mean_success_rate = RunningAverage(10)\n",
    "        mean_reward = RunningAverage(10)\n",
    "        mean_episode_length = RunningAverage(10)\n",
    "\n",
    "    # Logging setup\n",
    "    logger = get_logger(policy_path, args, wandb_kwargs)\n",
    "\n",
    "    time_remaining = (\n",
    "        args.time_limit * 3600 - (time() - start_time) if args.time_limit else None\n",
    "    )\n",
    "\n",
    "    train(\n",
    "        agent,\n",
    "        envs,\n",
    "        eval_envs,\n",
    "        args.envs,\n",
    "        args.num_steps,\n",
    "        eps,\n",
    "        args.eval_frequency,\n",
    "        args.eval_episodes,\n",
    "        policy_path,\n",
    "        args.save_policy,\n",
    "        logger,\n",
    "        mean_success_rate,\n",
    "        mean_reward,\n",
    "        mean_episode_length,\n",
    "        time_remaining,\n",
    "        args.verbose,\n",
    "    )\n",
    "\n",
    "    # Save a small checkpoint if we finish training to let following runs know we are finished\n",
    "    agent.save_mini_checkpoint(\n",
    "        checkpoint_dir=policy_path, wandb_id=wandb.run.id if logger == wandb else None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49891cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment #get_args"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
