{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commodity Food Pricing: Model Experiments\n",
    "\n",
    "**By:** `MSOE AI-Club \"Nourish\" Student Research Team`<br/>\n",
    "**Primary Notebook Developer:** Ben Paulson, ____\n",
    "\n",
    "**Notebook Purpose:** To explore the use of machine learning to predict the price of commodities in the food industry. By utilizing the clean datasets loaded from `data_analysis.ipynb` into the directory `model_data`, we can begin to explore the use of machine learning to predict the price of commodities in the food industry. This notebook will be divided into sections of varying complexity, each of which will explore a different model type and its performance on the data. The models will be evaluated based on their ability to predict the price of a commodity in the future, given a set of features. The features will be selected based on their correlation to the price of the commodity, as determined in `data_analysis.ipynb`. There is also significant documentation with each section for both code/purpose sanity, sake of future reproducibility, and to help other members of the `MSOE AI-Club \"Nourish\" Student Research Team` understand the code and its purpose.\n",
    "* **Part 1: Loading the Experiment(s) Data**\n",
    "* **Part 2: Building Complex ML Models**\n",
    "    * **Model 1:** Simple, 1-layer Transformer\n",
    "    * **Model 2:** ...\n",
    "* **Part 3: Experiments with Training Methods**\n",
    "    * **Method 1:** ...\n",
    "    * **Method 2:** ...\n",
    "\n",
    "**Research Context:** This research is being conducted as part of the MSOE AI-Club's \"Nourish\" project, which aims to use machine learning to predict future food prices in order to help farmers in developing countries make better decisions about food storage and crop selection, achieved through the accurate warning administration and prediction of food commodity pricing data by country and food market. This project is pending as part of a relationship with the United Nation's Food and Agriculture Organization (FAO). [FAO Official Statement on the Importance of Research Like This](https://youtu.be/sZx3hhnEHiI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"apt-get\"\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ERROR: unknown command \"apt-get\"\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imageio==2.4.0 in /home/iveyg/.local/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: numpy in /home/iveyg/.local/lib/python3.10/site-packages (from imageio==2.4.0) (1.26.3)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.0) (9.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyvirtualdisplay in /home/iveyg/.local/lib/python3.10/site-packages (3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tf-agents[reverb] in /home/iveyg/.local/lib/python3.10/site-packages (0.19.0)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
      "Requirement already satisfied: gin-config>=0.4.0 in /home/iveyg/.local/lib/python3.10/site-packages (from tf-agents[reverb]) (0.5.0)\n",
      "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /home/iveyg/.local/lib/python3.10/site-packages (from tf-agents[reverb]) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/iveyg/.local/lib/python3.10/site-packages (from tf-agents[reverb]) (1.26.3)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.5.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (4.5.0)\n",
      "Requirement already satisfied: pygame==2.1.3 in /home/iveyg/.local/lib/python3.10/site-packages (from tf-agents[reverb]) (2.1.3)\n",
      "Requirement already satisfied: tensorflow-probability~=0.23.0 in /home/iveyg/.local/lib/python3.10/site-packages (from tf-agents[reverb]) (0.23.0)\n",
      "Requirement already satisfied: rlds in /home/iveyg/.local/lib/python3.10/site-packages (from tf-agents[reverb]) (0.1.8)\n",
      "Requirement already satisfied: dm-reverb~=0.14.0 in /home/iveyg/.local/lib/python3.10/site-packages (from tf-agents[reverb]) (0.14.0)\n",
      "Requirement already satisfied: tensorflow~=2.15.0 in /home/iveyg/.local/lib/python3.10/site-packages (from tf-agents[reverb]) (2.15.0.post1)\n",
      "Requirement already satisfied: dm-tree in /home/iveyg/.local/lib/python3.10/site-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (0.1.8)\n",
      "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (1.3.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/iveyg/.local/lib/python3.10/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/iveyg/.local/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.6.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (13.0.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/iveyg/.local/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (67.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.30.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.52.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/iveyg/.local/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/iveyg/.local/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/iveyg/.local/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents[reverb]) (5.1.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-agents[reverb]) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/iveyg/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.29.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.3.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.1.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyglet in /home/iveyg/.local/lib/python3.10/site-packages (2.0.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tf-keras in /home/iveyg/.local/lib/python3.10/site-packages (2.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip apt-get update\n",
    "%pip apt-get install -y xvfb ffmpeg freeglut3-dev\n",
    "%pip install 'imageio==2.4.0'\n",
    "%pip install pyvirtualdisplay\n",
    "%pip install tf-agents[reverb]\n",
    "%pip install pyglet\n",
    "%pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 16:55:54.914569: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-18 16:55:54.952394: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-18 16:55:54.952423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-18 16:55:54.953580: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-18 16:55:54.959902: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import reverb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from typing import Union, Optional\n",
    "import numpy as np\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.environments import tf_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data for model usage\n",
    "model_data = pd.read_csv('model_data/argentina_wheat_model_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading the Experiment(s) Data\n",
    "Based on the data retrieved from `data_analysis.ipynb`, get that data into a format capable of being used by a Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TREND_SAMPLES = 3 #can change here for training\n",
    "TEST_SIZE = 0.2\n",
    "OUTPUT_COLUMN_NAME = 'Price'\n",
    "\n",
    "num_input_samples = len(model_data.columns[2:-1]) + N_TREND_SAMPLES + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_n_previous_prices(model_data, n):\n",
    "    \"\"\"\n",
    "    For the given model_data, grab the n previous prices and store as a list\n",
    "    in a new column called 'n_previous_prices' for each row's associated 'Date'.\n",
    "    The current date's price should not be included in the list of previous prices.\n",
    "    :param pd.DataFrame model_data: data (model_data to grab from)\n",
    "    :param int n: Number of previous prices to grab\n",
    "    \"\"\"\n",
    "    # Create a new column to store the n_previous_prices\n",
    "    model_data['n_previous_prices'] = None\n",
    "\n",
    "    # Iterate through each row\n",
    "    for index, row in model_data.iterrows():\n",
    "        # Grab the date and price for the current row\n",
    "        date = row['Date']\n",
    "        price = row['Price']\n",
    "\n",
    "        # Grab the n previous prices\n",
    "        n_previous_prices = model_data.loc[model_data['Date'] < date]['Price'].tail(n).tolist()\n",
    "\n",
    "        # Store the n_previous_prices in the new column\n",
    "        model_data.at[index, 'n_previous_prices'] = n_previous_prices\n",
    "\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_inputs_and_outputs(row):\n",
    "    \"\"\"\n",
    "    Create a column to specifically be input into an ML model and a column for output\n",
    "    :param pd.DataFrame row: row to divide\n",
    "\n",
    "    :return: row with input and output columns\n",
    "    \"\"\"\n",
    "    output = row[OUTPUT_COLUMN_NAME]\n",
    "    inputs = row[2:-1].tolist()\n",
    "\n",
    "    # For each element in n_previous_price, add it to the inputs\n",
    "    for price in row['n_previous_prices']:\n",
    "        inputs.append(price)\n",
    "\n",
    "    return pd.Series([inputs, output], index=['inputs', 'output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_ML_usage(inputs, outputs):\n",
    "    \"\"\"\n",
    "    Given the inputs and outputs for the model, format the data to be used for ML.\n",
    "    This is accomplished by splitting the data into training/test tensors\n",
    "    :param inputs: inputs for the model (list of lists)\n",
    "    :param output: output for the model (list of values)\n",
    "\n",
    "    :return: The tensors responsible for testing/training the model and the scaler used to scale the data\n",
    "            (x_train_tensor, x_test_tensor, y_train_tensor, y_test_tensor, scaler)\n",
    "    \"\"\"\n",
    "    # Create a train/test split for the dataset\n",
    "    x_train, x_test, y_train, y_test = train_test_split(inputs, outputs, test_size=TEST_SIZE, shuffle=False)\n",
    "\n",
    "    # Determine if there are any nan values in x_train\n",
    "    for i in range(len(x_train)):\n",
    "        if np.isnan(x_train[i]).any():\n",
    "            print(f'x_train[{i}] has nan values! Check the data!')\n",
    "\n",
    "    # Convert them all to numpy arrays\n",
    "    x_train = np.array(x_train).reshape(-1, num_input_samples)\n",
    "    x_test = np.array(x_test).reshape(-1, num_input_samples)\n",
    "    y_train = np.array(y_train).reshape(-1, 1) # required reshape for StandardScaler\n",
    "    y_test = np.array(y_test).reshape(-1, 1) # required reshape for StandardScaler\n",
    "\n",
    "    # Standardize/Normalize the dataset (smaller distribution of a range of numbers)\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train).reshape(-1, num_input_samples, 1)\n",
    "    x_test_scaled = scaler.transform(x_test).reshape(-1, num_input_samples, 1)\n",
    "    y_train_scaled = scaler.fit_transform(y_train)\n",
    "    y_test_scaled = scaler.transform(y_test)\n",
    "\n",
    "    # Convert to tensors for model ingestion (true training data) (like numpy arrays but in the terms of TensorFlow)\n",
    "    x_train_tensor = tf.convert_to_tensor(x_train_scaled)\n",
    "    x_test_tensor = tf.convert_to_tensor(x_test_scaled)\n",
    "    y_train_tensor = tf.convert_to_tensor(y_train_scaled)\n",
    "    y_test_tensor = tf.convert_to_tensor(y_test_scaled)\n",
    "\n",
    "    return (x_train_tensor, x_test_tensor, y_train_tensor, y_test_tensor, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Proteus2</th>\n",
       "      <th>Food Price Index</th>\n",
       "      <th>Cereals Price Index</th>\n",
       "      <th>Wheat Futures</th>\n",
       "      <th>Harvest</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>n_previous_prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2003-02-01</td>\n",
       "      <td>0.305216</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.454255</td>\n",
       "      <td>0.428151</td>\n",
       "      <td>355.50000</td>\n",
       "      <td>2936474.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.284833281070346, 0.2717675342322567, 0.2879...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2003-03-01</td>\n",
       "      <td>0.312010</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.446577</td>\n",
       "      <td>0.418139</td>\n",
       "      <td>334.40000</td>\n",
       "      <td>2936474.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.2717675342322567, 0.2879690603114874, 0.305...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2003-04-01</td>\n",
       "      <td>0.298108</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.446577</td>\n",
       "      <td>0.417550</td>\n",
       "      <td>321.31250</td>\n",
       "      <td>2936474.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.2879690603114874, 0.3052158461377652, 0.312...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2003-05-01</td>\n",
       "      <td>0.327689</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.452335</td>\n",
       "      <td>0.430506</td>\n",
       "      <td>344.21875</td>\n",
       "      <td>2936474.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3052158461377652, 0.3120100344935716, 0.298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2003-06-01</td>\n",
       "      <td>0.330302</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.450416</td>\n",
       "      <td>0.426973</td>\n",
       "      <td>315.45000</td>\n",
       "      <td>2936474.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3120100344935716, 0.2981080798578447, 0.327...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>0.449462</td>\n",
       "      <td>0.252506</td>\n",
       "      <td>0.648752</td>\n",
       "      <td>0.559482</td>\n",
       "      <td>402.43750</td>\n",
       "      <td>2679728.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.422284937807045, 0.4390090937597993, 0.4390...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>0.420717</td>\n",
       "      <td>0.252506</td>\n",
       "      <td>0.653871</td>\n",
       "      <td>0.540636</td>\n",
       "      <td>407.78125</td>\n",
       "      <td>2679728.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.4390090937597993, 0.4390090937597993, 0.449...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>0.384656</td>\n",
       "      <td>0.252506</td>\n",
       "      <td>0.653231</td>\n",
       "      <td>0.543581</td>\n",
       "      <td>414.17500</td>\n",
       "      <td>2679728.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.4390090937597993, 0.4494616912302707, 0.420...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>0.368768</td>\n",
       "      <td>0.252506</td>\n",
       "      <td>0.651951</td>\n",
       "      <td>0.538869</td>\n",
       "      <td>408.40625</td>\n",
       "      <td>2679728.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.4494616912302707, 0.4207170481864744, 0.384...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>0.351207</td>\n",
       "      <td>0.252506</td>\n",
       "      <td>0.647473</td>\n",
       "      <td>0.540047</td>\n",
       "      <td>405.90625</td>\n",
       "      <td>2679728.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.4207170481864744, 0.3846555869133479, 0.368...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date     Price  Proteus2  Food Price Index  Cereals Price Index  \\\n",
       "164  2003-02-01  0.305216  0.284869          0.454255             0.428151   \n",
       "163  2003-03-01  0.312010  0.284869          0.446577             0.418139   \n",
       "162  2003-04-01  0.298108  0.284869          0.446577             0.417550   \n",
       "161  2003-05-01  0.327689  0.284869          0.452335             0.430506   \n",
       "160  2003-06-01  0.330302  0.284869          0.450416             0.426973   \n",
       "..          ...       ...       ...               ...                  ...   \n",
       "4    2016-08-01  0.449462  0.252506          0.648752             0.559482   \n",
       "3    2016-09-01  0.420717  0.252506          0.653871             0.540636   \n",
       "2    2016-10-01  0.384656  0.252506          0.653231             0.543581   \n",
       "1    2016-11-01  0.368768  0.252506          0.651951             0.538869   \n",
       "0    2016-12-01  0.351207  0.252506          0.647473             0.540047   \n",
       "\n",
       "     Wheat Futures    Harvest  Sentiment  \\\n",
       "164      355.50000  2936474.0        1.0   \n",
       "163      334.40000  2936474.0        1.0   \n",
       "162      321.31250  2936474.0        1.0   \n",
       "161      344.21875  2936474.0        1.0   \n",
       "160      315.45000  2936474.0        1.0   \n",
       "..             ...        ...        ...   \n",
       "4        402.43750  2679728.0        1.0   \n",
       "3        407.78125  2679728.0        1.0   \n",
       "2        414.17500  2679728.0        1.0   \n",
       "1        408.40625  2679728.0        1.0   \n",
       "0        405.90625  2679728.0        1.0   \n",
       "\n",
       "                                     n_previous_prices  \n",
       "164  [0.284833281070346, 0.2717675342322567, 0.2879...  \n",
       "163  [0.2717675342322567, 0.2879690603114874, 0.305...  \n",
       "162  [0.2879690603114874, 0.3052158461377652, 0.312...  \n",
       "161  [0.3052158461377652, 0.3120100344935716, 0.298...  \n",
       "160  [0.3120100344935716, 0.2981080798578447, 0.327...  \n",
       "..                                                 ...  \n",
       "4    [0.422284937807045, 0.4390090937597993, 0.4390...  \n",
       "3    [0.4390090937597993, 0.4390090937597993, 0.449...  \n",
       "2    [0.4390090937597993, 0.4494616912302707, 0.420...  \n",
       "1    [0.4494616912302707, 0.4207170481864744, 0.384...  \n",
       "0    [0.4207170481864744, 0.3846555869133479, 0.368...  \n",
       "\n",
       "[165 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort model data where the earliest date comes first and the latest date comes last\n",
    "model_data = model_data.sort_values(by='Date', ascending=True)\n",
    "model_data = grab_n_previous_prices(model_data, N_TREND_SAMPLES)\n",
    "\n",
    "# Remove rows where the len(n_previous_prices) != N_TREND_SAMPLES\n",
    "model_data = model_data[model_data['n_previous_prices'].apply(lambda x: len(x) == N_TREND_SAMPLES)]\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'Sentiment' column contains some nan values when articles weren't published. Replace these with 0\n",
    "model_data['Sentiment'] = model_data['Sentiment'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_tensor shape: (132, 9, 1)\n",
      "x_test_tensor shape: (33, 9, 1)\n",
      "y_train_tensor shape: (132, 1)\n",
      "y_test_tensor shape: (33, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 16:56:00.004688: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-02-18 16:56:00.004720: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: dh-node4\n",
      "2024-02-18 16:56:00.004725: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: dh-node4\n",
      "2024-02-18 16:56:00.004771: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got \"1\"\n",
      "2024-02-18 16:56:00.004783: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 530.30.2\n"
     ]
    }
   ],
   "source": [
    "inputs_and_outputs = model_data.apply(divide_inputs_and_outputs, axis=1)\n",
    "inputs = inputs_and_outputs['inputs'].tolist(); outputs = inputs_and_outputs['output'].tolist()\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, outputs, test_size=TEST_SIZE, shuffle=False) # Don't shuffle, should be cohesive samples not seen\n",
    "x_train_tensor, x_test_tensor, y_train_tensor, y_test_tensor, scaler = format_for_ML_usage(inputs, outputs)\n",
    "\n",
    "print('x_train_tensor shape:', x_train_tensor.shape) #matricies (think of the multiple features being used)\n",
    "print('x_test_tensor shape:', x_test_tensor.shape)\n",
    "print('y_train_tensor shape:', y_train_tensor.shape) #think of a list\n",
    "print('y_test_tensor shape:', y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building Complex ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define positional encoding function (another portion for data preprocessing)\n",
    "def positional_encoding(length, depth): #\"this one came before this one\" -> sequencing\n",
    "    \"\"\" \n",
    "    Create positional encoding\n",
    "    args:\n",
    "        length: length of the sequence\n",
    "        depth: depth of the model\n",
    "    \"\"\"\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / depth) for j in range(depth)]\n",
    "        for pos in range(length)\n",
    "    ])\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])\n",
    "    return tf.cast(pos_enc, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Simple, 1-Layer Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD_SIZE = 64\n",
    "NUM_HEADS = 16\n",
    "FF_DIM = 16\n",
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "d_model = x_train_tensor.shape[-1] \n",
    "length = x_train_tensor.shape[1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(head_size, num_heads, ff_dim, dropout=0.5, num_transformers = 100):\n",
    "    \"\"\"\n",
    "    Building a simple transformer based off the original work compiled in\n",
    "    the research paper \"Attention Is All You Need\" (https://arxiv.org/pdf/1706.03762.pdf)\n",
    "    :param int head_size: Size of the attention heads\n",
    "    :param int num_heads: Number of heads\n",
    "    :param int ff_dim: Feed forward dimension\n",
    "    :param float dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input Layer\n",
    "    inputs = tf.keras.layers.Input(shape=(num_input_samples, d_model)) # (None, 9, 1) (none is the number of samples given)\n",
    "\n",
    "    # Add positional encoding to the input\n",
    "    # The positional encoding is added to the input in order to give the model some information about the relative position of the words in the sequence\n",
    "    # Not including the positional encoding is basically the same as randomizing the order of the data\n",
    "    positional_inputs = inputs + positional_encoding(num_input_samples, d_model) # (None, 9, 1)\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(positional_inputs) # (None, 9, 1)\n",
    "    \n",
    "    for i in range(num_transformers):\n",
    "        x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x) # (None, 9, 1)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x) # (None, 9, 1)\n",
    "\n",
    "        # Add & Norm\n",
    "        res = x + positional_inputs # (None, 9, 1)\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res) # (None, 9, 1)\n",
    "\n",
    "        # Feed-Forward Network (Using Dense layers instead of Conv1D layers)\n",
    "        x = tf.keras.layers.Dense(ff_dim, activation='relu')(x) # (None, 9, 16)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x) # (None, 9, 16)\n",
    "\n",
    "        # Skip Connection     \n",
    "        x = x + res # (None, 9, 16)\n",
    "\n",
    "    # Global Average Pooling layer\n",
    "    # The Global Average Pooling layer reduces the dimensionality/complexity of the data\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x) # (None, 16)\n",
    "    \n",
    "    # Output Layer\n",
    "    outputs = tf.keras.layers.Dense(1)(x) # (None, 1)\n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,outputs = transformer(HEAD_SIZE, NUM_HEADS, FF_DIM)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model (tracking)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE) #(how does it step down?)(gradient descent video)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#add in learning rate\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.16914994, 0.14720488, 0.15284207, 0.17423838, 0.23109588,\n",
       "       0.30928382, 0.3557428 , 0.37050232, 0.3743851 , 0.41198504,\n",
       "       0.41978008, 0.42255113, 0.3907956 , 0.22041222, 0.24432272,\n",
       "       0.22987977, 0.31729937, 0.46899554, 0.4939364 , 0.5487927 ,\n",
       "       0.4589561 , 0.23700486, 0.50725186, 0.4942932 , 0.5099007 ,\n",
       "       0.26929864, 0.34520122, 0.23464152, 0.34188366, 0.37266535,\n",
       "       0.36762798, 0.23584223, 0.51960653], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example prediction provided by the model (without training)\n",
    "example_prediction = model.predict(x_test_tensor)\n",
    "example_prediction = scaler.inverse_transform(example_prediction)\n",
    "example_prediction = example_prediction.reshape(-1)\n",
    "example_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: RL Framework (Mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration =   1 #@param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceEnv(py_environment.PyEnvironment): #creating the environment\n",
    "    def __init__(self, prediction): #what could the actions be, what could the observations be\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.int32, minimum=0, maximum=np.max(prediction), name='observation')\n",
    "        self._state = 0 #what is the state of the environment\n",
    "        self._episode_ended = False\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            return self.reset()\n",
    "\n",
    "        # Make sure episodes don't go on forever.\n",
    "        if action == 1: #move down/move up past slope 1/2\n",
    "            self._state += 1 #time step for every 15 days?\n",
    "        elif action == 0: #move up/move down under slow 1/2\n",
    "            self._episode_ended = True\n",
    "        else:\n",
    "            raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "        if self._episode_ended:\n",
    "            reward = self._state - 10 if self._state <= 2 else -20\n",
    "            return ts.termination(np.array([self._state], dtype=np.int32), reward) #ts = time step\n",
    "        #Returns a TimeStep with step_type set to StepType.LAST.\n",
    "        else:\n",
    "            return ts.transition(\n",
    "              np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)\n",
    "                #Returns a TimeStep with step_type set equal to StepType.MID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Given `time_step`: TimeStep(\n{'step_type': array(1, dtype=int32),\n 'reward': array(0., dtype=float32),\n 'discount': array(1., dtype=float32),\n 'observation': array([1], dtype=int32)}) does not match expected `time_step_spec`: TimeStep(\n{'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'),\n 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n 'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n 'observation': BoundedArraySpec(shape=(1,), dtype=dtype('int32'), name='observation', minimum=0, maximum=0)})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m eval_py_env \u001b[38;5;241m=\u001b[39m PriceEnv(example_prediction)\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m PriceEnv(example_prediction)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_py_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m tf_py_environment\u001b[38;5;241m.\u001b[39mTFPyEnvironment(env)\n\u001b[1;32m      6\u001b[0m train_env \u001b[38;5;241m=\u001b[39m tf_py_environment\u001b[38;5;241m.\u001b[39mTFPyEnvironment(train_py_env)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tf_agents/environments/utils.py:83\u001b[0m, in \u001b[0;36mvalidate_py_environment\u001b[0;34m(environment, episodes, observation_and_action_constraint_splitter)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m episode_count \u001b[38;5;241m<\u001b[39m episodes:\n\u001b[1;32m     82\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m array_spec\u001b[38;5;241m.\u001b[39mcheck_arrays_nest(time_step, batched_time_step_spec):\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGiven `time_step`: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not match expected `time_step_spec`: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;241m%\u001b[39m (time_step, batched_time_step_spec)\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     88\u001b[0m   action \u001b[38;5;241m=\u001b[39m random_policy\u001b[38;5;241m.\u001b[39maction(time_step)\u001b[38;5;241m.\u001b[39maction\n\u001b[1;32m     89\u001b[0m   time_step \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[0;31mValueError\u001b[0m: Given `time_step`: TimeStep(\n{'step_type': array(1, dtype=int32),\n 'reward': array(0., dtype=float32),\n 'discount': array(1., dtype=float32),\n 'observation': array([1], dtype=int32)}) does not match expected `time_step_spec`: TimeStep(\n{'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'),\n 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n 'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n 'observation': BoundedArraySpec(shape=(1,), dtype=dtype('int32'), name='observation', minimum=0, maximum=0)})"
     ]
    }
   ],
   "source": [
    "train_py_env = PriceEnv(example_prediction)\n",
    "eval_py_env = PriceEnv(example_prediction)\n",
    "env = PriceEnv(example_prediction)\n",
    "utils.validate_py_environment(env, episodes=5)\n",
    "env = tf_py_environment.TFPyEnvironment(env) #need to transfer to a Tensorflow Environment\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "    return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer]) #creating the q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent( #make the Deep Q Network Agent\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10): #metric \n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = environment.step(action_step.action)\n",
    "        episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defines the way an agent acts in an environment\n",
    "#eval vs. collecting data\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "# return is the sum of rewards obtained while running a policy in an environment for an episode. \n",
    "# several episodes are run, creating an average return.\n",
    "#the higher the reward - closer to less dips in the prices\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /tmp/tmp11gz38f8.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:565] Loading latest checkpoint from /tmp/tmp11gz38f8\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 21611\n"
     ]
    }
   ],
   "source": [
    "#to store trajectories of experience when executing a policy in an environment\n",
    "#experience in this case would be the last two TimeStamps \n",
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_max_length,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  replay_buffer.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': BoundedTensorSpec(shape=(1,), dtype=tf.int32, name='observation', minimum=array(0, dtype=int32), maximum=array(2147483647, dtype=int32)),\n",
       " 'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32)),\n",
       " 'policy_info': (),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take the length of shape with unknown rank.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[43mpy_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyDriver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpy_tf_eager_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTFEagerPolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mrandom_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tf_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrb_observer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_collect_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_py_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tf_agents/drivers/py_driver.py:119\u001b[0m, in \u001b[0;36mPyDriver.run\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mbatched \u001b[38;5;129;01mand\u001b[39;00m time_step\u001b[38;5;241m.\u001b[39mis_first() \u001b[38;5;129;01mand\u001b[39;00m num_episodes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    117\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mget_initial_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m action_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m next_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action_step\u001b[38;5;241m.\u001b[39maction)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# When using observer (for the purpose of training), only the previous\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# policy_state is useful. Therefore substitube it in the PolicyStep and\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# consume it w/ the observer.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tf_agents/policies/py_policy.py:169\u001b[0m, in \u001b[0;36mPyPolicy.action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    167\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action(time_step, policy_state, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tf_agents/policies/py_tf_eager_policy.py:107\u001b[0m, in \u001b[0;36mPyTFEagerPolicyBase._action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    105\u001b[0m   policy_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy_action_fn(time_step, policy_state, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m   policy_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_policy_action_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_time_steps:\n\u001b[1;32m    109\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m policy_step\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tf_agents/policies/tf_policy.py:333\u001b[0m, in \u001b[0;36mTFPolicy.action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_automatic_state_reset:\n\u001b[1;32m    332\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_reset_state(time_step, policy_state)\n\u001b[0;32m--> 333\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[43maction_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_action\u001b[39m(action, action_spec):\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action_spec, tensor_spec\u001b[38;5;241m.\u001b[39mBoundedTensorSpec):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tf_agents/utils/common.py:193\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m check_tf1_allowed()\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_eager_been_enabled():\n\u001b[1;32m    191\u001b[0m   \u001b[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[1;32m    192\u001b[0m   \u001b[38;5;66;03m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_variables_enabled():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tf_agents/policies/random_tf_policy.py:142\u001b[0m, in \u001b[0;36mRandomTFPolicy._action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, time_step, policy_state, seed):  \u001b[38;5;66;03m# pytype: disable=signature-mismatch  # overriding-parameter-count-checks\u001b[39;00m\n\u001b[1;32m    138\u001b[0m   observation_and_action_constraint_splitter \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    139\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_and_action_constraint_splitter\n\u001b[1;32m    140\u001b[0m   )\n\u001b[0;32m--> 142\u001b[0m   outer_dims \u001b[38;5;241m=\u001b[39m \u001b[43mnest_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_outer_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_time_step_spec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m observation_and_action_constraint_splitter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     observation, mask \u001b[38;5;241m=\u001b[39m observation_and_action_constraint_splitter(\n\u001b[1;32m    145\u001b[0m         time_step\u001b[38;5;241m.\u001b[39mobservation\n\u001b[1;32m    146\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tf_agents/utils/nest_utils.py:902\u001b[0m, in \u001b[0;36mget_outer_shape\u001b[0;34m(nested_tensor, spec)\u001b[0m\n\u001b[1;32m    899\u001b[0m first_spec \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(spec)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# Check tensors have same batch shape.\u001b[39;00m\n\u001b[0;32m--> 902\u001b[0m num_outer_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfirst_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(first_spec\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched_nested_tensors(\n\u001b[1;32m    904\u001b[0m     nested_tensor, spec, num_outer_dims\u001b[38;5;241m=\u001b[39mnum_outer_dims, check_dtypes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    905\u001b[0m ):\n\u001b[1;32m    906\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconstant([], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take the length of shape with unknown rank."
     ]
    }
   ],
   "source": [
    "#execute the random policy in the environment for a few steps, recording the data in the replay buffer\n",
    "py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      random_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=initial_collect_steps).run(train_py_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_step_spec.step_type: TensorSpec(shape=(), dtype=tf.int32, name='step_type')\n"
     ]
    }
   ],
   "source": [
    "print('time_step_spec.step_type:', train_env.time_step_spec().step_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (896374) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (896374) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (896374) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (896374) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (896374) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (896374) so Table uniform_table is accessed directly without gRPC.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_11_device_/job:localhost/replica:0/task:0/device:CPU:0}} Received incompatible tensor at flattened index 3 from table 'uniform_table'.  Specification has (dtype, shape): (int32, [?]).  Tensor has (dtype, shape): (int32, [2,1]).\nTable signature: 0: Tensor<name: 'step_type/step_type', dtype: int32, shape: [?]>, 1: Tensor<name: 'observation/observation', dtype: int32, shape: [?,1]>, 2: Tensor<name: 'action/action', dtype: int32, shape: [?]>, 3: Tensor<name: 'next_step_type/step_type', dtype: int32, shape: [?]>, 4: Tensor<name: 'reward/reward', dtype: float, shape: [?]>, 5: Tensor<name: 'discount/discount', dtype: float, shape: [?]> [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m   time_step, _ \u001b[38;5;241m=\u001b[39m collect_driver\u001b[38;5;241m.\u001b[39mrun(time_step)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Sample a batch of data from the buffer and update the agent's network.\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m   experience, unused_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m   train_loss \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain(experience)\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     31\u001b[0m   step \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain_step_counter\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:810\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    809\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:773\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 773\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3029\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3027\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3028\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3029\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3031\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_11_device_/job:localhost/replica:0/task:0/device:CPU:0}} Received incompatible tensor at flattened index 3 from table 'uniform_table'.  Specification has (dtype, shape): (int32, [?]).  Tensor has (dtype, shape): (int32, [2,1]).\nTable signature: 0: Tensor<name: 'step_type/step_type', dtype: int32, shape: [?]>, 1: Tensor<name: 'observation/observation', dtype: int32, shape: [?,1]>, 2: Tensor<name: 'action/action', dtype: int32, shape: [?]>, 3: Tensor<name: 'next_step_type/step_type', dtype: int32, shape: [?]>, 4: Tensor<name: 'reward/reward', dtype: float, shape: [?]>, 5: Tensor<name: 'discount/discount', dtype: float, shape: [?]> [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step.\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "# Reset the environment.\n",
    "time_step = train_py_env.reset()\n",
    "\n",
    "# Create a driver to collect experience.\n",
    "collect_driver = py_driver.PyDriver(\n",
    "    train_env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      agent.collect_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=collect_steps_per_iteration)\n",
    "\n",
    "for _ in range(2):\n",
    "\n",
    "  # Collect a few steps and save to the replay buffer.\n",
    "    time_step, _ = collect_driver.run(time_step)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training the Model\n",
    "This section is specifically for training the model built in the previous section. Some contants (`NUM_EPOCHS`, `BATCH_SIZE`) are provided and should be the only required parameters to adjust for this section of the experiment. \n",
    "\n",
    "A plot of the loss throughout the training process is provided for easy understanding about if the model is overfitting or underfitting. For a review of these concepts, see [this article](https://www.analyticsfordecisions.com/overfitting-and-underfitting/#:~:text=Overfitting%20happens%20when%20the%20model%20is%20too%20complex,poor%20performance%20on%20both%20training%20and%20test%20datasets.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 15 #number of times the entire data set has to be worked through the learning algorithm\n",
    "BATCH_SIZE = 32 #what was put in through the model before updating the weights\n",
    "#based on one batch size, the # of BATCH_SIZE is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on our training data (train tensors)\n",
    "#validation data?\n",
    "history = model.fit(\n",
    "    x_train_tensor, y_train_tensor,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over time\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Examining Results\n",
    "In this section, we're plotting the model's predictions versus the actual price point for the commodity in question. However, one plot focuses specifically on the testing data only (this is a better plot to see how well the model is performing/generalizing) and the other focuses on the entire dataset (this is a better plot to see if the model is correlating to the provided dataset at all).\n",
    "\n",
    "Therefore, when **evaluating the performance** of the model, **the first plot should be used.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model's predictions of the testing data vs the actual data\n",
    "predictions = model.predict(x_test_tensor)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "predictions = predictions.reshape(-1)\n",
    "\n",
    "plt.plot(y_test, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.title('Model Predictions (Testing Data Only)')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Days')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model's predictions (both training and testing) vs the actual data\n",
    "predictions = model.predict(x_train_tensor)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "predictions = predictions.reshape(-1)\n",
    "\n",
    "plt.plot(y_train, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.title('Model Predictions (Training and Testing Data)')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Days')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
