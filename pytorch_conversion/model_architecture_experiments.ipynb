{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commodity Food Pricing: Model Experiments\n",
    "\n",
    "**By:** `MSOE AI-Club \"Nourish\" Student Research Team`<br/>\n",
    "**Primary Notebook Developer:** Ben Paulson, ____\n",
    "\n",
    "**Notebook Purpose:** To explore the use of machine learning to predict the price of commodities in the food industry. By utilizing the clean datasets loaded from `data_analysis.ipynb` into the directory `model_data`, we can begin to explore the use of machine learning to predict the price of commodities in the food industry. This notebook will be divided into sections of varying complexity, each of which will explore a different model type and its performance on the data. The models will be evaluated based on their ability to predict the price of a commodity in the future, given a set of features. The features will be selected based on their correlation to the price of the commodity, as determined in `data_analysis.ipynb`. There is also significant documentation with each section for both code/purpose sanity, sake of future reproducibility, and to help other members of the `MSOE AI-Club \"Nourish\" Student Research Team` understand the code and its purpose.\n",
    "* **Part 1: Loading the Experiment(s) Data**\n",
    "* **Part 2: Building Complex ML Models**\n",
    "    * **Model 1:** Simple, 1-layer Transformer\n",
    "    * **Model 2:** ...\n",
    "* **Part 3: Experiments with Training Methods**\n",
    "    * **Method 1:** ...\n",
    "    * **Method 2:** ...\n",
    "\n",
    "**Research Context:** This research is being conducted as part of the MSOE AI-Club's \"Nourish\" project, which aims to use machine learning to predict future food prices in order to help farmers in developing countries make better decisions about food storage and crop selection, achieved through the accurate warning administration and prediction of food commodity pricing data by country and food market. This project is pending as part of a relationship with the United Nation's Food and Agriculture Organization (FAO). [FAO Official Statement on the Importance of Research Like This](https://youtu.be/sZx3hhnEHiI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\GitHub\\Predicting-Commodity-Food-Pricing\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import data_loading as dl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading the Experiment(s) Data\n",
    "Based on the data retrieved from `data_analysis.ipynb`, get that data into a format capable of being used by a Machine Learning model.\n",
    "\n",
    "<span style=\"color: orange;\">**Future Experiment:** Generating multiple months or simply estimating a year in advance.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  9\n"
     ]
    }
   ],
   "source": [
    "MODEL_DATA_DIRECTORY = 'model_data/wheat_model_data/'\n",
    "N_TREND_SAMPLES = 3 # Number of price samples for \"history\" trend (training data)\n",
    "N_MONTHS_AHEAD = 1 # Number of months ahead to predict\n",
    "TEST_SIZE = 0.2\n",
    "OUTPUT_COLUMN_NAME = 'Price'\n",
    "\n",
    "# Get the number of features for this model (given the model_data chosen)\n",
    "sample_file_path = os.path.join(MODEL_DATA_DIRECTORY, os.listdir(MODEL_DATA_DIRECTORY)[0])\n",
    "sample_columns = pd.read_csv(sample_file_path).columns[1:] # Skip 'Unnamed: 0' column\n",
    "num_input_samples = len(sample_columns[2:-1]) + N_TREND_SAMPLES + 1 # +1 for 'n_previous_prices' later\n",
    "print(\"Num Features: \", num_input_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GitHub\\Predicting-Commodity-Food-Pricing\\data_loading.py:175: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_data = pd.concat([model_data, data_to_concat], ignore_index=True).drop(columns=['Unnamed: 0'])\n"
     ]
    }
   ],
   "source": [
    "model_data = dl.get_model_data_from_directory(MODEL_DATA_DIRECTORY, sample_columns, N_TREND_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_tensor shape: (709, 9, 1)\n",
      "x_test_tensor shape: (178, 9, 1)\n",
      "y_train_tensor shape: (709, 1)\n",
      "y_test_tensor shape: (178, 1)\n"
     ]
    }
   ],
   "source": [
    "inputs_and_outputs = model_data.apply(dl.divide_inputs_and_outputs, axis=1)\n",
    "inputs = inputs_and_outputs['inputs'].tolist(); outputs = inputs_and_outputs['output'].tolist()\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, outputs, test_size=TEST_SIZE, shuffle=False) # Don't shuffle, should be cohesive samples not seen\n",
    "x_train_tensor, x_test_tensor, y_train_tensor, y_test_tensor, scaler = dl.format_for_ML_usage_tf(inputs, outputs, num_input_samples)\n",
    "\n",
    "print('x_train_tensor shape:', x_train_tensor.shape)\n",
    "print('x_test_tensor shape:', x_test_tensor.shape)\n",
    "print('y_train_tensor shape:', y_train_tensor.shape)\n",
    "print('y_test_tensor shape:', y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building Complex ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define positional encoding function (another portion for data preprocessing)\n",
    "def positional_encoding(length, depth):\n",
    "    \"\"\" \n",
    "    Create positional encoding\n",
    "    args:\n",
    "        length: length of the sequence\n",
    "        depth: depth of the model\n",
    "    \"\"\"\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / depth) for j in range(depth)]\n",
    "        for pos in range(length)\n",
    "    ])\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])\n",
    "    return tf.cast(pos_enc, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Simple, 1-Layer Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "HEAD_SIZE = 64\n",
    "NUM_HEADS = 16\n",
    "FF_DIM = 16\n",
    "\n",
    "# ML Optimizer\n",
    "LEARNING_RATE = 1e-5\n",
    "CLIP_VALUE = 0.5 # Gradient Clipping (https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem)\n",
    "DROP_OUT_RATE = 0.2\n",
    "\n",
    "d_model = x_train_tensor.shape[-1] \n",
    "length = x_train_tensor.shape[1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(head_size, num_heads, ff_dim, dropout=0, num_transformers=10):\n",
    "    \"\"\"\n",
    "    Building a simple transformer based off the original work compiled in\n",
    "    the research paper \"Attention Is All You Need\" (https://arxiv.org/pdf/1706.03762.pdf)\n",
    "    :param int head_size: Size of the attention heads\n",
    "    :param int num_heads: Number of heads\n",
    "    :param int ff_dim: Feed forward dimension\n",
    "    :param float dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input Layer\n",
    "    inputs = tf.keras.layers.Input(shape=(num_input_samples, d_model)) # (None, 9, 1)\n",
    "\n",
    "    # Add positional encoding to the input\n",
    "    # The positional encoding is added to the input in order to give the model some information about the relative position of the words in the sequence\n",
    "    # Not including the positional encoding is basically the same as randomizing the order of the data\n",
    "    positional_inputs = inputs + positional_encoding(num_input_samples, d_model) # (None, 9, 1)\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(positional_inputs) # (None, 9, 1)\n",
    "     \n",
    "    for i in range(num_transformers):\n",
    "        # print(\"At start of transformer \", i, \" x shape: \", x.shape)\n",
    "        x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x) # (None, 9, 1)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x) # (None, 9, 1)\n",
    "        \n",
    "        # Add & Norm\n",
    "        res = x + positional_inputs # (None, 9, 1)\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res) # (None, 9, 1)\n",
    "        \n",
    "        # Feed-Forward Network (Using Dense layers instead of Conv1D layers)\n",
    "        x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x) # (None, 9, 16)\n",
    "\n",
    "        # Get the shape of x back to (None, 9, 1)\n",
    "        x = tf.keras.layers.Conv1D(filters=d_model, kernel_size=1)(x)\n",
    "        \n",
    "        # Skip Connection     \n",
    "        x = x + res # (None, 9, 1)\n",
    "        # print(\"At end of transformer \", i, \" x shape: \", x.shape)\n",
    "\n",
    "    # Global Average Pooling layer\n",
    "    # The Global Average Pooling layer reduces the dimensionality/complexity of the data\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x) # (None, 16)\n",
    "    \n",
    "    # Output Layer (WARNING: Don't use sigmoid, requires too high of gradient to achieve ~1 or ~0)\n",
    "    outputs = tf.keras.layers.Dense(1)(x) # (None, 1)\n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\GitHub\\Predicting-Commodity-Food-Pricing\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs,outputs = transformer(HEAD_SIZE, NUM_HEADS, FF_DIM, dropout = DROP_OUT_RATE)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss='mae')\n",
    "\n",
    "# Ensure all the weights are reset\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Researching Best Loss Functions for Trendline Prediction\n",
    "# - Mean Absolute Error (MAE) - Emphasizes that all errors are equally important\n",
    "# - Mean Absolute Percentage Error (MAPE) - Emphasizes that all errors are equally important (percentage-wise; normalized)\n",
    "\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example prediction provided by the model (without training)\n",
    "# example_prediction = model.predict(x_test_tensor)\n",
    "# example_prediction = scaler.inverse_transform(example_prediction)\n",
    "# example_prediction = example_prediction.reshape(-1)\n",
    "# example_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training the Model\n",
    "This section is specifically for training the model built in the previous section. Some contants (`NUM_EPOCHS`, `BATCH_SIZE`) are provided and should be the only required parameters to adjust for this section of the experiment. \n",
    "\n",
    "A plot of the loss throughout the training process is provided for easy understanding about if the model is overfitting or underfitting. For a review of these concepts, see [this article](https://www.analyticsfordecisions.com/overfitting-and-underfitting/#:~:text=Overfitting%20happens%20when%20the%20model%20is%20too%20complex,poor%20performance%20on%20both%20training%20and%20test%20datasets.).\n",
    "<br/><br/>\n",
    "**Potential Future Parameters**\n",
    "* **Regularization:** L1, L2, Dropout; helps prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LearningRateScheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m lr \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[43mLearningRateScheduler\u001b[49m(learning_rate_scheduler)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LearningRateScheduler' is not defined"
     ]
    }
   ],
   "source": [
    "def learning_rate_scheduler(epoch, lr):\n",
    "    \"\"\"\n",
    "    Learning rate scheduler\n",
    "    :param int epoch: current epoch\n",
    "    :param float lr: current learning rate\n",
    "    \"\"\"\n",
    "    if epoch < 15:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "callback = LearningRateScheduler(learning_rate_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 7\n",
    "BATCH_SIZE = 3 # Fiscal Quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on our training data (train tensors)\n",
    "history = model.fit(\n",
    "    x_train_tensor[:-1, :], y_train_tensor[:-1, :],\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    callbacks = [callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over time\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Examining Results\n",
    "In this section, we're plotting the model's predictions versus the actual price point for the commodity in question. However, one plot focuses specifically on the testing data only (this is a better plot to see how well the model is performing/generalizing) and the other focuses on the entire dataset (this is a better plot to see if the model is correlating to the provided dataset at all).\n",
    "\n",
    "Therefore, when **evaluating the performance** of the model, **the first plot should be used.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model's predictions of the testing data vs the actual data\n",
    "predictions = model.predict(x_test_tensor)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "predictions = predictions.reshape(-1)\n",
    "\n",
    "plt.plot(y_test, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.title('Model Predictions (Testing Data Only)')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Months')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# We use Mean Absolute Error (MAE) as the final accuracy metric because it is the true error (difference) between the actual and predicted values\n",
    "final_mae = mean_absolute_error(y_test, predictions)\n",
    "print('Mean Absolute Error (FINAL ACCURACY METRIC):', final_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model's predictions (both training and testing) vs the actual data\n",
    "predictions = model.predict(x_train_tensor)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "predictions = predictions.reshape(-1)\n",
    "\n",
    "plt.plot(y_train, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.title('Model Predictions (Training and Testing Data)')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Days')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to saved_models\n",
    "model.save(f'saved_models/wheat_price_transformer_model_{final_mae}.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
