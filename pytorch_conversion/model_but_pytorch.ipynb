{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import data_loading as dl\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading the Experiment(s) Data\n",
    "Based on the data retrieved from `data_analysis.ipynb`, get that data into a format capable of being used by a Machine Learning model.\n",
    "\n",
    "<span style=\"color: orange;\">**Future Experiment:** Generating multiple months or simply estimating a year in advance.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  9\n"
     ]
    }
   ],
   "source": [
    "MODEL_DATA_DIRECTORY = '../model_data/wheat_model_data/'\n",
    "N_TREND_SAMPLES = 3 # Number of price samples for \"history\" trend (training data)\n",
    "N_MONTHS_AHEAD = 1 # Number of months ahead to predict\n",
    "TEST_SIZE = 0.2\n",
    "OUTPUT_COLUMN_NAME = 'Price'\n",
    "\n",
    "# Get the number of features for this model (given the model_data chosen)\n",
    "sample_file_path = os.path.join(MODEL_DATA_DIRECTORY, os.listdir(MODEL_DATA_DIRECTORY)[0])\n",
    "sample_columns = pd.read_csv(sample_file_path).columns[1:] # Skip 'Unnamed: 0' column\n",
    "num_input_samples = len(sample_columns[2:-1]) + N_TREND_SAMPLES + 1 # +1 for 'n_previous_prices' later\n",
    "print(\"Num Features: \", num_input_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Git\\Predicting-Commodity-Food-Pricing\\pytorch_conversion\\data_loading.py:176: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_data = pd.concat([model_data, data_to_concat], ignore_index=True).drop(columns=['Unnamed: 0'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Proteus2</th>\n",
       "      <th>Food Price Index</th>\n",
       "      <th>Cereals Price Index</th>\n",
       "      <th>Wheat Futures</th>\n",
       "      <th>Harvest</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>n_previous_prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-01</td>\n",
       "      <td>0.353945</td>\n",
       "      <td>0.368133</td>\n",
       "      <td>0.405947</td>\n",
       "      <td>0.370020</td>\n",
       "      <td>0.357385</td>\n",
       "      <td>0.383057</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.307030630305142, 0.2854220135885636, 0.3188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-03-01</td>\n",
       "      <td>0.368640</td>\n",
       "      <td>0.416001</td>\n",
       "      <td>0.374578</td>\n",
       "      <td>0.347719</td>\n",
       "      <td>0.327074</td>\n",
       "      <td>0.376625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.2854220135885636, 0.3188965093387809, 0.353...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-04-01</td>\n",
       "      <td>0.343996</td>\n",
       "      <td>0.468290</td>\n",
       "      <td>0.366889</td>\n",
       "      <td>0.343675</td>\n",
       "      <td>0.306746</td>\n",
       "      <td>0.369808</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3188965093387809, 0.3539453436152275, 0.368...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-05-01</td>\n",
       "      <td>0.401215</td>\n",
       "      <td>0.518191</td>\n",
       "      <td>0.377723</td>\n",
       "      <td>0.366430</td>\n",
       "      <td>0.333327</td>\n",
       "      <td>0.363512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3539453436152275, 0.3686397613884553, 0.343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-06-01</td>\n",
       "      <td>0.407073</td>\n",
       "      <td>0.569035</td>\n",
       "      <td>0.363607</td>\n",
       "      <td>0.356144</td>\n",
       "      <td>0.292171</td>\n",
       "      <td>0.357312</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3686397613884553, 0.3439957116460621, 0.401...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>0.348200</td>\n",
       "      <td>0.678242</td>\n",
       "      <td>0.588079</td>\n",
       "      <td>0.458857</td>\n",
       "      <td>0.333606</td>\n",
       "      <td>0.756849</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3168927164383402, 0.3456880352521089, 0.331...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>0.364530</td>\n",
       "      <td>0.656036</td>\n",
       "      <td>0.623093</td>\n",
       "      <td>0.439521</td>\n",
       "      <td>0.360232</td>\n",
       "      <td>0.756083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3456880352521089, 0.3311239570156358, 0.348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>0.393514</td>\n",
       "      <td>0.634037</td>\n",
       "      <td>0.639507</td>\n",
       "      <td>0.463615</td>\n",
       "      <td>0.388047</td>\n",
       "      <td>0.755328</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3311239570156358, 0.3482003383651588, 0.364...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>0.404325</td>\n",
       "      <td>0.610774</td>\n",
       "      <td>0.654949</td>\n",
       "      <td>0.473455</td>\n",
       "      <td>0.401033</td>\n",
       "      <td>0.754534</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3482003383651588, 0.3645302430734246, 0.393...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>0.410096</td>\n",
       "      <td>0.587743</td>\n",
       "      <td>0.659965</td>\n",
       "      <td>0.494896</td>\n",
       "      <td>0.418092</td>\n",
       "      <td>0.753753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3645302430734246, 0.3935141590851573, 0.404...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>887 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date     Price  Proteus2  Food Price Index  Cereals Price Index  \\\n",
       "0   2003-02-01  0.353945  0.368133          0.405947             0.370020   \n",
       "1   2003-03-01  0.368640  0.416001          0.374578             0.347719   \n",
       "2   2003-04-01  0.343996  0.468290          0.366889             0.343675   \n",
       "3   2003-05-01  0.401215  0.518191          0.377723             0.366430   \n",
       "4   2003-06-01  0.407073  0.569035          0.363607             0.356144   \n",
       "..         ...       ...       ...               ...                  ...   \n",
       "882 2016-08-01  0.348200  0.678242          0.588079             0.458857   \n",
       "883 2016-09-01  0.364530  0.656036          0.623093             0.439521   \n",
       "884 2016-10-01  0.393514  0.634037          0.639507             0.463615   \n",
       "885 2016-11-01  0.404325  0.610774          0.654949             0.473455   \n",
       "886 2016-12-01  0.410096  0.587743          0.659965             0.494896   \n",
       "\n",
       "     Wheat Futures   Harvest  Sentiment  \\\n",
       "0         0.357385  0.383057        1.0   \n",
       "1         0.327074  0.376625        1.0   \n",
       "2         0.306746  0.369808        1.0   \n",
       "3         0.333327  0.363512        1.0   \n",
       "4         0.292171  0.357312        1.0   \n",
       "..             ...       ...        ...   \n",
       "882       0.333606  0.756849        1.0   \n",
       "883       0.360232  0.756083        1.0   \n",
       "884       0.388047  0.755328        1.0   \n",
       "885       0.401033  0.754534        1.0   \n",
       "886       0.418092  0.753753        1.0   \n",
       "\n",
       "                                     n_previous_prices  \n",
       "0    [0.307030630305142, 0.2854220135885636, 0.3188...  \n",
       "1    [0.2854220135885636, 0.3188965093387809, 0.353...  \n",
       "2    [0.3188965093387809, 0.3539453436152275, 0.368...  \n",
       "3    [0.3539453436152275, 0.3686397613884553, 0.343...  \n",
       "4    [0.3686397613884553, 0.3439957116460621, 0.401...  \n",
       "..                                                 ...  \n",
       "882  [0.3168927164383402, 0.3456880352521089, 0.331...  \n",
       "883  [0.3456880352521089, 0.3311239570156358, 0.348...  \n",
       "884  [0.3311239570156358, 0.3482003383651588, 0.364...  \n",
       "885  [0.3482003383651588, 0.3645302430734246, 0.393...  \n",
       "886  [0.3645302430734246, 0.3935141590851573, 0.404...  \n",
       "\n",
       "[887 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = dl.get_model_data_from_directory(MODEL_DATA_DIRECTORY, sample_columns, N_TREND_SAMPLES)\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (887, 9)\n",
      "input: [0.3681326395264126, 0.4059467085633423, 0.3700199350892781, 0.3573849110164853, 0.383056505368272, 1.0, 0.307030630305142, 0.2854220135885636, 0.3188965093387809]\n",
      "x_train shape: (709, 9)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "format_for_ML_usage_torch() missing 1 required positional argument: 'num_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(inputs, outputs, test_size\u001b[38;5;241m=\u001b[39mTEST_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_train shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39marray(x_train)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 11\u001b[0m x_train_tensor, x_test_tensor, y_train_tensor, y_test_tensor, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_for_ML_usage_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_input_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_train_tensor shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, x_train_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_test_tensor shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, x_test_tensor\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mTypeError\u001b[0m: format_for_ML_usage_torch() missing 1 required positional argument: 'num_features'"
     ]
    }
   ],
   "source": [
    "inputs_and_outputs = model_data.apply(dl.divide_inputs_and_outputs, axis=1)\n",
    "inputs = inputs_and_outputs['inputs'].tolist(); outputs = inputs_and_outputs['output'].tolist()\n",
    "\n",
    "print('inputs shape:', np.array(inputs).shape)\n",
    "print('input:', inputs[0])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, outputs, test_size=TEST_SIZE, shuffle=False) \n",
    "\n",
    "print('x_train shape:', np.array(x_train).shape)\n",
    "\n",
    "x_train_tensor, x_test_tensor, y_train_tensor, y_test_tensor, scaler = dl.format_for_ML_usage_torch(inputs, outputs, num_input_samples, num_features=9)\n",
    "\n",
    "print('x_train_tensor shape:', x_train_tensor.shape)\n",
    "print('x_test_tensor shape:', x_test_tensor.shape)\n",
    "print('y_train_tensor shape:', y_train_tensor.shape)\n",
    "print('y_test_tensor shape:', y_test_tensor.shape)\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train_tensor, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_tensor, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_tensor, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_tensor, dtype=torch.float32)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_data = torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building Complex ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Simple, 1-Layer Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "HEAD_SIZE = 64\n",
    "NUM_HEADS = 8\n",
    "FF_DIM = 16\n",
    "\n",
    "# ML Optimizer\n",
    "LEARNING_RATE = 1e-3\n",
    "CLIP_VALUE = 0.5 # Gradient Clipping (https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem)\n",
    "DROP_OUT_RATE = 0.2\n",
    "\n",
    "d_model = NUM_HEADS\n",
    "length = x_train_tensor.shape[1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a long enough positional encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # Register as a buffer that is not a model parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is [batch_size, seq_len, feature_size]\n",
    "        # Adjust positional encoding to have the same size as the input\n",
    "        pe = self.pe[:x.size(1), :]  # Shape: [seq_len, d_model]\n",
    "        pe = pe.squeeze(1)  # Remove the singleton dimension\n",
    "        # Ensure pe is expanded to match the batch size of x\n",
    "        pe = pe.unsqueeze(0).repeat(x.size(0), 1, 1)  # Shape: [batch_size, seq_len, d_model]\n",
    "        # The add operation below should now be valid\n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, head_size, num_heads, ff_dim, dropout=0):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout)\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=ff_dim, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=ff_dim, out_channels=d_model, kernel_size=1)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Note: MultiheadAttention expects input of shape (L, N, E) where L is the sequence length, N is the batch size, and E is the embedding dimension.\n",
    "        attn_output, _ = self.attention(query, key, value)\n",
    "        out1 = self.layernorm1(query + attn_output)\n",
    "        \n",
    "        # Conv1D layers expect input of shape (N, C, L), hence we permute\n",
    "        out1_permuted = out1.permute(1, 2, 0)\n",
    "        ff_output = F.relu(self.conv1(out1_permuted))\n",
    "        ff_output = self.conv2(ff_output)\n",
    "        \n",
    "        # Permute back to match the MultiheadAttention output shape\n",
    "        ff_output = ff_output.permute(2, 0, 1)\n",
    "        out2 = self.layernorm2(out1 + self.dropout(ff_output))\n",
    "        \n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_input_samples, d_model, head_size, num_heads, ff_dim, dropout=0, num_transformers=10):\n",
    "        super(TransformerModel, self).__init__()  # Corrected super() call\n",
    "        self.d_model = d_model\n",
    "        self.input_projection = nn.Linear(9, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, num_input_samples)\n",
    "        self.transformers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, head_size, num_heads, ff_dim, dropout) \n",
    "            for _ in range(num_transformers)\n",
    "        ])\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)  # Projects input to d_model dimensions\n",
    "        x = x + self.pos_encoding(x)  # Apply positional encoding\n",
    "\n",
    "        for transformer in self.transformers:\n",
    "            x = transformer(x, x, x)  # Process through transformer blocks\n",
    "\n",
    "        x = x.mean(dim=1, keepdim=True)  # Aggregate features\n",
    "        x = self.output_layer(x)  # Apply the output layer to get the final prediction\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss function, and optimizer\n",
    "model = TransformerModel(num_input_samples, d_model=d_model, head_size=HEAD_SIZE, num_heads=NUM_HEADS, ff_dim=FF_DIM, dropout=DROP_OUT_RATE)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training the Model\n",
    "This section is specifically for training the model built in the previous section. Some contants (`NUM_EPOCHS`, `BATCH_SIZE`) are provided and should be the only required parameters to adjust for this section of the experiment. \n",
    "\n",
    "A plot of the loss throughout the training process is provided for easy understanding about if the model is overfitting or underfitting. For a review of these concepts, see [this article](https://www.analyticsfordecisions.com/overfitting-and-underfitting/#:~:text=Overfitting%20happens%20when%20the%20model%20is%20too%20complex,poor%20performance%20on%20both%20training%20and%20test%20datasets.).\n",
    "<br/><br/>\n",
    "**Potential Future Parameters**\n",
    "* **Regularization:** L1, L2, Dropout; helps prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def learning_rate_scheduler(epoch, lr):\n",
    "#     \"\"\"\n",
    "#     Learning rate scheduler\n",
    "#     :param int epoch: current epoch\n",
    "#     :param float lr: current learning rate\n",
    "#     \"\"\"\n",
    "#     if epoch < 15:\n",
    "#         return lr\n",
    "#     else:\n",
    "#         return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "# callback = LearningRateScheduler(learning_rate_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 3 # Fiscal Quarters\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(train_loader.dataset.tensors[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.\n",
    "    for batch, (input, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        output = output.reshape(-1)\n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Example data loading and training loop\n",
    "# Assuming data_loader is defined and loads your dataset\n",
    "train_loss = []\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss.append(train(model, train_loader, optimizer, criterion))\n",
    "    print(f'| epoch {epoch:3d} | train_loss {train_loss[-1]:5.2f} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over time\n",
    "plt.plot(train_loss)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Examining Results\n",
    "In this section, we're plotting the model's predictions versus the actual price point for the commodity in question. However, one plot focuses specifically on the testing data only (this is a better plot to see how well the model is performing/generalizing) and the other focuses on the entire dataset (this is a better plot to see if the model is correlating to the provided dataset at all).\n",
    "\n",
    "Therefore, when **evaluating the performance** of the model, **the first plot should be used.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is in evaluation mode\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Turn off gradients for validation, saves memory and computations\n",
    "with torch.no_grad():\n",
    "    predictions = model(x_test_tensor)\n",
    "    \n",
    "predictions = predictions.reshape(-1)\n",
    "plt.plot(y_test, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.title('Model Predictions (Testing Data Only)')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Months')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Mean Absolute Error\n",
    "final_mae = mean_absolute_error(y_test, predictions)\n",
    "print('Mean Absolute Error (FINAL ACCURACY METRIC):', final_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to saved_models\n",
    "model.save(f'saved_models/wheat_price_transformer_model_{final_mae}.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
