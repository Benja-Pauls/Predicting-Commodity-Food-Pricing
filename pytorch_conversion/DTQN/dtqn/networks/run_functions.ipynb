{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de063380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.29.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.4)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.40.6-py2.py3-none-any.whl (258 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.8.0)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "\u001b[33m  WARNING: The scripts wandb and wb are installed in '/home/iveyg/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed GitPython-3.1.42 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.40.6 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d692a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from time import time, sleep\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b077857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from gym import Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e12fd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: ``gym_gridverse`` is not installed. This means you cannot run an experiment with the `gv_*` domains.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'envs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env_processing, epsilon_anneal\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODEL_MAP, get_agent\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_global_seed, RNG\n",
      "File \u001b[0;32m~/Aiclub/Predicting-Commodity-Food-Pricing/pytorch_conversion/DTQN/dtqn/networks/utils/env_processing.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: ``gym_gridverse`` is not installed. This means you cannot run an experiment with the `gv_*` domains.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m     GymEnvironment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgv_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridVerseWrapper\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'envs'"
     ]
    }
   ],
   "source": [
    "from utils import env_processing, epsilon_anneal\n",
    "from utils.agent_utils import MODEL_MAP, get_agent\n",
    "from utils.random import set_global_seed, RNG\n",
    "from utils.logging_utils import RunningAverage, get_logger, timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4afbac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    agent,\n",
    "    eval_env: Env,\n",
    "    eval_episodes: int,\n",
    "    render: Optional[bool] = None,\n",
    "):\n",
    "    \"\"\"Evaluate the network for n_episodes using a greedy policy.\n",
    "\n",
    "    Arguments:\n",
    "        agent:          the agent to evaluate.\n",
    "        eval_env:       gym.Env, the environment to use for the evaluation.\n",
    "        eval_episodes:  int, the number of episodes to run.\n",
    "        render:         bool, whether or not to render the timesteps for enjoy mode.\n",
    "\n",
    "    Returns:\n",
    "        mean_success:           float, number of successes divided by number of episodes.\n",
    "        mean_return:            float, the average return per episode.\n",
    "        mean_episode_length:    float, the average episode length.\n",
    "    \"\"\"\n",
    "    # Set networks to eval mode (turns off dropout, etc.)\n",
    "    agent.eval_on()\n",
    "\n",
    "    total_reward = 0\n",
    "    num_successes = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for _ in range(eval_episodes):\n",
    "        agent.context_reset(eval_env.reset())\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        if render:\n",
    "            eval_env.render()\n",
    "            sleep(0.5)\n",
    "        while not done:\n",
    "            action = agent.get_action(epsilon=0.0)\n",
    "            obs_next, reward, done, info = eval_env.step(action)\n",
    "            agent.observe(obs_next, action, reward, done)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                eval_env.render()\n",
    "                if done:\n",
    "                    print(f\"Episode terminated. Episode reward: {ep_reward}\")\n",
    "                sleep(0.5)\n",
    "        total_reward += ep_reward\n",
    "        total_steps += agent.context.timestep\n",
    "        if info.get(\"is_success\", False) or ep_reward > 0:\n",
    "            num_successes += 1\n",
    "\n",
    "    # Set networks back to train mode\n",
    "    agent.eval_off()\n",
    "    # Prevent divide by 0\n",
    "    episodes = max(eval_episodes, 1)\n",
    "    return (\n",
    "        num_successes / episodes,\n",
    "        total_reward / episodes,\n",
    "        total_steps / episodes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8157ff91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epsilon_anneal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m      2\u001b[0m     agent,\n\u001b[1;32m      3\u001b[0m     envs: Tuple[Env],\n\u001b[1;32m      4\u001b[0m     eval_envs: Tuple[Env],\n\u001b[1;32m      5\u001b[0m     env_strs: Tuple[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m      6\u001b[0m     total_steps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m----> 7\u001b[0m     eps: \u001b[43mepsilon_anneal\u001b[49m\u001b[38;5;241m.\u001b[39mEpsilonAnneal,\n\u001b[1;32m      8\u001b[0m     eval_frequency: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      9\u001b[0m     eval_episodes: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     10\u001b[0m     policy_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     11\u001b[0m     save_policy: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m     12\u001b[0m     logger,\n\u001b[1;32m     13\u001b[0m     mean_success_rate: RunningAverage,\n\u001b[1;32m     14\u001b[0m     mean_episode_length: RunningAverage,\n\u001b[1;32m     15\u001b[0m     mean_reward: RunningAverage,\n\u001b[1;32m     16\u001b[0m     time_remaining: Optional[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m     17\u001b[0m     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the agent.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m        verbose:            bool, whether or not to print updates to standard out.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epsilon_anneal' is not defined"
     ]
    }
   ],
   "source": [
    "def train(\n",
    "    agent,\n",
    "    envs: Tuple[Env],\n",
    "    eval_envs: Tuple[Env],\n",
    "    env_strs: Tuple[str],\n",
    "    total_steps: int,\n",
    "    eps: epsilon_anneal.EpsilonAnneal,\n",
    "    eval_frequency: int,\n",
    "    eval_episodes: int,\n",
    "    policy_path: str,\n",
    "    save_policy: bool,\n",
    "    logger,\n",
    "    mean_success_rate: RunningAverage,\n",
    "    mean_episode_length: RunningAverage,\n",
    "    mean_reward: RunningAverage,\n",
    "    time_remaining: Optional[int],\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Train the agent.\n",
    "\n",
    "    Arguments:\n",
    "        agent:              the agent to train.\n",
    "        envs:               Tuple[gym.Env], the list of envs to train on.\n",
    "        eval_envs:          Tuple[gym.Env], the list of envs to evaluate with.\n",
    "        env_strs:           Tuple[str], the list of environment names.\n",
    "        total_steps:        int, the total number of timesteps to train.\n",
    "        eps:                EpsilonAnneal, the schedule to set for epsilon throughout training.\n",
    "        eval_frequency:     int, the number of training steps between evaluation periods.\n",
    "        eval_episodes:      int, the number of episodes to evaluate on for each eval period.\n",
    "        policy_path:        str, the path to store the policy and checkpoints at.\n",
    "        logger:             the logger to use (either wandb or csv).\n",
    "        mean_success_rate:  RunningAverage, the success rate over several evaluation periods.\n",
    "        mean_episode_length:RunningAverage, the episode length over several evaluation periods.\n",
    "        mean_reward:        RunningAverage, the episodic return over several evaluation periods.\n",
    "        time_remaining:     int, if using time limits, the amount of time left since starting the job.\n",
    "        verbose:            bool, whether or not to print updates to standard out.\n",
    "    \"\"\"\n",
    "    start_time = time()\n",
    "    # Turn on train mode\n",
    "    agent.eval_off()\n",
    "    # Choose an environment at the start and on every episode reset.\n",
    "    env = RNG.rng.choice(envs)\n",
    "    agent.context_reset(env.reset())\n",
    "\n",
    "    for timestep in range(agent.num_train_steps, total_steps):\n",
    "        done = step(agent, env, eps)\n",
    "\n",
    "        if done:\n",
    "            agent.replay_buffer.flush()\n",
    "            env = RNG.rng.choice(envs)\n",
    "            agent.context_reset(env.reset())\n",
    "        agent.train()\n",
    "        eps.anneal()\n",
    "\n",
    "        if timestep % eval_frequency == 0:\n",
    "            hours = (time() - start_time) / 3600\n",
    "            # Log training values\n",
    "            log_vals = {\n",
    "                \"losses/TD_Error\": agent.td_errors.mean(),\n",
    "                \"losses/Grad_Norm\": agent.grad_norms.mean(),\n",
    "                \"losses/Max_Q_Value\": agent.qvalue_max.mean(),\n",
    "                \"losses/Mean_Q_Value\": agent.qvalue_mean.mean(),\n",
    "                \"losses/Min_Q_Value\": agent.qvalue_min.mean(),\n",
    "                \"losses/Max_Target_Value\": agent.target_max.mean(),\n",
    "                \"losses/Mean_Target_Value\": agent.target_mean.mean(),\n",
    "                \"losses/Min_Target_Value\": agent.target_min.mean(),\n",
    "                \"losses/hours\": hours,\n",
    "            }\n",
    "            # Perform an evaluation for each of the eval environments and add to our log\n",
    "            for env_str, eval_env in zip(env_strs, eval_envs):\n",
    "                sr, ret, length = evaluate(agent, eval_env, eval_episodes)\n",
    "\n",
    "                log_vals.update(\n",
    "                    {\n",
    "                        f\"{env_str}/SuccessRate\": sr,\n",
    "                        f\"{env_str}/Return\": ret,\n",
    "                        f\"{env_str}/EpisodeLength\": length,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Commit the log values.\n",
    "            logger.log(\n",
    "                log_vals,\n",
    "                step=timestep,\n",
    "            )\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"[ {timestamp()} ] Training Steps: {timestep}, Env: {env_str}, Success Rate: {sr:.2f}, Return: {ret:.2f}, Episode Length: {length:.2f}, Hours: {hours:.2f}\"\n",
    "                )\n",
    "\n",
    "        if save_policy and timestep % 50_000 == 0:\n",
    "            torch.save(agent.policy_network.state_dict(), policy_path)\n",
    "\n",
    "        if time_remaining and time() - start_time >= time_remaining:\n",
    "            print(\n",
    "                f\"Reached time limit. Saving checkpoint with {agent.num_train_steps} steps completed.\"\n",
    "            )\n",
    "\n",
    "            agent.save_checkpoint(\n",
    "                policy_path,\n",
    "                wandb.run.id if logger == wandb else None,\n",
    "                mean_success_rate,\n",
    "                mean_reward,\n",
    "                mean_episode_length,\n",
    "                eps,\n",
    "            )\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(agent, env: Env, eps: float) -> bool:\n",
    "    \"\"\"Use the agent's policy to get the next action, take it, and then record the result.\n",
    "\n",
    "    Arguments:\n",
    "        agent:  the agent to use.\n",
    "        env:    gym.Env\n",
    "        eps:    the epsilon value (for epsilon-greedy policy)\n",
    "\n",
    "    Returns:\n",
    "        done: bool, whether or not the episode has finished.\n",
    "    \"\"\"\n",
    "    action = agent.get_action(epsilon=eps.val)\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    # OpenAI Gym TimeLimit truncation: don't store it in the buffer as done\n",
    "    if info.get(\"TimeLimit.truncated\", False):\n",
    "        buffer_done = False\n",
    "    else:\n",
    "        buffer_done = done\n",
    "\n",
    "    agent.observe(next_obs, action, reward, buffer_done)\n",
    "    return done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb645610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepopulate(agent, prepop_steps: int, envs: Tuple[Env]) -> None:\n",
    "    \"\"\"Prepopulate the replay buffer. Sample an enviroment on each episode.\n",
    "\n",
    "    Arguments:\n",
    "        agent:          the agent whose buffer needs to be stored.\n",
    "        prepop_steps:   int, the number of timesteps to populate.\n",
    "        envs:           Tuple[gym.Env], the list of environments to use for sampling.\n",
    "    \"\"\"\n",
    "    timestep = 0\n",
    "    while timestep < prepop_steps:\n",
    "        env = RNG.rng.choice(envs)\n",
    "        agent.context_reset(env.reset())\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = RNG.rng.integers(env.action_space.n)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            # OpenAI Gym TimeLimit truncation: don't store it in the buffer as done\n",
    "            if info.get(\"TimeLimit.truncated\", False):\n",
    "                buffer_done = False\n",
    "            else:\n",
    "                buffer_done = done\n",
    "\n",
    "            agent.observe(next_obs, action, reward, buffer_done)\n",
    "            timestep += 1\n",
    "        agent.replay_buffer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9bcbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(args):\n",
    "    \"\"\"Uses the command-line arguments to create the agent and associated tools, then begin training.\"\"\"\n",
    "    start_time = time()\n",
    "    # Create envs, set seed, create RL agent\n",
    "    envs = []\n",
    "    eval_envs = []\n",
    "    for env_str in args.envs:\n",
    "        envs.append(env_processing.make_env(env_str))\n",
    "        eval_envs.append(env_processing.make_env(env_str))\n",
    "    device = torch.device(args.device)\n",
    "    set_global_seed(args.seed, *(envs + eval_envs))\n",
    "\n",
    "    eps = epsilon_anneal.LinearAnneal(1.0, 0.1, args.num_steps // 10)\n",
    "\n",
    "    agent = get_agent(\n",
    "        args.model,\n",
    "        envs,\n",
    "        args.obs_embed,\n",
    "        args.a_embed,\n",
    "        args.in_embed,\n",
    "        args.buf_size,\n",
    "        device,\n",
    "        args.lr,\n",
    "        args.batch,\n",
    "        args.context,\n",
    "        args.max_episode_steps,\n",
    "        args.history,\n",
    "        args.tuf,\n",
    "        args.discount,\n",
    "        # DTQN specific\n",
    "        args.heads,\n",
    "        args.layers,\n",
    "        args.dropout,\n",
    "        args.identity,\n",
    "        args.gate,\n",
    "        args.pos,\n",
    "        args.bag_size,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[ {timestamp()} ] Creating {args.model} with {sum(p.numel() for p in agent.policy_network.parameters())} parameters\"\n",
    "    )\n",
    "\n",
    "    # Create logging dir\n",
    "    policy_save_dir = os.path.join(\n",
    "        os.getcwd(), \"policies\", args.project_name, *args.envs\n",
    "    )\n",
    "    os.makedirs(policy_save_dir, exist_ok=True)\n",
    "    policy_path = os.path.join(\n",
    "        policy_save_dir,\n",
    "        f\"model={args.model}_envs={','.join(args.envs)}_obs_embed={args.obs_embed}_a_embed={args.a_embed}_in_embed={args.in_embed}_context={args.context}_heads={args.heads}_layers={args.layers}_\"\n",
    "        f\"batch={args.batch}_gate={args.gate}_identity={args.identity}_history={args.history}_pos={args.pos}_bag={args.bag_size}_seed={args.seed}\",\n",
    "    )\n",
    "\n",
    "    # Enjoy mode\n",
    "    if args.render:\n",
    "        agent.policy_network.load_state_dict(\n",
    "            torch.load(policy_path, map_location=\"cpu\")\n",
    "        )\n",
    "        evaluate(agent, eval_envs[0], 1_000_000, render=True)\n",
    "\n",
    "    # If there is already a saved checkpoint, load it and resume training if more steps are needed\n",
    "    # Or exit early if we have already finished training.\n",
    "    if os.path.exists(policy_path + \"_mini_checkpoint.pt\"):\n",
    "        steps_completed = agent.load_mini_checkpoint(policy_path)[\"step\"]\n",
    "        print(\n",
    "            f\"Found a mini checkpoint that completed {steps_completed} training steps.\"\n",
    "        )\n",
    "        if steps_completed >= args.num_steps:\n",
    "            print(f\"Removing checkpoint and exiting...\")\n",
    "            if os.path.exists(policy_path + \"_checkpoint.pt\"):\n",
    "                os.remove(policy_path + \"_checkpoint.pt\")\n",
    "            exit(0)\n",
    "        else:\n",
    "            (\n",
    "                wandb_id,\n",
    "                mean_success_rate,\n",
    "                mean_reward,\n",
    "                mean_episode_length,\n",
    "                eps_val,\n",
    "            ) = agent.load_checkpoint(policy_path)\n",
    "            eps.val = eps_val\n",
    "            wandb_kwargs = {\"resume\": \"must\", \"id\": wandb_id}\n",
    "    # Begin training from scratch\n",
    "    else:\n",
    "        wandb_kwargs = {\"resume\": None}\n",
    "        # Prepopulate the replay buffer\n",
    "        prepopulate(agent, 50_000, envs)\n",
    "        mean_success_rate = RunningAverage(10)\n",
    "        mean_reward = RunningAverage(10)\n",
    "        mean_episode_length = RunningAverage(10)\n",
    "\n",
    "    # Logging setup\n",
    "    logger = get_logger(policy_path, args, wandb_kwargs)\n",
    "\n",
    "    time_remaining = (\n",
    "        args.time_limit * 3600 - (time() - start_time) if args.time_limit else None\n",
    "    )\n",
    "\n",
    "    train(\n",
    "        agent,\n",
    "        envs,\n",
    "        eval_envs,\n",
    "        args.envs,\n",
    "        args.num_steps,\n",
    "        eps,\n",
    "        args.eval_frequency,\n",
    "        args.eval_episodes,\n",
    "        policy_path,\n",
    "        args.save_policy,\n",
    "        logger,\n",
    "        mean_success_rate,\n",
    "        mean_reward,\n",
    "        mean_episode_length,\n",
    "        time_remaining,\n",
    "        args.verbose,\n",
    "    )\n",
    "\n",
    "    # Save a small checkpoint if we finish training to let following runs know we are finished\n",
    "    agent.save_mini_checkpoint(\n",
    "        checkpoint_dir=policy_path, wandb_id=wandb.run.id if logger == wandb else None\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
