{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commodity Food Pricing: Model Experiments\n",
    "\n",
    "**By:** `MSOE AI-Club \"Nourish\" Student Research Team`<br/>\n",
    "**Primary Notebook Developer:** Ben Paulson, ____\n",
    "\n",
    "**Notebook Purpose:** To explore the use of machine learning to predict the price of commodities in the food industry. By utilizing the clean datasets loaded from `data_analysis.ipynb` into the directory `model_data`, we can begin to explore the use of machine learning to predict the price of commodities in the food industry. This notebook will be divided into sections of varying complexity, each of which will explore a different model type and its performance on the data. The models will be evaluated based on their ability to predict the price of a commodity in the future, given a set of features. The features will be selected based on their correlation to the price of the commodity, as determined in `data_analysis.ipynb`. There is also significant documentation with each section for both code/purpose sanity, sake of future reproducibility, and to help other members of the `MSOE AI-Club \"Nourish\" Student Research Team` understand the code and its purpose.\n",
    "* **Part 1: Loading the Experiment(s) Data**\n",
    "* **Part 2: Building Complex ML Models**\n",
    "    * **Model 1:** Simple, 1-layer Transformer\n",
    "    * **Model 2:** ...\n",
    "* **Part 3: Experiments with Training Methods**\n",
    "    * **Method 1:** ...\n",
    "    * **Method 2:** ...\n",
    "\n",
    "**Research Context:** This research is being conducted as part of the MSOE AI-Club's \"Nourish\" project, which aims to use machine learning to predict future food prices in order to help farmers in developing countries make better decisions about food storage and crop selection, achieved through the accurate warning administration and prediction of food commodity pricing data by country and food market. This project is pending as part of a relationship with the United Nation's Food and Agriculture Organization (FAO). [FAO Official Statement on the Importance of Research Like This](https://youtu.be/sZx3hhnEHiI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data for model usage\n",
    "model_data = pd.read_csv('model_data/argentina_wheat_model_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading the Experiment(s) Data\n",
    "Based on the data retrieved from `data_analysis.ipynb`, get that data into a format capable of being used by a Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TREND_SAMPLES = 3\n",
    "TEST_SIZE = 0.2\n",
    "OUTPUT_COLUMN_NAME = 'Price'\n",
    "\n",
    "num_input_samples = len(model_data.columns[2:-1]) + N_TREND_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_n_previous_prices(model_data, n):\n",
    "    \"\"\"\n",
    "    For the given model_data, grab the n previous prices and store as a list\n",
    "    in a new column called 'n_previous_prices' for each row's associated 'Date'.\n",
    "    The current date's price should not be included in the list of previous prices.\n",
    "    :param pd.DataFrame model_data: data (model_data to grab from)\n",
    "    :param int n: Number of previous prices to grab\n",
    "    \"\"\"\n",
    "    # Create a new column to store the n_previous_prices\n",
    "    model_data['n_previous_prices'] = None\n",
    "\n",
    "    # Iterate through each row\n",
    "    for index, row in model_data.iterrows():\n",
    "        # Grab the date and price for the current row\n",
    "        date = row['Date']\n",
    "        price = row['Price']\n",
    "\n",
    "        # Grab the n previous prices\n",
    "        n_previous_prices = model_data.loc[model_data['Date'] < date]['Price'].tail(n).tolist()\n",
    "\n",
    "        # Store the n_previous_prices in the new column\n",
    "        model_data.at[index, 'n_previous_prices'] = n_previous_prices\n",
    "\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_inputs_and_outputs(row):\n",
    "    \"\"\"\n",
    "    Create a column to specifically be input into an ML model and a column for output\n",
    "    :param pd.DataFrame row: row to divide\n",
    "\n",
    "    :return: row with input and output columns\n",
    "    \"\"\"\n",
    "    output = row[OUTPUT_COLUMN_NAME]\n",
    "    inputs = row[2:-1].tolist()\n",
    "\n",
    "    # For each element in n_previous_price, add it to the inputs\n",
    "    for price in row['n_previous_prices']:\n",
    "        inputs.append(price)\n",
    "\n",
    "    return pd.Series([inputs, output], index=['inputs', 'output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_ML_usage(inputs, outputs):\n",
    "    \"\"\"\n",
    "    Given the inputs and outputs for the model, format the data to be used for ML.\n",
    "    This is accomplished by splitting the data into training/test tensors\n",
    "    :param inputs: inputs for the model (list of lists)\n",
    "    :param output: output for the model (list of values)\n",
    "\n",
    "    :return: The tensors responsible for testing/training the model and the scaler used to scale the data\n",
    "            (x_train_tensor, x_test_tensor, y_train_tensor, y_test_tensor, scaler)\n",
    "    \"\"\"\n",
    "    # Create a train/test split for the dataset\n",
    "    x_train, x_test, y_train, y_test = train_test_split(inputs, outputs, test_size=TEST_SIZE, shuffle=False)\n",
    "\n",
    "    # Convert them all to numpy arrays\n",
    "    x_train = np.array(x_train).reshape(-1, num_input_samples)\n",
    "    x_test = np.array(x_test).reshape(-1, num_input_samples)\n",
    "    y_train = np.array(y_train).reshape(-1, 1) # required reshape for StandardScaler\n",
    "    y_test = np.array(y_test).reshape(-1, 1) # required reshape for StandardScaler\n",
    "\n",
    "    # Standardize/Normalize the dataset\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train).reshape(-1, num_input_samples, 1)\n",
    "    x_test_scaled = scaler.transform(x_test).reshape(-1, num_input_samples, 1)\n",
    "    y_train_scaled = scaler.fit_transform(y_train)\n",
    "    y_test_scaled = scaler.transform(y_test)\n",
    "\n",
    "    # Convert to tensors for model ingestion (true training data)\n",
    "    x_train_tensor = tf.convert_to_tensor(x_train_scaled)\n",
    "    x_test_tensor = tf.convert_to_tensor(x_test_scaled)\n",
    "    y_train_tensor = tf.convert_to_tensor(y_train_scaled)\n",
    "    y_test_tensor = tf.convert_to_tensor(y_test_scaled)\n",
    "\n",
    "    return (x_train_tensor, x_test_tensor, y_train_tensor, y_test_tensor, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Proteus2</th>\n",
       "      <th>Food Price Index</th>\n",
       "      <th>Cereals Price Index</th>\n",
       "      <th>Wheat Futures</th>\n",
       "      <th>Harvest</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>n_previous_prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2003-05-01</td>\n",
       "      <td>0.327689</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.452335</td>\n",
       "      <td>0.430506</td>\n",
       "      <td>344.21875</td>\n",
       "      <td>2936474.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3052158461377652, 0.3120100344935716, 0.298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2003-06-01</td>\n",
       "      <td>0.330302</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.450416</td>\n",
       "      <td>0.426973</td>\n",
       "      <td>315.45000</td>\n",
       "      <td>2936474.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3120100344935716, 0.2981080798578447, 0.327...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2003-07-01</td>\n",
       "      <td>0.296854</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.456174</td>\n",
       "      <td>0.418139</td>\n",
       "      <td>320.81250</td>\n",
       "      <td>2936474.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.2981080798578447, 0.3276889306992788, 0.330...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2003-08-01</td>\n",
       "      <td>0.320268</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.463212</td>\n",
       "      <td>0.443463</td>\n",
       "      <td>361.75000</td>\n",
       "      <td>2936474.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3276889306992788, 0.3303020800668966, 0.296...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2003-09-01</td>\n",
       "      <td>0.317341</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.472169</td>\n",
       "      <td>0.448174</td>\n",
       "      <td>348.03125</td>\n",
       "      <td>2936474.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3303020800668966, 0.2968537681613881, 0.320...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>0.449462</td>\n",
       "      <td>0.252506</td>\n",
       "      <td>0.648752</td>\n",
       "      <td>0.559482</td>\n",
       "      <td>402.43750</td>\n",
       "      <td>2679728.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.422284937807045, 0.4390090937597993, 0.4390...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>0.420717</td>\n",
       "      <td>0.252506</td>\n",
       "      <td>0.653871</td>\n",
       "      <td>0.540636</td>\n",
       "      <td>407.78125</td>\n",
       "      <td>2679728.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.4390090937597993, 0.4390090937597993, 0.449...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>0.384656</td>\n",
       "      <td>0.252506</td>\n",
       "      <td>0.653231</td>\n",
       "      <td>0.543581</td>\n",
       "      <td>414.17500</td>\n",
       "      <td>2679728.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.4390090937597993, 0.4494616912302707, 0.420...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>0.368768</td>\n",
       "      <td>0.252506</td>\n",
       "      <td>0.651951</td>\n",
       "      <td>0.538869</td>\n",
       "      <td>408.40625</td>\n",
       "      <td>2679728.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.4494616912302707, 0.4207170481864744, 0.384...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>0.351207</td>\n",
       "      <td>0.252506</td>\n",
       "      <td>0.647473</td>\n",
       "      <td>0.540047</td>\n",
       "      <td>405.90625</td>\n",
       "      <td>2679728.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.4207170481864744, 0.3846555869133479, 0.368...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date     Price  Proteus2  Food Price Index  Cereals Price Index  \\\n",
       "161  2003-05-01  0.327689  0.284869          0.452335             0.430506   \n",
       "160  2003-06-01  0.330302  0.284869          0.450416             0.426973   \n",
       "159  2003-07-01  0.296854  0.284869          0.456174             0.418139   \n",
       "158  2003-08-01  0.320268  0.284869          0.463212             0.443463   \n",
       "157  2003-09-01  0.317341  0.284869          0.472169             0.448174   \n",
       "..          ...       ...       ...               ...                  ...   \n",
       "4    2016-08-01  0.449462  0.252506          0.648752             0.559482   \n",
       "3    2016-09-01  0.420717  0.252506          0.653871             0.540636   \n",
       "2    2016-10-01  0.384656  0.252506          0.653231             0.543581   \n",
       "1    2016-11-01  0.368768  0.252506          0.651951             0.538869   \n",
       "0    2016-12-01  0.351207  0.252506          0.647473             0.540047   \n",
       "\n",
       "     Wheat Futures    Harvest  Sentiment  \\\n",
       "161      344.21875  2936474.0        1.0   \n",
       "160      315.45000  2936474.0        1.0   \n",
       "159      320.81250  2936474.0        1.0   \n",
       "158      361.75000  2936474.0        1.0   \n",
       "157      348.03125  2936474.0        1.0   \n",
       "..             ...        ...        ...   \n",
       "4        402.43750  2679728.0        1.0   \n",
       "3        407.78125  2679728.0        1.0   \n",
       "2        414.17500  2679728.0        1.0   \n",
       "1        408.40625  2679728.0        1.0   \n",
       "0        405.90625  2679728.0        1.0   \n",
       "\n",
       "                                     n_previous_prices  \n",
       "161  [0.3052158461377652, 0.3120100344935716, 0.298...  \n",
       "160  [0.3120100344935716, 0.2981080798578447, 0.327...  \n",
       "159  [0.2981080798578447, 0.3276889306992788, 0.330...  \n",
       "158  [0.3276889306992788, 0.3303020800668966, 0.296...  \n",
       "157  [0.3303020800668966, 0.2968537681613881, 0.320...  \n",
       "..                                                 ...  \n",
       "4    [0.422284937807045, 0.4390090937597993, 0.4390...  \n",
       "3    [0.4390090937597993, 0.4390090937597993, 0.449...  \n",
       "2    [0.4390090937597993, 0.4494616912302707, 0.420...  \n",
       "1    [0.4494616912302707, 0.4207170481864744, 0.384...  \n",
       "0    [0.4207170481864744, 0.3846555869133479, 0.368...  \n",
       "\n",
       "[162 rows x 9 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort model data where the earliest date comes first and the latest date comes last\n",
    "model_data = model_data.sort_values(by='Date', ascending=True)\n",
    "model_data = grab_n_previous_prices(model_data, N_TREND_SAMPLES)\n",
    "\n",
    "# Remove rows where the len(n_previous_prices) != N_TREND_SAMPLES\n",
    "model_data = model_data[model_data['n_previous_prices'].apply(lambda x: len(x) == N_TREND_SAMPLES)]\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_tensor shape: (129, 9, 1)\n",
      "x_test_tensor shape: (33, 9, 1)\n",
      "y_train_tensor shape: (129, 1)\n",
      "y_test_tensor shape: (33, 1)\n"
     ]
    }
   ],
   "source": [
    "inputs_and_outputs = model_data.apply(divide_inputs_and_outputs, axis=1)\n",
    "inputs = inputs_and_outputs['inputs'].tolist(); outputs = inputs_and_outputs['output'].tolist()\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, outputs, test_size=TEST_SIZE, shuffle=False) # Don't shuffle, should be cohesive samples not seen\n",
    "x_train_tensor, x_test_tensor, y_train_tensor, y_test_tensor, scaler = format_for_ML_usage(inputs, outputs)\n",
    "\n",
    "print('x_train_tensor shape:', x_train_tensor.shape)\n",
    "print('x_test_tensor shape:', x_test_tensor.shape)\n",
    "print('y_train_tensor shape:', y_train_tensor.shape)\n",
    "print('y_test_tensor shape:', y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building Complex ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define positional encoding function (another portion for data preprocessing)\n",
    "def positional_encoding(length, depth):\n",
    "    \"\"\" \n",
    "    Create positional encoding\n",
    "    args:\n",
    "        length: length of the sequence\n",
    "        depth: depth of the model\n",
    "    \"\"\"\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / depth) for j in range(depth)]\n",
    "        for pos in range(length)\n",
    "    ])\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])\n",
    "    return tf.cast(pos_enc, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Simple, 1-Layer Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD_SIZE = 64\n",
    "NUM_HEADS = 16\n",
    "FF_DIM = 16\n",
    "\n",
    "d_model = x_train_tensor.shape[-1] \n",
    "length = x_train_tensor.shape[1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(head_size, num_heads, ff_dim, dropout=0):\n",
    "    \"\"\"\n",
    "    Building a simple transformer based off the original work compiled in\n",
    "    the research paper \"Attention Is All You Need\" (https://arxiv.org/pdf/1706.03762.pdf)\n",
    "    :param int head_size: Size of the attention heads\n",
    "    :param int num_heads: Number of heads\n",
    "    :param int ff_dim: Feed forward dimension\n",
    "    :param float dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input Layer\n",
    "    inputs = tf.keras.layers.Input(shape=(num_input_samples, d_model))\n",
    "\n",
    "    # Add positional encoding to the input\n",
    "    # The positional encoding is added to the input in order to give the model some information about the relative position of the words in the sequence\n",
    "    # Not including the positional encoding is basically the same as randomizing the order of the data\n",
    "    positional_inputs = inputs + positional_encoding(num_input_samples, d_model)\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(positional_inputs)\n",
    "    x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    \n",
    "    # Add & Norm\n",
    "    res = x + positional_inputs  # Skip connection\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    \n",
    "    # Feed-Forward Network (Using Dense layers instead of Conv1D layers)\n",
    "    x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    \n",
    "    # Skip Connection\n",
    "    x = x + res\n",
    "\n",
    "    # Global Average Pooling layer\n",
    "    # The Global Average Pooling layer reduces the dimensionality/complexity of the data\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,outputs = transformer(HEAD_SIZE, NUM_HEADS, FF_DIM)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Experiments with Training Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 2s 10ms/step - loss: nan\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 88/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Train the model on our training data (train tensors)\n",
    "history = model.fit(\n",
    "    x_train_tensor, y_train_tensor,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\paulsonb\\OneDrive - Milwaukee School of Engineering\\Documents\\Student Orgs\\MAIC\\Research\\Commodity Pricing Prediction\\Experiments\\Predicting-Commodity-Food-Pricing\\model_experiments.ipynb Cell 23\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/paulsonb/OneDrive%20-%20Milwaukee%20School%20of%20Engineering/Documents/Student%20Orgs/MAIC/Research/Commodity%20Pricing%20Prediction/Experiments/Predicting-Commodity-Food-Pricing/model_experiments.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Plot the prediction result\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/paulsonb/OneDrive%20-%20Milwaukee%20School%20of%20Engineering/Documents/Student%20Orgs/MAIC/Research/Commodity%20Pricing%20Prediction/Experiments/Predicting-Commodity-Food-Pricing/model_experiments.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dates \u001b[39m=\u001b[39m model_data[\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/paulsonb/OneDrive%20-%20Milwaukee%20School%20of%20Engineering/Documents/Student%20Orgs/MAIC/Research/Commodity%20Pricing%20Prediction/Experiments/Predicting-Commodity-Food-Pricing/model_experiments.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39;49mscatter(dates, y_pred, color\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mred\u001b[39;49m\u001b[39m'\u001b[39;49m, label\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPredicted Values\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/paulsonb/OneDrive%20-%20Milwaukee%20School%20of%20Engineering/Documents/Student%20Orgs/MAIC/Research/Commodity%20Pricing%20Prediction/Experiments/Predicting-Commodity-Food-Pricing/model_experiments.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mscatter(dates, y_test, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m'\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mActual Values\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/paulsonb/OneDrive%20-%20Milwaukee%20School%20of%20Engineering/Documents/Student%20Orgs/MAIC/Research/Commodity%20Pricing%20Prediction/Experiments/Predicting-Commodity-Food-Pricing/model_experiments.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mMonths Since September 2010\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\paulsonb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\pyplot.py:2862\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mscatter)\n\u001b[0;32m   2858\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscatter\u001b[39m(\n\u001b[0;32m   2859\u001b[0m         x, y, s\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, c\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, marker\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2860\u001b[0m         vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, linewidths\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[0;32m   2861\u001b[0m         edgecolors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, plotnonfinite\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2862\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mscatter(\n\u001b[0;32m   2863\u001b[0m         x, y, s\u001b[39m=\u001b[39;49ms, c\u001b[39m=\u001b[39;49mc, marker\u001b[39m=\u001b[39;49mmarker, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm,\n\u001b[0;32m   2864\u001b[0m         vmin\u001b[39m=\u001b[39;49mvmin, vmax\u001b[39m=\u001b[39;49mvmax, alpha\u001b[39m=\u001b[39;49malpha, linewidths\u001b[39m=\u001b[39;49mlinewidths,\n\u001b[0;32m   2865\u001b[0m         edgecolors\u001b[39m=\u001b[39;49medgecolors, plotnonfinite\u001b[39m=\u001b[39;49mplotnonfinite,\n\u001b[0;32m   2866\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   2867\u001b[0m     sci(__ret)\n\u001b[0;32m   2868\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mc:\\Users\\paulsonb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py:1461\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m   1459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1460\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1461\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1463\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1464\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[0;32m   1465\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\paulsonb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:4578\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4576\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mma\u001b[39m.\u001b[39mravel(y)\n\u001b[0;32m   4577\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39msize \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39msize:\n\u001b[1;32m-> 4578\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mx and y must be the same size\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   4580\u001b[0m \u001b[39mif\u001b[39;00m s \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4581\u001b[0m     s \u001b[39m=\u001b[39m (\u001b[39m20\u001b[39m \u001b[39mif\u001b[39;00m mpl\u001b[39m.\u001b[39mrcParams[\u001b[39m'\u001b[39m\u001b[39m_internal.classic_mode\u001b[39m\u001b[39m'\u001b[39m] \u001b[39melse\u001b[39;00m\n\u001b[0;32m   4582\u001b[0m          mpl\u001b[39m.\u001b[39mrcParams[\u001b[39m'\u001b[39m\u001b[39mlines.markersize\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2.0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the prediction result\n",
    "dates = model_data['Date'].tolist()\n",
    "plt.scatter(dates, y_pred, color='red', label='Predicted Values')\n",
    "plt.scatter(dates, y_test, color='blue', label='Actual Values') \n",
    "plt.xlabel('Months Since September 2010')\n",
    "plt.ylabel('Cost Index of Crop Products in Nigerian Naira')\n",
    "plt.title('Actual vs Predicted Values with Transformer')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
