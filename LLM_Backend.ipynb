{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commodity Food Pricing: Chatbot LLM\n",
    "\n",
    "**By:** `MSOE AI-Club \"Nourish\" Student Research Team`<br/>\n",
    "\n",
    "**Notebook Purpose:** This notebook provides the code implementation of a Large Language Model (LLM) with an algorithm developed for the application of providing context to the severity warnings of commodity food prices. This algorithm receives great responses for a question-thought-response framework with an open-source LLM Llama.cpp (https://github.com/ggerganov/llama.cpp).\n",
    "\n",
    "Llama.cpp was chosen as a demonstrative experiment due to its small size (allowing it to be run on many consumer-grade hardware), Llama's proficiency in chat-bot related tasks, and its small size due to its implementation in C/C++ without the use of external ML/Tensor libraries. Lessoning the amount of external libraries used also increases the security of this system.\n",
    "\n",
    "This algorithm has been proven to work using a basic T4 (\"teaching\") node on the [MSOE ROSIE Supercomputer](https://www.msoe.edu/about-msoe/news/details/meet-rosie/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPTIONAL] Install llama.cpp here!\n",
    "# %env CMAKE_ARGS=-DLLAMA_CUBLAS=on\n",
    "# %env FORCE_CMAKE=1\n",
    "# %pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --no-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from llama_cpp import ChatCompletionRequestResponseFormat, ChatCompletionMessageToolCall\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "Each of these constants are surface-level changes that any developer could make to slightly tune the system to different datasets, alternative LLMs, and getting different responses from the internal LLMs.\n",
    "\n",
    "**Constant's Descriptions:**\n",
    "* **LLM_FILE_PATH:** File path of the Large Langauge Model (LLM) that will be used throughout this chat-bot's prompt/thought/response framework.\n",
    "* **TEMPERATURE:** The \"temperature\" is a common parameter for LLMs, and quantifies the risk that the LLM should take in its responses. Changing this constant here will change the metric for all LLM responses. You can read more about temperature [here](https://medium.com/@lazyprogrammerofficial/what-is-temperature-in-nlp-llms-aa2a7212e687).\n",
    "* **COMMUNICATIVE_LLM_SYSTEM_CONFIG:** The \"system prompt\" for the main llm responsible for speaking with the user. A system prompt is the \"back story\" of a model and is the main way to alter the behavior of the LLM.\n",
    "* **RESPONSE_FILTER_LLM_SYSTEM_CONFIG:** The \"system prompt\" for the main llm responsible for filtering the user's questions in the case that they're deemed inappropriate for the main communicative chat bot to respond to. A system prompt is the \"back story\" of a model and is the main way to alter the behavior of the LLM.\n",
    "* **PROFILE_BUILDER_LLM_SYSTEM_CONFIG:** The \"system prompt\" for the main llm responsible for building a profile of the user as it has a conversation with the chat bot. A system prompt is the \"back story\" of a model and is the main way to alter the behavior of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TO_EMBED = ['data/NS_Providers.xlsx', 'data/ASD Videos.csv', 'data/Blog Data.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_FILE_PATH = '/data/ai_club/llms/llama-2-7b-chat.Q5_K_M.gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Argentina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commodity = 'corn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMUNICATIVE_LLM_SYSTEM_CONFIG = {\n",
    "    'role': 'system',\n",
    "    'content': \"\"\"\n",
    "You are a chat-bot responsible for responding with concise advice on behalf of the United Nations Food and Agriculture Organization (FAO). \n",
    "You specifically provide advice in regards to the warnings sent out on the Global Information and Early Warning System on Food and Agriculture \n",
    "(GIEWS) where specific countries have no warning, a moderate warning, or a high warning in regards to food insecurity and price. \n",
    "Understanding this, and given that a user speaks English and is from the \"\"\" + country + \"\"\" which is currently under high warning for \"\"\"+commodity+\"\"\", \n",
    "write a preliminary message to start the conversation and outline the situation. Remember, your messages should be concise and provide the most \n",
    "amount of information possible in the shortest amount of text. One of your messages should never exceed more than 45 words.\n",
    " \n",
    "Your message should be more targeted towards an individual rather than a government official or an internal UN statistician. \n",
    "Change the tone/jargon of your message to match this new requirement.\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_FILTER_LLM_SYSTEM_CONFIG = { # Yes = Should not be filtered (not allowed), No = Should be filtered (allowed)\n",
    "    'role': 'system',\n",
    "    'content': \"\"\"\n",
    "    You are a professional assistant that filters incoming messages from a user before they reach a chat bot.\n",
    "    The chat bot you are protecting is only able to answer questions about food securtity, commodity food prices, \n",
    "    commodity food price warnings, food price warnings, or other questions that are appropriate and friendly for a chat bot on the FAO's web page. \n",
    "    Therefore, when you receive a message, you must respond with either \"no\" when the user's message is appropriate, or \"yes\" when the \n",
    "    user's message is not appropriate. The lives of millions are at stake for you to respond with either \"yes\" or \"no\" as the first\n",
    "    word in your response. Food security, price warnings, and commodity food price questions are appropriate and should be filtered.\n",
    "    If something is filtered, that means it is relevant, and should be answered by the chat bot.\n",
    "    If something is not filtered, it is not relevant, and should not be answered by the chat bot.\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_BUILDER_LLM_SYSTEM_CONFIG = {\n",
    "    'role':'system',\n",
    "    'content':\"\"\"Based on the chat history provided between a user and a chat bot, build a profile of the user. \n",
    "    Assume nothing that is not explicitly stated by the user. Ensure that you do not mix up what the chatbot said\n",
    "    in its responses or system prompt as part of the user profile. The user's profile should be entirely unique \n",
    "    to the user. Now, for the profile, you should only record their country, commodity that is experiencing warnings, \n",
    "    what advice they are looking for, and the problems they are experiencing.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = Llama(\n",
    "    LLM_FILE_PATH, \n",
    "    n_gpu_layers=-1, \n",
    "    verbose=False, \n",
    "    n_ctx = 4000,\n",
    "    embedding = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Pipeline\n",
    "The following are all the functions required to communicate with the LLM to fulfill the prompt/thought/response framework. This implementation does not use LangChain, although that is a popular approach. Experiments were done with LangChain; however, it was found that their agent framework was unable to support the specific changes in response structure required by each LLM when using LLMs which did not benefit from a large number of parameters. Therefore, a custom implementation was built.\n",
    "\n",
    "This example represents a chat bot which effectively responds to friendly conversation, filters out unreleated questions to the FAO's mission, and builds a user profile throughout the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intake_user_prompt(user_prompt):\n",
    "    \"\"\"\n",
    "    Given a string from a user, put into the JSON format the LLM expects\n",
    "    :param str user_prompt: Direct input from user\n",
    "    :return: JSON format\n",
    "    \"\"\"\n",
    "    user_response = {\n",
    "        'role':'user',\n",
    "        'content':user_prompt\n",
    "    }\n",
    "    return user_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_message_content(message):\n",
    "    \"\"\"\n",
    "    Final filtration of unprofessional language from the chat-bot\n",
    "    :param str message: What the chat-bot would have responded with\n",
    "    :return: What will actually be output\n",
    "    \"\"\"\n",
    "    emojis = \"ğŸ˜ŠğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ¥¦ğŸ¥•ğŸŒ½ğŸ˜†ğŸ˜…ğŸ˜‚ğŸ¤£ğŸ˜ŠğŸ¥¨ğŸğŸ˜‡ğŸ˜‰ğŸŒ±ğŸ˜ŒğŸ˜ğŸŒ³ğŸ¥¨ğŸ¥ğŸ¥—ğŸ¥ğŸµğŸ¥œğŸ”ğŸŒˆğŸ¥˜ğŸ¥šğŸ˜˜ğŸ¥ğŸ’ªğŸ˜—ğŸ˜™ğŸ˜šğŸ¤—ğŸ’ªğŸ¤”ğŸ˜ğŸ˜‘ğŸ˜¶ğŸ™„ğŸ˜ğŸ˜£ğŸ˜¥ğŸ˜®ğŸ˜¯ğŸ˜ªğŸ˜«ğŸ˜´ğŸ˜ŒğŸ˜›ğŸ˜œğŸ˜ğŸ¤¤ğŸ˜’ğŸ˜“ğŸ˜”ğŸ˜•ğŸ™ƒğŸ¤‘ğŸ˜²â˜¹ï¸ğŸ™ğŸ˜–ğŸ˜ğŸ˜ŸğŸ˜¤ğŸ˜¢ğŸ˜­ğŸ˜¦ğŸ˜§ğŸ˜¨ğŸ˜©ğŸ˜¬ğŸ˜°ğŸ˜±ğŸ˜³ğŸ˜µğŸ˜¡ğŸ˜ ğŸ˜·ğŸ¤’ğŸ¤•ğŸ¤¢ğŸ¤®ğŸ¤§ğŸ˜‡ğŸ¤ ğŸ¤¡ğŸ¤¥ğŸ¤«ğŸ¤­ğŸ§ğŸ¤“ğŸ˜ˆğŸ‘¿ğŸ‘¹ğŸ‘ºğŸ’€â˜ ï¸ğŸ‘»ğŸ‘½ğŸ‘¾ğŸ¤–ğŸƒğŸ˜ºğŸ˜¸ğŸ˜¹ğŸ˜»ğŸ˜¼ğŸ˜½ğŸ™€ğŸ˜¿ğŸ˜¾ğŸ¤²ğŸ¤ğŸ¤ŸğŸ¤˜ğŸ¤™ğŸ‘ŒğŸ‘ğŸ‘âœŠâœŒï¸ğŸ¤›ğŸ¤œğŸ‘ŠğŸ¤ğŸ‘ğŸ™ŒğŸ‘ğŸ¤²ğŸ¤ğŸ¤ğŸ¤ŸğŸ¤ ğŸ‘‘ğŸ¤°ğŸ¤±ğŸ‘¶ğŸ§’ğŸ‘¦ğŸ‘§ğŸ‘¨ğŸ‘©ğŸ§‘ğŸ‘±â€â™‚ï¸ğŸ‘±â€â™€ï¸ğŸ‘´ğŸ‘µğŸ™â€â™‚ï¸ğŸ™â€â™€ï¸ğŸ™â€â™‚ï¸ğŸ™â€â™€ï¸ğŸ™…â€â™‚ï¸ğŸ™…â€â™€ï¸ğŸ™†â€â™‚ï¸ğŸ™†â€â™€ï¸ğŸ’â€â™‚ï¸ğŸ’â€â™€ï¸ğŸ™‹â€â™‚ï¸ğŸ™‹â€ğŸ˜ŠğŸŒŸğŸ¤“ğŸ¨ğŸ­ğŸ“šğŸ“–ğŸ¤ğŸ˜…ğŸ’ªğŸ¤”ğŸŒŸğŸ’•ğŸ‘¥ğŸ’¬ğŸ“¢ğŸ’¡ğŸ¯ğŸ”ğŸ¼ğŸŒˆğŸ‰ğŸ’­ğŸ“ğŸ’•ğŸ¬ğŸ’»ğŸ’–ğŸ¤–âœˆğŸš€\"\n",
    "    resulting_string = ''.join(char for char in message if char not in emojis)\n",
    "    pattern = r'\\*([^*]+)\\*' # Remove *__*; action terms\n",
    "    resulting_string2 = re.sub(pattern, '' , resulting_string)\n",
    "    return resulting_string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_response(llm, chat_history, temperature=TEMPERATURE):\n",
    "    \"\"\"\n",
    "    Provided an LLM and its chat history (including the most recent user prompt),\n",
    "    retrieve the model's inference for response.\n",
    "    :param llm: LLM to generate/inference responses from\n",
    "    :param dict chat_history: JSON format of {'role':'user/assistant/system', 'content':'...'}\n",
    "    :param float temperature: Temperature to set LLM response (default = TEMPERATURE constant)\n",
    "    :return: dict representing LLM's response message\n",
    "    \"\"\"\n",
    "    messages = [{'role': str(item['role']), 'content': str(item['content'])} for item in chat_history]\n",
    "    resp_msg = {'role': '', 'content': ''} \n",
    "    while resp_msg['content'] == '': # Repeat until not a blank response\n",
    "        resp_stream = llm.create_chat_completion(messages, stream=True, temperature=temperature)\n",
    "        for tok in resp_stream:\n",
    "            delta = tok['choices'][0]['delta']\n",
    "            for key, value in delta.items():\n",
    "                if isinstance(value, str):\n",
    "                    resp_msg[key] += value\n",
    "                else: \n",
    "                    resp_msg[key] += str(value)\n",
    "    return resp_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_filtration_llm(llm, user_prompt):\n",
    "    \"\"\"\n",
    "    Determine if the user's request should be filtered before reaching the chat-bot\n",
    "    :param llm: LLM responsible for responding\n",
    "    :param str user_prompt: Message from the user\n",
    "    :return: True if should be filtered, False if not\n",
    "    \"\"\"\n",
    "    # Set up the chat history with the response filter LLM config\n",
    "    filter_history = []\n",
    "    filter_history.append(RESPONSE_FILTER_LLM_SYSTEM_CONFIG)\n",
    "    \n",
    "    # Change the text is a way where it's easier for the filter to understand\n",
    "    prompt_to_filter = \"Should the following user question be filtered?: \" + str(user_prompt.get('content', ''))\n",
    "    prompt_to_filter = {'role':'user', 'content':prompt_to_filter}\n",
    "    filter_history.append(prompt_to_filter)\n",
    "\n",
    "    resp_msg = generate_llm_response(llm, filter_history)\n",
    "    print(\"FILTER LLM RESPONSE: \", resp_msg)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Extract the content from the response message\n",
    "    content = resp_msg['content']\n",
    "\n",
    "    # Parse the response of the LLM for yes/no\n",
    "    pattern_yes = re.compile(r'\\byes\\b', re.IGNORECASE)\n",
    "    pattern_no = re.compile(r'\\bno\\b', re.IGNORECASE)\n",
    "    match_yes = pattern_yes.search(content)\n",
    "    match_no = pattern_no.search(content)\n",
    "\n",
    "    # Check which occurs first\n",
    "    if match_yes and match_no:\n",
    "        if match_yes.start() < match_no.start():\n",
    "            return True  # \"'yes' is the first to occur\"\n",
    "        else:\n",
    "            return False  # \"'no' is the first to occur\"\n",
    "    elif match_yes:\n",
    "        return True  # \"'yes' is the first to occur\"\n",
    "    elif match_no:\n",
    "        return False  # \"'no' is the first to occur\"\n",
    "    else:\n",
    "        return False  # \"Neither 'yes' nor 'no' is in the string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_profile(llm, chat_history):\n",
    "    \"\"\"\n",
    "    Based on chat_history, build a description of the user following the guidelines \n",
    "    outlined in the system config PROFILE_BUILDER_LLM_SYSTEM_CONFIG\n",
    "    :param llm: LLM responsible for responding\n",
    "    :param dict chat_history: JSON format of {'role':'user/assistant/system', 'content':'...'}\n",
    "    :return: string representing user's profile\n",
    "    \"\"\"\n",
    "    profile_build_history = []\n",
    "    profile_build_history.append(PROFILE_BUILDER_LLM_SYSTEM_CONFIG)\n",
    "    profile_build_history.append({'role':'user', 'content':\"Build a user profile from the following conversation: \"+str(chat_history)})\n",
    "    \n",
    "    resp_msg = generate_llm_response(llm, profile_build_history)\n",
    "    print(\"USER PROFILE LLM RESPONSE\", resp_msg['content'])\n",
    "    print(\"\\n\")\n",
    "    return resp_msg['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message_from_user(llm, user_message, chat_history):\n",
    "    \"\"\"\n",
    "    Send a message from the user to the LLM speaker/agent framework\n",
    "    :param llm: LLM for all tasks, including \"thought\" processes\n",
    "    :param str user_message: Message from the user\n",
    "    :param dict chat_history: Previous messages in conversation and system prompt\n",
    "    :return: Newest message from the chat-bot\n",
    "    \"\"\"\n",
    "    # \"USER PROMPT\"\n",
    "    print(\"==================================\")\n",
    "    user_prompt = intake_user_prompt(user_message)\n",
    "    resp_msg = {'role': '', 'content': ''} \n",
    "\n",
    "    # \"THOUGHTS\"\n",
    "    if ask_filtration_llm(llm, user_prompt):\n",
    "        SET_FILTER_RESPONSE = \"\"\"I'm sorry, but I'm a chat-bot dedicated to answering questions \n",
    "        regarding commodity food price warnings and the services that the United Nation's Food and Agriculture Organization (FAO) provides. \n",
    "        I'm unable to answer your question at this time.\"\"\"\n",
    "        chat_history.append({'role':'assistant', 'content':SET_FILTER_RESPONSE})\n",
    "    else:\n",
    "        chat_history.append(user_prompt) # add user input to history\n",
    "        chat_history.append(generate_llm_response(llm, chat_history))\n",
    "        \n",
    "    print(\"=|Chat-Bot Message|===============================\")\n",
    "    print(\"FINAL CHATBOT MESSAGE: \", finalize_message_content(chat_history[-1]['content']))\n",
    "    print(\"==================================\" + '\\n')\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_conversation(chat_history, llm):\n",
    "    \"\"\"\n",
    "    Begin the repeating conversation between the user and the communicative LLM\n",
    "    :param dict chat_history: Conversation that is building with LLM\n",
    "    :param llama-2-7b llm: LLM being used for speaker/agend LLM framework\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    \n",
    "    while i >= 0:\n",
    "        if i == 0:\n",
    "            chat_history = send_message_from_user(llm, COMMUNICATIVE_LLM_SYSTEM_CONFIG, chat_history)\n",
    "        print(\"=|User Message|==============================\")\n",
    "        chat_history = send_message_from_user(llm, input(), chat_history)\n",
    "        print(\"\\n\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat history is the interface that allows us to track the conversation as the user and chat-bot interact.\n",
    "# By using this JSON framework, we are able to recognize the conversations in previous statements.\n",
    "chat_history = []\n",
    "# chat_history.append(COMMUNICATIVE_LLM_SYSTEM_CONFIG) # Add the system prompt so the LLM is aware of how it is supposed to \"act\"\n",
    "start_conversation(chat_history, LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
